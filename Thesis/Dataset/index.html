<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Well maintained lihttps://github.com/coqui-ai/open-speech-corpora
Potential Source https://speechresearch.github.io/prompttts/#dataset
5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS API The emotion factor has 5 categories Thee gender factor has 2 categories rest of style factors including pitch, speaking speed, and volume, we extract the value of style factors from speech with signal processing tools divided into 3 categories (high/normal/low) we ask experts to write style prompts for each category further augment the style prompts, we utilize SimBERT to generate more style prompts with similar semantics."><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.87988a113f9da26a245c74dfd4fe9ffb.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.08aa5668b0574ddf4c558e24b5d47c32.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><p>Well maintained lihttps://github.com/coqui-ai/open-speech-corpora</p><p>Potential Source
<a target=“_blank” href=https://speechresearch.github.io/prompttts/#dataset rel=noopener>https://speechresearch.github.io/prompttts/#dataset</a></p><ul><li>5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS
<a target=“_blank” href=https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/#overview rel=noopener>API</a></li><li>The emotion factor has 5 categories</li><li>Thee gender factor has 2 categories</li><li>rest of style factors including <strong>pitch</strong>, <strong>speaking</strong> <strong>speed</strong>, and <strong>volume</strong>, we extract the value of style factors from speech with signal processing tools<ul><li>divided into 3 categories (high/normal/low)</li></ul></li><li>we ask experts to write style prompts for each category<ul><li>further augment the style prompts, we utilize SimBERT to generate more style prompts with similar semantics.</li></ul></li></ul><p>Our definition of Speaking Style
<strong>speaking style as any perceptually distinct manner of speaking that is context-appropriate.</strong></p><p>narration (book reading), novel/anime characters, neutral discourse, news reader, customer-service style, poetry style</p><p><strong>News Reading</strong></p><ol><li>The &ldquo;<strong>1996 English Broadcast News Speech (HUB4)</strong>&rdquo; dataset contains a total of 104 hours of broadcasts from ABC, CNN, and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts. The dataset is divided into a training set, development data, and evaluation data. Transcripts have been made of all recordings in this publication, manually time aligned to the phrasal level, annotated to identify boundaries between news stories, speaker turn boundaries, and gender information about the speakers​
<a target=“_blank” href=https://catalog.ldc.upenn.edu/LDC97S44 rel=noopener>1</a></li><li>The &ldquo;<strong>TDT4 Multilingual Broadcast News Speech Corpus</strong>&rdquo; was developed by the Linguistic Data Consortium (LDC) with support from the DARPA TIDES Program. This release contains the complete set of American English, Modern Standard Arabic, and Mandarin Chinese broadcast news audio used in the 2002 and 2003 Topic Detection and Tracking (TDT) technology evaluations, totaling approximately 607 hours. The TDT4 corpus contains news data collected daily from 20 news sources in three languages (American English, Mandarin Chinese, and Modern Standard Arabic), over a period of four months (October 2000 through January 2001)​
<a target=“_blank” href=https://catalog.ldc.upenn.edu/LDC2005S11 rel=noopener>2</a>​.​</li></ol><ul><li><p><strong>BBC Programs</strong> (Not News)
<strong>Lip Reading Sentences 2 (LRS2)</strong>: This is a dataset for lip reading sentences. It&rsquo;s one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists mainly of news and talk shows from BBC programs. Each sentence is up to 100 characters in length. It contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances, and the test set contains 1,242 utterances​
<a target=“_blank” href="https://paperswithcode.com/datasets?task=speech-recognition" rel=noopener>1</a>​.</p><ul><li><a target=“_blank” href=https://www.robots.ox.ac.uk/~vgg/data/lip_reading/ rel=noopener>https://www.robots.ox.ac.uk/~vgg/data/lip_reading/</a></li></ul></li><li><p>Blizzard challenge:</p><ul><li>The books are read by the 2013 Blizzard Challenge speaker, Catherine Byers, in an<br>animated and emotive storytelling style. Some books contain very expressive character voices with high dynamic range, which are challenging to model</li><li><a target=“_blank” href=https://arxiv.org/pdf/1808.01410.pdf rel=noopener>https://arxiv.org/pdf/1808.01410.pdf</a> - PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS</li></ul></li></ul><ol><li><p><strong>LibriSpeech</strong>: This is a standard large-scale dataset for evaluating ASR (Automatic Speech Recognition) systems. It consists of approximately 1,000 hours of narrated audiobooks collected from the LibriVox project. The speaking style is described as &ldquo;narrated"​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>Common Voice</strong>: This is a crowd-sourced open-licensed speech dataset where speakers record text from Wikipedia in various languages. The speaking style is &ldquo;narrated&rdquo; and the dataset contains significant variation in both audio quality and speakers. The English subset of version 11.0 contains approximately 2,300 hours of validated data​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>VoxPopuli</strong>: This is a large-scale multilingual speech corpus consisting of data sourced from 2009-2020 European Parliament event recordings. The speaking style is described as &ldquo;oratory, political speech&rdquo; and the dataset largely consists of non-native speakers. The English subset contains approximately 550 hours of labeled speech​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>TED-LIUM</strong>: This dataset is based on English-language TED Talk conference videos. The speaking style is &ldquo;oratory educational talks&rdquo; covering a range of different cultural, political, and academic topics. The Release 3 (latest) edition of the dataset contains approximately 450 hours of training data​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>GigaSpeech</strong>: This is a multi-domain English speech recognition corpus curated from audiobooks, podcasts, and YouTube. It covers both &ldquo;narrated and spontaneous speech&rdquo; over a variety of topics. The dataset contains training splits varying from 10 hours - 10,000 hours​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>SPGISpeech</strong>: This is an English speech recognition corpus composed of company earnings calls that have been manually transcribed. The speaking style is described as &ldquo;oratory and spontaneous speech&rdquo; and it contains training splits ranging from 200 hours - 5,000 hours​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li><li><p><strong>Earnings-22</strong>: This is a 119-hour corpus of English-language earnings calls collected from global companies. The dataset does not explicitly mention the speaking style, but it is described as covering a range of real-world financial topics​
<a target=“_blank” href=https://huggingface.co/blog/audio-datasets rel=noopener>1</a>​.</p></li></ol></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fThesis%2fDataset%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ♥️ hearing when the post was
helpful. It makes me smile 😄.</p></div><div id=contact_buttons><footer><p>Made with ♥ in Edinburgh, Scotland, UK<br>© 26263 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>