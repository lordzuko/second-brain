<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Himanshu Maurya</title><link>https://lordzuko.github.io/second-brain/papers/</link><description>Recent content from Himanshu Maurya's Digital Garden</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://lordzuko.github.io/second-brain/papers/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Categoris-of-Models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Categoris-of-Models/</guid><description> Autoregressive WaveNet WaveRNN GAN MelGAN Parallel WaveGAN Diffusion WaveGrad DiffWave</description><content:encoded><![CDATA[<ul>
<li><em>Autoregressive</em>
<ul>
<li><em>WaveNet</em></li>
<li><em>WaveRNN</em></li>
</ul>
</li>
<li><em>GAN</em>
<ul>
<li><em>MelGAN</em></li>
<li><em>Parallel WaveGAN</em></li>
</ul>
</li>
<li><em>Diffusion</em>
<ul>
<li><em>WaveGrad</em></li>
<li><em>DiffWave</em></li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Datasets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Datasets/</guid><description> LJ Speech LibriTTS VCTK</description><content:encoded><![CDATA[<ul>
<li><em>LJ Speech</em></li>
<li><em>LibriTTS</em></li>
<li><em>VCTK</em></li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Diffusion/Grad-TTS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Diffusion/Grad-TTS/</guid><description> Atli&amp;rsquo;s Notes: https://docs.google.com/presentation/d/1-SVmRGUJw7dA3zs7duvpFq_JthljMtF0LbUMyRMSZCQ/edit#slide=id.p</description><content:encoded><![CDATA[<ul>
<li>Atli&rsquo;s Notes: 

<a target=“_blank” href="https://docs.google.com/presentation/d/1-SVmRGUJw7dA3zs7duvpFq_JthljMtF0LbUMyRMSZCQ/edit#slide=id.p" rel="noopener">https://docs.google.com/presentation/d/1-SVmRGUJw7dA3zs7duvpFq_JthljMtF0LbUMyRMSZCQ/edit#slide=id.p</a></li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/second-brain/Papers/Speech-Synthesis/Metrics/</guid><description>Mean Opinion Score (MOS)
Objective evaluation metric for quality measurement Frechet Audio Distance (FAD)
Objective evaluation metric for quality measurement FAD score is the distance between two multivariate Gaussian distributins (i.</description><content:encoded><![CDATA[<ul>
<li>
<p><em>Mean Opinion Score</em> (MOS)</p>
<ul>
<li>Objective evaluation metric for quality measurement</li>
</ul>
</li>
<li>
<p><em>Frechet Audio Distance</em> (FAD)</p>
<ul>
<li>Objective evaluation metric for quality measurement</li>
<li>FAD score is the distance between two multivariate Gaussian distributins (i.e. the background and evaluation embeddings)</li>
</ul>
</li>
<li>
<p><em>Structural Similarity Index</em> (SSIM)</p>
</li>
<li>
<p><em>Log-mel Spectrogram Mean Squared Error</em> (LS-MSE)</p>
</li>
<li>
<p><em>Peak Signal-to-Noise Ration</em> (PSNR)</p>
</li>
<li>
<p>SSIM, LS-MSE, PSNR are quantitative evaluation metrics for similarity and noise measurement</p>
</li>
<li>
<p>All the computations in each metric are done in the frequency domain to compare the synthesized Mel-Spectrogram with ground truth</p>
</li>
</ul>
<p>Reference:</p>
<ul>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/2112.03099.pdf" rel="noopener">Vocbench: A Neural Vocoder Benchmark for Speech Synthesis</a></li>
</ul>
]]></content:encoded></item></channel></rss>