<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="PCA Algo The algorithm for PCA can be broken down into the following steps:
Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all features are on the same scale."><title>Brandon K Boswell</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.16fcf40f37c93b955bce577ac87ad193.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.22dfeb6e4b9a4d8d7fec5c46b13d453d.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0)}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-V9PR00CYK8"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-V9PR00CYK8",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=/header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#pca-algo><h2 id=pca-algo><span class=hanchor arialabel=Anchor># </span>PCA Algo</h2></a><p><img src=/Screenshot%202023-04-30%20at%2010.25.38%20PM.png alt="Screenshot 2023-04-30 at 10.25.38 PM.png"></p><p>The algorithm for PCA can be broken down into the following steps:</p><ol><li>Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all features are on the same scale.</li></ol><p>**2. Compute the covariance matrix: Calculate the covariance matrix of the standardized data.</p><p><strong>3. Compute the eigenvectors and eigenvalues of the covariance matrix: Use eigenvalue decomposition to find the eigenvectors and eigenvalues of the covariance matrix.</strong></p><p><strong>4. Sort the eigenvectors by decreasing eigenvalues: Arrange the eigenvectors in order of decreasing eigenvalues to identify the principal components.</strong></p><p><strong>5. Choose the number of principal components: Determine how many principal components to retain based on the amount of variance explained and/or the desired dimensionality reduction.</strong></p><ol start=6><li>Project the data onto the principal components: Multiply the standardized data by the matrix of eigenvectors corresponding to the top k principal components to obtain the k-dimensional representation of the data.<ol><li>In part 4. of the screenshot -> do this instead cuz it does not highlight the fact that we are using M principal component i.e. the M principal eigen values for representing the data.</li></ol></li></ol><p>The key step in this algorithm is step 3, which involves computing the eigenvectors and eigenvalues of the covariance matrix. This can be done using eigenvalue decomposition, which decomposes the covariance matrix into a product of eigenvectors and eigenvalues.</p><p>Mathematically, this can be expressed as:</p><p>$\Sigma=Q\Lambda Q^{-1}$</p><p>where $\Sigma$ is the covariance matrix, $Q$ is the matrix of eigenvectors, and $\Lambda$ is the diagonal matrix of eigenvalues.</p><p>To compute the eigenvectors and eigenvalues, we can use an algorithm such as the power iteration method, which iteratively estimates the largest eigenvector and eigenvalue. Once the largest eigenvector has been found, it can be removed from the data and the process can be repeated to find the next largest eigenvector and eigenvalue.</p><p>Overall, eigenvalue decomposition is a critical part of the PCA algorithm as it enables us to find the eigenvectors and eigenvalues of the covariance matrix, which in turn allows us to identify the principal components of the data.</p><a href=#here-is-a-step-by-step-process-for-performing-pca-and-spatial-whitening-on-a-given-dataset><h3 id=here-is-a-step-by-step-process-for-performing-pca-and-spatial-whitening-on-a-given-dataset><span class=hanchor arialabel=Anchor># </span>Here is a step-by-step process for performing PCA and spatial whitening on a given dataset:</h3></a><ol><li>Given a dataset $X$ of $n$ samples each having $d$ dimensions, compute the mean of the data and subtract it from each sample to center the data:</li></ol><p>$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \\x_i \leftarrow x_i - \bar{x}
$$</p><ol start=2><li>Compute the covariance matrix $\Sigma$ of the centered data:</li></ol><p>$$
\Sigma = \frac{1}{n} \sum_{i=1}^{n} x_i x_i^T
$$</p><ol start=3><li>Compute the eigendecomposition of the covariance matrix $\Sigma$:</li></ol><p>$$
\Sigma = U \Lambda U^T
$$</p><p>where $\Lambda$ is a diagonal matrix of the eigenvalues and $U$ is a matrix of the corresponding eigenvectors. Note that the eigenvectors in $U$ are already orthonormal.</p><ol start=4><li>Sort the eigenvectors in $U$ in descending order of their corresponding eigenvalues in $\Lambda$ to obtain a matrix $W$:</li></ol><p>$$
W = [u_1, u_2, &mldr;, u_d]
$$</p><p>where $u_i$ is the eigenvector corresponding to the $i$th largest eigenvalue.</p><ol start=5><li>Project the centered data onto the new basis defined by $W$ to obtain the transformed data $Z$:</li></ol><p>$$
Z = XW
$$
<strong>Now -> The step 5 in the previous answer only guarantees zero covariance between dimensions. To ensure unit variance along all dimensions, the covariance matrix of the whitened data needs to be normalized to the identity matrix</strong></p><ol start=6><li>Compute the whitening matrix $W_{white}$ as:</li></ol><p>$$
W_{white} = \Lambda^{-1/2} U^T
$$</p><ol start=7><li>Compute the whitened data $Z_{white}$ as:</li></ol><p>$$
Z_{white} = ZW_{white}
$$</p><p>Now, the whitened data $Z_{white}$ has a covariance matrix equal to the identity matrix, indicating that the dimensions are uncorrelated and have unit variance.</p><p>Note that the spacial whitening is applied after performing PCA, as it is necessary to first transform the data into a new basis before computing the whitening matrix.</p><p>In Summary:</p><p><strong>Spatial whitening is typically applied after performing PCA</strong>. This is because PCA decorrelates the data, but it does not necessarily produce data with unit variance along all dimensions. By applying spatial whitening after PCA, we can ensure that the resulting transformed data has unit variance along all dimensions, which can be useful for certain applications such as gradient descent optimization.</p><p>After performing PCA, the eigenvalues of the covariance matrix are used to normalize the data, resulting in a new set of transformed data points. These transformed data points are then further modified through the process of spatial whitening, which results in data that has unit variance along all dimensions.</p><a href=#independent-component-analysis-ica><h1 id=independent-component-analysis-ica><span class=hanchor arialabel=Anchor># </span>Independent Component Analysis (ICA)</h1></a><p>PCA aims to retain the max variance in the components. However, these components may not be meaningful for the purpose of for example classification; as the component which has maximum variance, may not be the direction in which we have maximum separation of the data.</p><ul><li><p>Non-Gaussianity: Non-Gaussianity refers to the property of a distribution that does not follow a Gaussian or normal distribution.</p></li><li><p><strong>Independent Component Analysis (ICA)</strong> is a technique that is particularly suited for dealing with non-Gaussian data. The <em>basic idea behind ICA is to identify a set of independent sources that underlie the observed data, by exploiting the non-Gaussianity of the sources</em>.</p><ul><li>The assumption is that the data can be expressed as a linear combination of these sources, and that the sources are mutually independent.</li></ul></li></ul><p>Independent Component Analysis (ICA) is a technique used to extract independent signals from a set of mixed signals. It is similar to PCA but it assumes that the sources are not only linearly correlated but also statistically independent. ICA is often used in signal processing and machine learning applications such as image and speech recognition.</p><p><strong>ICA can be performed on a set of mixed signals by following these steps:</strong></p><p>ICA algorithm does not require PCA, but it can be useful to apply PCA or spacial whitening as preprocessing steps before performing ICA. The purpose of these preprocessing steps is to decorrelate the data and reduce the dimensionality of the problem, which can make the ICA algorithm more computationally efficient and more effective.</p><p>Here is the complete process for performing ICA with PCA and spacial whitening:</p><ol><li>Center the data: Subtract the mean of each feature from each data point to center the data around zero.</li></ol><p>$$x_{centered} = x - \mu_x$$</p><ol start=2><li>Perform PCA: Compute the principal components of the centered data using eigenvalue decomposition of the covariance matrix.</li></ol><p>$$\Sigma = U \Lambda U^T$$</p><p>where $\Sigma$ is the covariance matrix, $U$ is the matrix of eigenvectors, and $\Lambda$ is the diagonal matrix of eigenvalues.</p><ol start=3><li>Whiten the data: Transform the centered data into a new space where the covariance matrix is the identity matrix by multiplying it by the inverse square root of the diagonal matrix of eigenvalues and the transpose of the matrix of eigenvectors.</li></ol><p>$$x_{whitened} = \Lambda^{-1/2} U^T x_{centered}$$</p><ol start=4><li><p>Apply ICA to the pre-processed data. This involves finding a set of linear transformations, represented by a matrix $\mathbf{W}$, that decorrelates the columns of $x_{whitened}$ and maximizes the non-Gaussianity of the transformed data.</p></li><li><p>Retrieve the original sources: Compute the original sources by multiplying the mixing matrix by the whitened data.</p></li></ol><p>Originally we have $$ x = \mathbf{A}s $$, where A is an unknow invertible transform, we aim to learn $\textbf{W}$ which is $\mathbf{A^{-1}}$, such that$$ s = \mathbf{W} x_{whitened}$$
Note that step 2 involves the transpose of the matrix of eigenvectors, $U^T$, which is used in step 3 to transform the data into the whitened space. This transpose operation is necessary to ensure that the columns of $U$ form an orthonormal basis for the whitened space.</p><a href=#ambiguities-in-ica><h3 id=ambiguities-in-ica><span class=hanchor arialabel=Anchor># </span>Ambiguities in ICA</h3></a><ol><li><strong>Ambiguity in determining the variances of the independent components</strong></li></ol><p>In ICA, the variances of the estimated independent components are not uniquely determined, and this is known as the variance indeterminacy problem. This ambiguity arises because the independent components can be scaled by arbitrary factors without affecting their independence.</p><p>Mathematically, if we have a set of independent components $s_1, s_2, &mldr;, s_k$ and their corresponding mixing coefficients $A_{i,j}$, then we can scale each independent component $s_i$ by an arbitrary factor $\alpha_i$ to obtain a new set of independent components $s_1&rsquo;, s_2&rsquo;, &mldr;, s_k&rsquo;$:</p><p>$$s_i&rsquo; = \alpha_i s_i$$</p><p>The corresponding mixing coefficients for the new set of independent components will be:</p><p>$$A&rsquo;<em>{i,j} = \frac{A</em>{i,j}}{\alpha_i}$$</p><p>It can be shown that the new set of independent components $s_1&rsquo;, s_2&rsquo;, &mldr;, s_k&rsquo;$ is also independent and satisfies the same linear mixing model as the original set of independent components $s_1, s_2, &mldr;, s_k$.</p><p>Therefore, the variance of the independent components cannot be uniquely determined because any scaling factor applied to the independent components will affect their variances. However, in practice, the scaling factor can be normalized to make the variance of the independent components equal to one or any other desired value, making the indeterminacy of the variances less of a concern. <strong>(The step we take during whitenening step, helps resolve this as it ensures unit variances across all the independent components)</strong></p><ol start=2><li><strong>Ambiguity of order of independent components</strong></li></ol><p>The ambiguity of order of independent components refers to the fact that the order in which the independent components are estimated by the ICA algorithm is arbitrary. In other words, the independent components can be returned in any order, and this order may not be meaningful or interpretable.</p><p>This ambiguity can be resolved by using additional information or criteria to order the independent components. For example, if the independent components represent audio signals, they can be ordered based on their frequency content or their energy levels. Alternatively, if the independent components represent images, they can be ordered based on their spatial frequency content or their edge information.</p><p>Another approach is to use clustering algorithms to group independent components that are similar or belong to the same source. This can help to identify the underlying sources of the independent components and to assign them to specific sources in a meaningful way.</p><p>Overall, the ambiguity of order of independent components is a common challenge in ICA, but it can be mitigated by using additional information and criteria to order and group the independent components.</p></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:brandonkboswell@gmail.com target=_blank>Email</a></strong> or <strong><a href=https://twitter.com/brandonkboswell target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fMLSP%2fQS%2fWeek10%2fPCA-Notes%2f&text=&via=brandonkboswell" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ‚ô•Ô∏è hearing when the post was helpful. It makes me smile üòÑ.</p></div><div id=contact_buttons><footer><p>Made with ‚ô• in Richmond, VA<br>¬© 2023 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>