<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Expectations as Integrals In probability theory and statistics, expectations are often expressed as integrals. The expectation of a function $g(x)$ with respect to a probability distribution $p(x)$ is defined as:"><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.71139b94f3f29dfda52032fbfdd80a2a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.fab86bbe49cf4e5f56dd16934c1866ee.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#expectations-as-integrals><h2 id=expectations-as-integrals><span class=hanchor arialabel=Anchor># </span>Expectations as Integrals</h2></a><p>In probability theory and statistics, expectations are often expressed as integrals. The expectation of a function $g(x)$ with respect to a probability distribution $p(x)$ is defined as:</p><p>$\mathbb{E}[g(x)] = \int g(x)p(x)dx$</p><p>This integral represents the average value of $g(x)$ over the probability distribution $p(x)$. The integral is taken over the entire domain of $x$ and the function $g(x)$ is weighted by the probability density function $p(x)$.</p><p>For example, the mean of a random variable $X$ with probability density function $f(x)$ can be expressed as:</p><p>$\mathbb{E}[X] = \int x f(x)dx$</p><p>Similarly, the variance of $X$ can be expressed as:</p><p>$\text{Var}(X) = \mathbb{E}[(X-\mu)^2] = \int (x-\mu)^2 f(x)dx$</p><p>where $\mu$ is the mean of $X$.</p><p>Expectations as integrals allow us to compute various properties of probability distributions, such as mean, variance, and higher moments, and to perform statistical inference, such as hypothesis testing and parameter estimation, based on probability distributions. They also provide a way to generalize the concept of average or expected value beyond simple arithmetic means.</p><a href=#making-predictions-with-bayesian-learning-posterior-mean><h2 id=making-predictions-with-bayesian-learning-posterior-mean><span class=hanchor arialabel=Anchor># </span>Making Predictions with Bayesian Learning: Posterior Mean</h2></a><p>In Bayesian learning, the goal is to compute the posterior distribution over the model parameters given the observed data. Once we have obtained the posterior distribution, we can use it to make predictions for new, unseen data points.</p><p>One way to make predictions using Bayesian learning is to use the posterior mean of the model parameters as the predicted value. The posterior mean is a weighted average of the possible parameter values, where the weights are given by the posterior distribution.</p><p>Suppose we have a model with parameters $\theta$ and observed data $\mathcal{D}$. We can compute the posterior distribution over $\theta$ using Bayes&rsquo; rule:</p><p>$p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})}$</p><p>where $p(\mathcal{D}|\theta)$ is the likelihood function, $p(\theta)$ is the prior distribution over the parameters, and $p(\mathcal{D})$ is the marginal likelihood, which acts as a normalizing constant.</p><p>**Once we have obtained the posterior distribution $p(\theta|\mathcal{D})$, we can use the posterior mean of $\theta$ as the predicted value for a new data point $\mathbf{x}$. That is,</p><p>$\hat{y}(\mathbf{x}) = \mathbb{E}_{p(\theta|\mathcal{D})}[y(\mathbf{x},\theta)]$</p><p><strong>where $y(\mathbf{x},\theta)$ is the predicted output for $\mathbf{x}$ given the parameter value $\theta$, and $\mathbb{E}_{p(\theta|\mathcal{D})}$ denotes the expectation with respect to the posterior distribution over $\theta$.</strong></p><p>In practice, computing the posterior mean may involve numerical integration or Monte Carlo methods, depending on the complexity of the model and the posterior distribution. However, the posterior mean provides a principled way to make predictions that takes into account the uncertainty in the model parameters and the observed data.</p><a href=#making-predictions-with-bayesian-learning-maximum-a-priori><h2 id=making-predictions-with-bayesian-learning-maximum-a-priori><span class=hanchor arialabel=Anchor># </span>Making Predictions with Bayesian Learning: Maximum A Priori</h2></a><p>In Bayesian learning, we often seek to estimate the posterior distribution over the model parameters given the observed data. One way to make predictions using Bayesian learning is to use the maximum a priori (MAP) estimate of the parameters.</p><p>The MAP estimate of the parameters $\theta$ is the value that maximizes the posterior distribution $p(\theta|\mathcal{D})$. Mathematically, we can write:</p><p>$$
\theta_{\text{MAP}} = \operatorname*{argmax}<em>{\theta} p(\theta\mid D) = \operatorname*{argmax}</em>{\theta} p(D\mid\theta)p(\theta)
$$</p><p>where $\mathcal{D}$ represents the observed data, $p(\mathcal{D}|\theta)$ is the likelihood function, and $p(\theta)$ is the prior distribution over the parameters.</p><p>Once we have obtained the MAP estimate of the parameters, we can use it to make predictions for new, unseen data points. For example, in a regression problem, we can predict the output value $\hat{y}$ for a new input $\mathbf{x}$ as follows:</p><p>$$
\hat{y} = f(\mathbf{x};\theta_{\text{MAP}})
$$</p><p>where $f(\mathbf{x};\theta)$ is the regression function that maps inputs to outputs using the parameters $\theta$.</p><p>The MAP estimate can be viewed as a point estimate of the parameters, since it provides a single value for the parameters rather than a distribution over possible values. It can be more computationally efficient than computing the full posterior distribution, especially for large-scale problems.</p><p>However, the MAP estimate may not fully capture the uncertainty in the model parameters and can be sensitive to the choice of prior distribution. In addition, it can be affected by overfitting if the prior distribution is not well-suited to the data. Therefore, it is important to consider the limitations and assumptions of the MAP estimate and to compare it with other methods of Bayesian learning, such as computing the full posterior distribution or using Bayesian model averaging.</p></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fMLSP%2fQS%2fWeek7%2fBayesian-Learning%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ‚ô•Ô∏è hearing when the post was
helpful. It makes me smile üòÑ.</p></div><div id=contact_buttons><footer><p>Made with ‚ô• in Richmond, VA<br>¬© 2023 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>