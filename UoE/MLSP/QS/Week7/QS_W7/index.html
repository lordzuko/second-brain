<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="1.b Show that the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.
MAP vs. ML Estimates under Nonlinear Transformations The maximum likelihood (ML) estimate of the model parameters $\theta$ is the value that maximizes the likelihood function $p(D\mid\theta)$, where $D$ represents the observed data."><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io/second-brain//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/second-brain/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/second-brain/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/second-brain/",fetchData=Promise.all([fetch("https://lordzuko.github.io/second-brain/indices/linkIndex.87988a113f9da26a245c74dfd4fe9ffb.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/second-brain/indices/contentIndex.0c87a422af327b9ac82b07a84144e760.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io/second-brain",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io/second-brain",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/second-brain/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/second-brain/><img src=../../../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><p>1.b <strong>Show that the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.</strong></p><a href=#map-vs-ml-estimates-under-nonlinear-transformations><h5 id=map-vs-ml-estimates-under-nonlinear-transformations><span class=hanchor arialabel=Anchor># </span>MAP vs. ML Estimates under Nonlinear Transformations</h5></a><p>The maximum likelihood (ML) estimate of the model parameters $\theta$ is the value that maximizes the likelihood function $p(D\mid\theta)$, where $D$ represents the observed data. Mathematically, we can write:</p><p>$$
\theta_{\text{ML}} = \operatorname*{argmax}_{\theta} p(D\mid\theta)
$$</p><p>The maximum a posteriori (MAP) estimate of the parameters $\theta$ is the value that maximizes the posterior distribution $p(\theta\mid D)$. Mathematically, we can write:</p><p>$$
\theta_{\text{MAP}} = \operatorname*{argmax}<em>{\theta} p(\theta\mid D) = \operatorname*{argmax}</em>{\theta} p(D\mid\theta)p(\theta)
$$</p><p>Suppose we have a nonlinear transform $g$ of the parameter space such that $\theta = g(\phi)$, where $\phi$ represents the transformed parameters. If $g$ is invertible, we can write $\phi = g^{-1}(\theta)$.</p><p>The likelihood function under the transformed parameter space is given by $p(D\mid\phi)$. The ML estimate of the transformed parameters $\phi$ is the value that maximizes the likelihood function, which we can write as:</p><p>$$
\phi_{\text{ML}} = \operatorname*{argmax}_{\phi} p(D\mid\phi)
$$</p><p>However, the MAP estimate of the transformed parameters $\phi$ is given by:</p><p>$$
\phi_{\text{MAP}} = \operatorname*{argmax}<em>{\phi} p(\phi\mid D) = \operatorname*{argmax}</em>{\phi} p(D\mid\phi)p(\phi)
$$</p><p>Now, we can use the <strong>change of variables formula</strong> for probability densities to write:</p><p>(look for the change of variables concept below)</p><p>$$
p(D\mid\phi) = p(D\mid\theta) \left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|^{-1}
$$</p><p>where $\left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|$ is the absolute value of the determinant of the Jacobian matrix of $g$ with respect to $\phi$.</p><p>Substituting this into the expressions for the ML and MAP estimates, we get:</p><p>$$
\phi_{\text{ML}} = g^{-1}\left(\operatorname*{argmax}_{\theta} p(D\mid\theta)\right)
$$</p><p>and</p><p>$$
\phi_{\text{MAP}} = g^{-1}\left(\operatorname*{argmax}_{\theta} p(D\mid\theta)p(g(\phi))\left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|^{-1}\right)
$$</p><p>We can see that the ML estimate of the transformed parameters $\phi$ is simply the inverse transform of the ML estimate of the original parameters $\theta$. However, the MAP estimate of the transformed parameters $\phi$ depends on both the prior distribution over the transformed parameters $p(\phi)$ and the Jacobian of the transformation. Therefore, the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.</p><p><strong>Change of Variables Formula for Probability Densities</strong></p><p>The change of variables formula for probability densities is a formula that allows us to transform a probability density function from one variable to another. It is used when we want to work with a probability distribution in a new set of variables.</p><p>Suppose we have a probability density function $p_Y(y)$ over the random variable $Y$, and we want to transform it to a probability density function $p_X(x)$ over the random variable $X$ using a one-to-one function $g: X \rightarrow Y$. Then, the change of variables formula for probability densities states that:</p><p>$$
p_X(x) = p_Y(g(x)) \left|\frac{dg(x)}{dx}\right|
$$</p><p>where $\left|\frac{dg(x)}{dx}\right|$ is the absolute value of the derivative of the function $g$ with respect to $x$.</p><p>Intuitively, this formula tells us that the probability density of $X$ at $x$ is equal to the probability density of $Y$ at $g(x)$ multiplied by the factor that accounts for the stretching or compression of the probability density as we change variables.</p><p>To use the formula, we first find the inverse function of $g$, which we can use to express $y$ in terms of $x$. Then, we substitute this expression for $y$ into the original probability density function $p_Y(y)$ to get $p_Y(g(x))$. Finally, we multiply this by the factor $\left|\frac{dg(x)}{dx}\right|$ to get the probability density function $p_X(x)$ over the variable $X$.</p><p>The change of variables formula is commonly used in Bayesian inference and machine learning to transform probability distributions between different parameterizations or to transform data from one feature space to another.</p><ol start=3><li><strong>Let x be drawn from a uniform density; calculate the maximum likelihood estimate for Œ∏ given the data: D = {0.1, 0.4, 0.2, 0.8, 0.45}.</strong></li></ol><p>We know that $x$ is drawn from a uniform density with parameter $\theta$ over the interval $[0, 1]$. The probability density function of $x$ is given by:</p><p>$$
p(x | \theta) =
\begin{cases}
\frac{1}{\theta} & 0 \leq x \leq \theta \\0 & \text{otherwise}
\end{cases}
$$</p><p>Given the data $D = {0.1, 0.4, 0.2, 0.8, 0.45}$, the likelihood function is:</p><p>$$
L(\theta | D) = \prod_{i=1}^n p(x_i | \theta) =
\begin{cases}
\frac{1}{\theta^n} & x_{(1)} \leq \theta \leq x_{(n)} \\0 & \text{otherwise}
\end{cases}
$$</p><p>where $x_{(1)}$ and $x_{(n)}$ are the minimum and maximum values in the dataset, respectively.</p><p>To find the maximum likelihood estimate of $\theta$, we need to find the value of $\theta$ that maximizes the likelihood function. Since the likelihood function is a decreasing function of $\theta$ for $\theta &lt; x_{(n)}$, and an increasing function of $\theta$ for $\theta > x_{(1)}$, the maximum likelihood estimate of $\theta$ is given by:</p><p>$$
\hat{\theta}<em>{ML} = x</em>{(n)}
$$</p><p>In this case, the maximum value in the dataset is 0.8, so the maximum likelihood estimate of $\theta$ is $\hat{\theta}_{ML} = 0.8$.</p><p><strong>NOTE: Why are we not taking derivatives in this case? Can we also not solve for it by taking derivative of likelihood function?</strong></p><p><em>In this particular case, the likelihood function is not differentiable everywhere, so we cannot take its derivative to find the maximum likelihood estimate of $\theta$.</em></p><p>However, we can observe that the likelihood function is a decreasing function of $\theta$ for $\theta &lt; x_{(n)}$ and an increasing function of $\theta$ for $\theta > x_{(1)}$. This means that the maximum likelihood estimate of $\theta$ must lie in the interval $[x_{(1)}, x_{(n)}]$. We can then compare the likelihood of the two endpoints of this interval to find the maximum likelihood estimate. Since the likelihood is 0 outside of this interval, we can be sure that the maximum likelihood estimate lies within it.</p><p>In this case, the endpoint $x_{(n)}$ has the highest likelihood among all the possible values of $\theta$ within the interval, so the maximum likelihood estimate is $\hat{\theta}<em>{ML} = x</em>{(n)} = 0.8$.</p></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/second-brain/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fsecond-brain%2fUoE%2fMLSP%2fQS%2fWeek7%2fQS_W7%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ‚ô•Ô∏è hearing when the post was
helpful. It makes me smile üòÑ.</p></div><div id=contact_buttons><footer><p>Made with ‚ô• in Edinburgh, Scotland, UK<br>¬© 26262 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>