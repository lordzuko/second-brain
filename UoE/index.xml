<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Himanshu Maurya</title><link>https://lordzuko.github.io/uoe/</link><description>Recent content from Himanshu Maurya's Digital Garden</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 01 May 2023 12:00:00 +0000</lastBuildDate><atom:link href="https://lordzuko.github.io/uoe/index.xml" rel="self" type="application/rss+xml"/><item><title>Week 3</title><link>https://lordzuko.github.io/UoE/NLU/Week3/</link><pubDate>Mon, 01 May 2023 12:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week3/</guid><description>This is a summary of week 3</description><content:encoded><![CDATA[<p><strong>Attention Model Intuition</strong></p>
<h1 id="seq2seq-models-with-attention">Seq2Seq Models with Attention</h1>
<p>In a sequence-to-sequence (seq2seq) encoder-decoder model with attention, the goal is to generate a sequence of output tokens based on a sequence of input tokens. The model consists of two parts: an encoder that processes the input sequence and a decoder that generates the output sequence. The attention mechanism is used in the decoder to help the model focus on the most relevant parts of the input sequence while generating each output token.</p>
<p>The attention mechanism works as follows:</p>
<ol>
<li>
<p>The encoder produces a sequence of hidden states $h_1, h_2, &hellip;, h_n$ that represent the input sequence.</p>
</li>
<li>
<p>At each step $t$ of the decoder, the decoder produces a hidden state $s_t$ that represents the decoder&rsquo;s current state.</p>
</li>
<li>
<p>The attention mechanism calculates a set of attention weights $a^t_1, a^t_2, &hellip;, a^t_n$ that indicate how much attention should be given to each hidden state $h_i$ of the encoder when generating the output at step $t$ of the decoder.</p>
<ol>
<li>The attention weights are used to compute a context vector $c_t$ that is a weighted sum of the encoder hidden states, where each hidden state is weighted by its corresponding attention weight:</li>
</ol>
</li>
</ol>
<p>$$c_t = \sum_{i=1}^{n} a^t_i h_i$$</p>
<ol start="5">
<li>The context vector $c_t$ is concatenated with the decoder hidden state $s_t$ to form a combined vector $z_t$:</li>
</ol>
<p>$$z_t = [c_t; s_t]$$</p>
<p>where $[;]$ denotes concatenation.</p>
<ol start="6">
<li>The combined vector $z_t$ is used to predict the probability distribution over the output vocabulary using a softmax function:</li>
</ol>
<p>$$P_{vocab} = \text{softmax}(V&rsquo;^T(Vz_t + b&rsquo;)$$</p>
<p>where $V&rsquo;$ is a weight matrix and $b&rsquo;$ is a bias term.</p>
<p>The attention weights $a^t_i$ are calculated using the following equations:</p>
<p>$$e_i^t= v^T \text{tanh}(W_h h_i + W_s s_t + b_{attn})$$</p>
<p>$$a^t_i = \frac{\text{exp}(e_i^t)}{\sum_{j=1}^{n} \text{exp}(e_j^t)}$$</p>
<p>where $v$, $W_h$, $W_s$, and $b_{attn}$ are learnable parameters.</p>
<p>The first equation computes a score $e_i^t$ for each encoder hidden state $h_i$ based on the decoder hidden state $s_t$. The second equation applies a softmax function to the scores to obtain the attention weights $a^t_i$ that sum to 1. The attention weights indicate the relative importance of each encoder hidden state $h_i$ at the current step $t$ of the decoder.</p>
<p>By using the attention mechanism, the model can selectively focus on different parts of the input sequence at each step of the decoder, improving the model&rsquo;s ability to generate accurate and relevant output tokens.</p>
<h1 id="transformers">Transformers</h1>
<blockquote>
<p><strong>Calculating the parameters of transformers:</strong></p>
</blockquote>
<ul>
<li>
<p>

<a target=“_blank” href="https://stackoverflow.com/a/71472362" rel="noopener">https://stackoverflow.com/a/71472362</a></p>
<p>

<a target=“_blank” href="https://i.stack.imgur.com/BhVnx.png" rel="noopener">Transformer Encoder-Decoder Architecture</a> The BERT model contains only the encoder block of the transformer architecture. Let&rsquo;s look at individual elements of an encoder block for BERT to visualize the number weight matrices as well as the bias vectors. The given configuration L = 12 means there will be 12 layers of self attention, H = 768 means that the embedding dimension of individual tokens will be of 768 dimensions, A = 12 means there will be 12 attention heads in one layer of self attention. The encoder block performs the following sequence of operations:</p>
<ol>
<li>
<p>The input will be the sequence of tokens as a matrix of S * d dimension. Where s is the sequence length and d is the embedding dimension. The resultant input sequence will be the sum of token embeddings, token type embeddings as well as position embedding as a d-dimensional vector for each token. In the BERT model, the first set of parameters is the vocabulary embeddings. BERT uses WordPiece[

<a target=“_blank” href="https://arxiv.org/abs/1609.08144" rel="noopener">2</a>] embeddings that has 30522 tokens. Each token is of 768 dimensions.</p>
</li>
<li>
<p>Embedding layer normalization. One weight matrix and one bias vector.</p>
</li>
<li>
<p>Multi-head self attention. There will be h number of heads, and for each head there will be three matrices which will correspond to query matrix, key matrix and the value matrix. The first dimension of these matrices will be the embedding dimension and the second dimension will be the embedding dimension divided by the number of attention heads. Apart from this, there will be one more matrix to transform the concatenated values generated by attention heads to the final token representation.</p>
</li>
<li>
<p>Residual connection and layer normalization. One weight matrix and one bias vector.</p>
</li>
<li>
<p>Position-wise feedforward network will have one hidden layer, that will correspond to two weight matrices and two bias vectors. In the paper, it is mentioned that the number of units in the hidden layer will be four times the embedding dimension.</p>
</li>
<li>
<p>Residual connection and layer normalization. One weight matrix and one bias vector.</p>
</li>
</ol>
<p>Let&rsquo;s calculate the actual number of parameters by associating the right dimensions to the weight matrices and bias vectors for the BERT base model.</p>
<p><strong>Embedding Matrices:</strong></p>
<ul>
<li>
<p>Word Embedding Matrix size [Vocabulary size, embedding dimension] = [30522, 768] = 23440896</p>
</li>
<li>
<p>Position embedding matrix size, [Maximum sequence length, embedding dimension] = [512, 768] = 393216</p>
</li>
<li>
<p>Token Type Embedding matrix size [2, 768] = 1536</p>
</li>
<li>
<p>Embedding Layer Normalization, weight and Bias [768] + [768] = 1536</p>
</li>
<li>
<p>Total Embedding parameters = <strong>𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 ≈ 𝟐𝟒𝑴</strong>
<strong>Attention Head:</strong></p>
</li>
<li>
<p>Query Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li>
<p>Key Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li>
<p>Value Weight Matrix size [768, 64] = 49152 and Bias [768] = 768</p>
</li>
<li>
<p>Total parameters for one layer attention with 12 heads = 12∗(3 ∗(49152+768)) = 1797120</p>
</li>
<li>
<p>Dense weight for projection after concatenation of heads [768, 768] = 589824 and Bias [768] = 768, (589824+768 = 590592)</p>
</li>
<li>
<p>Layer Normalization weight and Bias [768], [768] = 1536</p>
</li>
<li>
<p>Position wise feedforward network weight matrices and bias [3072, 768] = 2359296, [3072] = 3072 and [768, 3072 ] = 2359296, [768] = 768, (2359296+3072+ 2359296+768 = 4722432)</p>
</li>
<li>
<p>Layer Normalization weight and Bias [768], [768] = 1536</p>
</li>
<li>
<p>Total parameters for one complete attention layer (1797120 + 590592 + 1536 + 4722432 + 1536 = <strong>7113216 ≈ 7𝑀</strong>)</p>
</li>
<li>
<p>Total parameters for 12 layers of attention (𝟏𝟐 ∗ 𝟕𝟏𝟏𝟑𝟐𝟏𝟔 = <strong>𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 ≈ 𝟖𝟓𝑴</strong>)</p>
</li>
</ul>
<p><strong>Output layer of BERT Encoder:</strong></p>
<ul>
<li>Dense Weight Matrix and Bias [768, 768] = 589824, [768] = 768, (589824 + 768 = 590592)
<em>Total Parameters in 𝑩𝑬𝑹𝑻 𝑩ase = 𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 + 𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 + 𝟓𝟗𝟎𝟓𝟗𝟐 = <strong>𝟏𝟎𝟗𝟕𝟖𝟔𝟑𝟔𝟖 ≈ 𝟏𝟏𝟎𝑴</strong></em></li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/CSAI/Summary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/CSAI/Summary/</guid><description/><content:encoded></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLPR/Probabilistic-Models-in-TTS/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLPR/Probabilistic-Models-in-TTS/</guid><description>In this note I am trying to understand what does Variational means in models like Variational Autoencoders , Variational Inference etc.
This is an interesting setting and after going through everything which is discussed below, it doesn&amp;rsquo;t sounds that complicated.</description><content:encoded><![CDATA[<p>In this note I am trying to understand what does <code>Variational</code> means in models like <code>Variational Autoencoders</code> , <code>Variational Inference</code> etc.</p>
<p>This is an interesting setting and after going through everything which is discussed below, it doesn&rsquo;t sounds that complicated. Often the papers, makes things too convoluted, but there are only a few things to keep in mind. There are a few steps, which are taken to make the model probabilistic, which will be made clear as you go through the explanations one by one and do look at the code and things won&rsquo;t seem that scary anymore.</p>
<p>Also, I was curious about how probabilistic modeling works for Neural networks, they talked some of that in MLPR, but they made it sound all too complicated. However, looking at the code it doesn&rsquo;t seem that complicated. All we do is draw weights and biases in various layers in the neural networks from a gausssian or some other distribution. Now, instead of giving a point wise prediction, we get a number of predictions for every point, which can be used to know uncertainty in predictions. Generally, non-bayesian neural network gives the mean-prediction, but now we have a distribution over prediction, where the variance term tells about the certainty or confidence of prediction. Frameworks like !(Pyro)[https://num.pyro.ai/en/stable/getting_started.html] makes it easier to implement various probabilistic models, including Bayesian Neural Netwoks. We will see an example towards the end of the note.</p>
<p>Alright; start digging.</p>
<h3 id="what-is-the-difference-between-autoencoder-and-variational-autoencoder">what is the difference between autoencoder and variational autoencoder</h3>
<p>Autoencoders and Variational Autoencoders (VAEs) are both types of neural network architectures used for unsupervised learning tasks, such as <em><strong>dimensionality reduction, representation learning, and generative modeling</strong>.</em> While they share similarities in their structure, they have different goals and properties.</p>
<ol>
<li>
<p><strong>Structure:</strong></p>
<ul>
<li>Autoencoder: Consists of an encoder and a decoder. The encoder maps the input data to a lower-dimensional latent space, and the decoder reconstructs the input data from the latent representation. The goal is to <em>minimize the reconstruction error</em>.</li>
<li>Variational Autoencoder: Also consists of an encoder and a decoder but introduces a probabilistic layer in the latent space. <em>The encoder learns to map the input data to a probability distribution (usually Gaussian) in the latent space, and the decoder reconstructs the input data from samples drawn from that distribution.</em></li>
</ul>
</li>
<li>
<p><strong>Objective:</strong></p>
<ul>
<li>Autoencoder: Minimizes the reconstruction error, which measures the difference between the input data and the data reconstructed by the autoencoder. This objective encourages the autoencoder to <em>learn a compact representation of the data in the latent space</em>.</li>
<li>Variational Autoencoder: Maximizes the Evidence Lower Bound (ELBO), which is a lower bound on the log likelihood of the observed data. The ELBO consists of the reconstruction term (similar to the autoencoder objective) and the Kullback-Leibler (KL) divergence term, which regularizes the latent space by measuring the difference between the learned distribution and a prior distribution (usually a standard normal distribution).</li>
</ul>
</li>
<li>
<p><strong>Properties:</strong></p>
<ul>
<li>Autoencoder: Learns deterministic mappings between the input data and the latent space. The <em>latent space may not be well-structured, which can make it challenging to generate new data or interpolate between data points</em>.</li>
<li>Variational Autoencoder: Learns a probabilistic mapping between the input data and the latent space. <em>The VAE encourages a more structured latent space by regularizing it with the KL divergence term, making it better suited for generative tasks, such as sampling new data points or interpolating between existing data points</em>.</li>
</ul>
</li>
</ol>
<p>In summary, <em>autoencoders focus on learning a compact and deterministic representation</em> of the data, while <em>Variational Autoencoders learn a probabilistic and structured representation that is more suitable for generative tasks</em>.</p>
<h3 id="explain-the-various-probabilistic-terminologies-used-in-vaes">explain the various probabilistic terminologies used in VAEs</h3>
<p>In Variational Autoencoders (VAEs), several probabilistic terminologies are used to describe different aspects of the model and its training process. Here is an explanation of some key probabilistic terms:</p>
<ol>
<li>
<p><strong>Latent variable:</strong> A latent variable, often denoted as z, is an unobserved or hidden variable that we assume influences the observed data. In VAEs, the latent variable captures the underlying structure of the data in a lower-dimensional space.</p>
</li>
<li>
<p><strong>Prior distribution:</strong> The prior distribution, typically denoted as p(z), represents our initial belief about the latent variables before observing any data. In VAEs, we usually assume a standard normal distribution (mean 0 and standard deviation 1) as the prior, which helps in regularizing the latent space and making it more structured.</p>
</li>
<li>
<p><strong>Likelihood:</strong> The likelihood, denoted as p(x|z), describes the probability of observing the data x given the latent variable z. In VAEs, the likelihood is modeled by the decoder network, which generates the data from the latent space.</p>
</li>
<li>
<p><strong>Posterior distribution:</strong> The posterior distribution, denoted as p(z|x), represents the updated belief about the latent variables after observing the data x. <strong>It describes the probability distribution of the latent variables conditioned on the observed data.</strong> In VAEs, the true posterior is often intractable, so we approximate it using the variational distribution.</p>
</li>
<li>
<p><strong>Variational distribution:</strong> The variational distribution, denoted as q(z|x), is an approximation of the true posterior distribution. In VAEs, it is modeled by the encoder network, which maps the input data to the latent space. The variational distribution is usually chosen to be a Gaussian distribution with <strong>learnable mean</strong> (μ) and <strong>standard deviation</strong> (σ) parameters.</p>
<ol>
<li>This is often called <strong>guide function</strong> in framework like Pyro</li>
</ol>
</li>
<li>
<p><strong>Evidence Lower Bound (ELBO):</strong> The ELBO is an objective function used in VAEs to maximize a lower bound on the log likelihood of the observed data. It consists of two terms: <strong>the reconstruction term</strong>, which encourages the VAE to reconstruct the input data accurately, and <strong>the Kullback-Leibler (KL) divergence term</strong>, which measures the difference between the variational distribution q(z|x) and the prior distribution p(z). The ELBO is given by:</p>
<p>ELBO = E_q(z|x)[log p(x|z)] - KL(q(z|x) || p(z))</p>
</li>
</ol>
<p>Maximizing the ELBO is equivalent to minimizing the difference between the true posterior and the variational distribution, leading to a better approximation of the true posterior.</p>
<p>These probabilistic terminologies form the foundation of the VAE framework, helping to describe the relationships between the observed data, latent variables, and the model&rsquo;s learning objectives.</p>
<h4 id="explain-variational-methods-in-tts-in-simple-terms">explain variational methods in TTS in simple terms</h4>
<p>Variational methods in Text-to-Speech (TTS) refer to a group of techniques that help convert written text into spoken words. In simple terms, these methods help computers generate speech that sounds natural and realistic.</p>
<p>The process involves breaking down the task into smaller steps, and the variational methods help the computer learn how to handle each step. One crucial part of TTS is learning how to pronounce words correctly and varying the speech to make it sound more human-like.</p>
<p>Variational methods help by introducing a bit of randomness or flexibility in the way the computer generates speech. This is done by creating a set of possible ways to pronounce words and then selecting the most appropriate option based on the context.</p>
<p>These methods help improve the quality of the generated speech by making it sound more natural and less robotic. They are particularly useful for creating voice assistants, audiobooks, and other applications where it is essential to have high-quality, human-like speech.</p>
<p>Variational Autoencoders are a type of neural network used to learn the underlying structure of data. In TTS, they help to model the variability in speech, such as prosody, which includes rhythm, stress, and intonation patterns.</p>
<p>Mathematically, VAEs consist of two main components: an encoder and a decoder. The encoder takes the input data (e.g., phoneme sequences) and maps it to a lower-dimensional latent space, usually represented by a Gaussian distribution with a mean (μ) and a standard deviation (σ). The decoder then takes a sample from this distribution and generates an output (e.g., speech waveform).</p>
<p>The math comes in when we define the loss function, which measures how well the VAE is performing. The loss function consists of two parts:</p>
<ol>
<li>
<p>Reconstruction loss: This measures the difference between the original input data and the data reconstructed by the VAE. It encourages the model to generate speech that closely matches the original. Mathematically, it can be defined as the mean squared error (MSE) or the negative log-likelihood (NLL) between the input and output.</p>
</li>
<li>
<p>KL divergence: This measures the difference between the distribution of the latent space and a standard Gaussian distribution (with a mean of 0 and a standard deviation of 1). It encourages the model to keep the latent space structured and prevent overfitting. Mathematically, the KL divergence is calculated as:</p>
<p>KL(P||Q) = ∫ P(x) * log(P(x) / Q(x)) dx</p>
</li>
</ol>
<p>In the context of VAEs, P represents the encoder&rsquo;s output distribution (with mean μ and standard deviation σ), and Q is the standard Gaussian distribution. The KL divergence term in the loss function helps the latent space to have a more regular structure, which makes it easier to sample and generate new data.</p>
<p>To train the VAE, we minimize the combined loss (reconstruction loss + KL divergence) using optimization algorithms like stochastic gradient descent (SGD) or Adam. This process adjusts the weights of the neural network to better generate natural-sounding speech.</p>
<p>Sure! Here&rsquo;s a simplified example of how to implement a Variational Autoencoder (VAE) for TTS using Python and the deep learning library PyTorch. Remember that a real-world implementation would involve more complexity and training data.</p>
<p>First, let&rsquo;s import the necessary libraries:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span><span class="lnt">88
</span><span class="lnt">89
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">import torch
</span></span><span class="line"><span class="cl">import torch.optim as optim
</span></span><span class="line"><span class="cl">from torch.utils.data import DataLoader, Dataset
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Next, let&#39;s define the VAE architecture, including the encoder, decoder, and  the reparameterization trick:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">class VAE(nn.Module):
</span></span><span class="line"><span class="cl">    def __init__(self, input_dim, hidden_dim, latent_dim):
</span></span><span class="line"><span class="cl">        super(VAE, self).__init__()
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        # Encoder
</span></span><span class="line"><span class="cl">        self.encoder = nn.Sequential(
</span></span><span class="line"><span class="cl">            nn.Linear(input_dim, hidden_dim),
</span></span><span class="line"><span class="cl">            nn.ReLU(),
</span></span><span class="line"><span class="cl">            nn.Linear(hidden_dim, hidden_dim),
</span></span><span class="line"><span class="cl">            nn.ReLU(),
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">		# The mean and variance are both learnable parameters
</span></span><span class="line"><span class="cl">        self.mu = nn.Linear(hidden_dim, latent_dim)
</span></span><span class="line"><span class="cl">        self.log_var = nn.Linear(hidden_dim, latent_dim)
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        # Decoder
</span></span><span class="line"><span class="cl">        self.decoder = nn.Sequential(
</span></span><span class="line"><span class="cl">            nn.Linear(latent_dim, hidden_dim),
</span></span><span class="line"><span class="cl">            nn.ReLU(),
</span></span><span class="line"><span class="cl">            nn.Linear(hidden_dim, hidden_dim),
</span></span><span class="line"><span class="cl">            nn.ReLU(),
</span></span><span class="line"><span class="cl">            nn.Linear(hidden_dim, input_dim),
</span></span><span class="line"><span class="cl">        )
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    def reparameterize(self, mu, log_var):
</span></span><span class="line"><span class="cl">        std = torch.exp(0.5 * log_var)
</span></span><span class="line"><span class="cl">        eps = torch.randn_like(std)
</span></span><span class="line"><span class="cl">        return mu + eps * std
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    def forward(self, x):
</span></span><span class="line"><span class="cl">        # Encode
</span></span><span class="line"><span class="cl">        h = self.encoder(x)
</span></span><span class="line"><span class="cl">        mu, log_var = self.mu(h), self.log_var(h)
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        # Reparameterize
</span></span><span class="line"><span class="cl">        z = self.reparameterize(mu, log_var)
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        # Decode
</span></span><span class="line"><span class="cl">        x_recon = self.decoder(z)
</span></span><span class="line"><span class="cl">        return x_recon, mu, log_var
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># Now, let&#39;s create a dataset loader for our data. In a real-world TTS scenario, you would preprocess and load the input features (e.g., mel-spectrograms) and target features (e.g., speech waveforms). Here, we&#39;ll use random data for demonstration purposes:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">class DummyDataset(Dataset):
</span></span><span class="line"><span class="cl">    def __init__(self, size, input_dim):
</span></span><span class="line"><span class="cl">        self.data = torch.randn(size, input_dim)
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    def __len__(self):
</span></span><span class="line"><span class="cl">        return len(self.data)
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    def __getitem__(self, idx):
</span></span><span class="line"><span class="cl">        return self.data[idx]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">input_dim = 128
</span></span><span class="line"><span class="cl">data_size = 1000
</span></span><span class="line"><span class="cl">dataset = DummyDataset(data_size, input_dim)
</span></span><span class="line"><span class="cl">dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># We&#39;ll also need a function to calculate the VAE loss, which includes the reconstruction loss and the KL divergence:
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">def vae_loss(x, x_recon, mu, log_var):
</span></span><span class="line"><span class="cl">    recon_loss = nn.MSELoss()(x_recon, x)
</span></span><span class="line"><span class="cl">    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
</span></span><span class="line"><span class="cl">    return recon_loss + kl_div
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">vae = VAE(input_dim, hidden_dim=256, latent_dim=64) optimizer = optim.Adam(vae.parameters(), lr=0.001)
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">epochs = 50
</span></span><span class="line"><span class="cl">for epoch in range(epochs):
</span></span><span class="line"><span class="cl">    for batch in dataloader:
</span></span><span class="line"><span class="cl">        x = batch
</span></span><span class="line"><span class="cl">        optimizer.zero_grad()
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        x_recon, mu, log_var = vae(x)
</span></span><span class="line"><span class="cl">        loss = vae_loss(x, x_recon, mu, log_var)
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        loss.backward()
</span></span><span class="line"><span class="cl">        optimizer.step()
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    print(f&#34;Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}&#34;)
</span></span></code></pre></td></tr></table>
</div>
</div><p>This code defines a training loop that runs for 50 epochs. For each epoch, we process the batches of data, perform forward propagation through the VAE, and calculate the loss. We then perform backpropagation to update the model parameters using the optimizer.</p>
<p>After the model is trained, you can use the decoder to generate speech features (e.g., mel-spectrograms) from the latent space. In a real-world TTS system, you would then convert these features into a speech waveform using a vocoder.</p>
<p>Keep in mind that this example is greatly simplified and uses dummy data. A real-world TTS implementation would involve more complex architectures, preprocessing, and larger datasets to achieve high-quality speech synthesis.</p>
<h3 id="what-is-the-reparameterization-trick-metioned-earlier">what is the reparameterization trick metioned earlier?</h3>
<p>The reparameterization trick is a technique used in Variational Autoencoders (VAEs) to enable backpropagation through the stochastic sampling process in the latent space. It allows the VAE to learn a more structured latent representation of the data while maintaining the ability to generate diverse outputs.</p>
<p>In VAEs, the encoder learns to represent the input data as a probability distribution in the latent space. During training, we sample a point from this distribution and use it as input for the decoder to generate the output. However, the sampling operation is non-differentiable, meaning that we cannot directly apply backpropagation through it.</p>
<p>The reparameterization trick addresses this issue by separating the random component from the sampling operation. Instead of directly sampling from the distribution with mean (μ) and standard deviation (σ), we sample from a standard normal distribution (with mean 0 and standard deviation 1) and scale the sampled value by σ and shift it by μ.</p>
<p>Mathematically, the reparameterization trick can be expressed as:</p>
<p>z = μ + ε * σ</p>
<p>where z is the sampled point in the latent space, μ and σ are the mean and standard deviation learned by the encoder, and ε is a random sample from the standard normal distribution.</p>
<p>By using this trick, we transform the non-differentiable sampling operation into a differentiable one. The randomness is now isolated in the ε term, while the rest of the equation is differentiable with respect to μ and σ. This allows us to apply backpropagation through the entire VAE architecture, making it possible to train the model using gradient-based optimization methods.</p>
<h3 id="what-is-sampling-operation-non-differenitable">what is sampling operation non-differenitable?</h3>
<p>In the context of Variational Autoencoders (VAEs) and other deep learning models that involve random sampling, the sampling operation is considered non-differentiable because it introduces a discrete or stochastic step in the computation, which breaks the chain of continuous and smooth operations required for gradient-based optimization methods like backpropagation.</p>
<p>To train deep learning models, we typically use gradient-based optimization algorithms that rely on computing gradients (partial derivatives) of the loss function with respect to the model&rsquo;s parameters. These gradients indicate how much each parameter should be adjusted to minimize the loss.</p>
<p>Differentiability is a crucial property required for calculating these gradients. A function is differentiable at a point if it has a derivative at that point, which means that it is smooth and has a well-defined tangent. Non-differentiable functions, on the other hand, have discontinuities, sharp turns, or points with undefined tangents, making it impossible to compute the gradients.</p>
<p>The sampling operation in VAEs is non-differentiable because it involves drawing random samples from a probability distribution. This random step introduces a discontinuity in the computation graph, and as a result, the gradients cannot be computed directly through the sampling operation.</p>
<p>The reparameterization trick, as explained earlier, helps overcome this issue by isolating the random component and making the rest of the computation differentiable. This allows us to apply gradient-based optimization methods, such as backpropagation, to train the VAE.</p>
<h3 id="what-is-the-difference-between-variational-methods-and-diffusion-methods">what is the difference between variational methods and diffusion methods?</h3>
<p>Variational methods and diffusion methods are two different approaches used in machine learning for different purposes. Here is a brief comparison of the two:</p>
<p>Variational Methods:</p>
<ol>
<li>
<p>Purpose: Variational methods are primarily used for approximate inference in probabilistic models, such as Bayesian networks or graphical models. They can also be employed in unsupervised learning tasks, such as representation learning and generative modeling, as seen in Variational Autoencoders (VAEs).</p>
</li>
<li>
<p>Approach: Variational methods work by approximating an intractable probability distribution (e.g., the true posterior) with a simpler distribution that has tractable properties. They optimize the parameters of the simpler distribution to minimize the difference (usually measured by the Kullback-Leibler divergence) between the approximated and target distributions.</p>
</li>
<li>
<p>Applications: Variational methods are widely used in Bayesian inference, unsupervised learning, and generative modeling. Examples include Variational Autoencoders (VAEs), Variational Bayesian methods, and mean-field approximations.</p>
</li>
</ol>
<p>Diffusion Methods:</p>
<ol>
<li>
<p>Purpose: Diffusion methods are a class of stochastic processes that model the random movement of particles or information through a medium. In machine learning, diffusion methods can be employed for optimization, sampling, and data generation, as seen in the recently proposed diffusion probabilistic models.</p>
</li>
<li>
<p>Approach: Diffusion methods rely on stochastic processes, such as random walks, Brownian motion, or diffusion processes. In the context of machine learning, diffusion processes can be utilized to design algorithms that involve iterative noisy updates or transitions between different states. One such example is the denoising score matching framework, where a diffusion process is used to model the generation of data from a simple noise distribution.</p>
</li>
<li>
<p>Applications: Diffusion methods have applications in optimization, sampling, data generation, and deep learning. Examples include stochastic gradient descent (SGD) optimization, Markov Chain Monte Carlo (MCMC) sampling, and diffusion probabilistic models for data generation.</p>
</li>
</ol>
<p>In summary, variational methods are primarily concerned with approximate inference and learning in probabilistic models, while diffusion methods involve stochastic processes for optimization, sampling, and data generation tasks. Both approaches have unique strengths and applications in machine learning.</p>
<h3 id="what-is-the-difference-in-the-variational-models-and-diffusion-based-models-in-tts">what is the difference in the variational models and diffusion based models in TTS?</h3>
<p>Variational models and diffusion-based models are two different approaches used in the context of Text-to-Speech (TTS) systems. While both methods have been used to develop TTS systems, they have different principles and mechanisms.</p>
<p>Variational Models in TTS:</p>
<ol>
<li>
<p>Principle: Variational models in TTS often employ Variational Autoencoders (VAEs) as the core architecture. VAEs are generative models that learn a probabilistic mapping between input text and speech features (e.g., mel-spectrograms) by optimizing an objective that balances reconstruction accuracy and latent space regularization.</p>
</li>
<li>
<p>Mechanism: VAEs in TTS consist of an encoder that maps input text to a probability distribution in the latent space and a decoder that generates speech features from samples drawn from the latent distribution. The reparameterization trick enables backpropagation through the stochastic sampling process, allowing the VAE to learn a more structured latent representation of the data.</p>
</li>
<li>
<p>Applications: Variational models have been used in TTS systems to generate more diverse and expressive speech, control prosody, and enable better disentanglement of speaker identity and linguistic content. Examples include Tacotron-VAE and Global Style Tokens (GST).</p>
</li>
</ol>
<p>Diffusion-Based Models in TTS:</p>
<ol>
<li>
<p>Principle: Diffusion-based models in TTS utilize diffusion processes or stochastic transitions to model the generation of speech features from simpler distributions. These models often employ denoising score matching frameworks or diffusion probabilistic models to learn the generation process.</p>
</li>
<li>
<p>Mechanism: Diffusion-based TTS models consist of a series of noisy transitions that progressively transform a simple noise distribution into the target speech features. The model learns a score function that captures the local structure of the data distribution and guides the diffusion process. The generation process involves reversing the diffusion process, producing speech features from noise samples.</p>
</li>
<li>
<p>Applications: Diffusion-based models have been used in TTS systems to generate high-quality speech and model complex dependencies between text and speech features. Examples include WaveGrad and Text-to-Diffusion.</p>
</li>
</ol>
<p>In summary, variational models in TTS rely on Variational Autoencoders to learn a probabilistic mapping between input text and speech features, while diffusion-based models utilize diffusion processes or stochastic transitions to model the generation of speech features from simpler distributions. Both approaches have been successful in generating high-quality and expressive speech in TTS systems.</p>
<h2 id="probabilistic-graphical-modeling-view-of-neural-networks">Probabilistic graphical modeling view of neural networks?</h2>
<p>Probabilistic graphical models (PGMs) are a powerful framework for modeling complex systems by representing joint probability distributions over multiple random variables. They use graphs to describe the conditional dependencies between variables, allowing for efficient inference and learning. Neural networks, on the other hand, are a class of machine learning models that consist of interconnected nodes or neurons that transform input data into desired outputs.</p>
<p>Although neural networks are not inherently probabilistic models, they can be viewed from a probabilistic graphical modeling perspective. In this view, the neural network&rsquo;s nodes represent random variables, and the connections between nodes represent conditional dependencies between these variables. The weights and biases in the network can be seen as parameters governing these dependencies.</p>
<p>To build a probabilistic graphical model of a neural network, we can follow these steps:</p>
<ol>
<li>
<p>Define random variables: Each node in the neural network corresponds to a random variable. For example, in a feedforward neural network, each neuron in the input, hidden, and output layers can be considered a random variable.</p>
</li>
<li>
<p>Define conditional dependencies: The connections between nodes in the neural network represent the conditional dependencies between the corresponding random variables. For example, the output of a neuron depends on the weighted sum of its inputs, passed through an activation function.</p>
</li>
<li>
<p>Define probability distributions: Assign probability distributions to the random variables in the graphical model. In a Bayesian neural network, the weights and biases can be assigned prior distributions (e.g., Gaussian distributions), and the output random variables can be assigned likelihood distributions (e.g., Gaussian for regression or categorical for classification).</p>
</li>
<li>
<p>Perform inference and learning: Once the probabilistic graphical model is constructed, we can perform inference and learning tasks using PGM techniques, such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, or variational inference.</p>
</li>
</ol>
<p>This probabilistic view of neural networks allows us to treat them as generative models, where the goal is to model the joint probability distribution of the input data and the outputs (or targets). It enables us to incorporate uncertainty into the model&rsquo;s predictions and learn more robust representations. Bayesian neural networks, for instance, use this perspective to model uncertainty in the weights and biases, leading to more robust and interpretable predictions.</p>
<p>In summary, although neural networks are not explicitly designed as probabilistic models, they can be viewed and analyzed from a probabilistic graphical modeling perspective, offering insights into their behavior and enabling the incorporation of uncertainty and robustness into their predictions.</p>
<h3 id="is-probabilistic-graphical-models-a-pre-requisitite-to-learn-generative-models">is probabilistic graphical models a pre-requisitite to learn generative models?</h3>
<p>Probabilistic graphical models (PGMs) are not a strict prerequisite for learning generative models, but having a good understanding of PGMs can be beneficial. Generative models aim to learn the underlying data distribution and can generate new samples that resemble the training data. While some generative models, such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs), are based on PGMs, there are other popular generative models that do not explicitly rely on PGMs, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs).</p>
<p>Understanding PGMs can provide you with a strong foundation in probability theory, Bayesian inference, and statistical modeling, which can be helpful when studying generative models. It will allow you to grasp the underlying principles and assumptions behind many generative models, as well as the inference and learning techniques used to train them.</p>
<p>However, it is possible to learn about generative models without a deep understanding of PGMs, especially if you focus on models like VAEs and GANs, which are primarily based on neural networks. In this case, having a good understanding of deep learning concepts, optimization techniques, and neural network architectures is more crucial.</p>
<p>In summary, while understanding PGMs is beneficial for learning generative models, it is not a strict prerequisite. You can still learn about generative models, especially those based on neural networks, by focusing on deep learning concepts and techniques. Nevertheless, having a strong foundation in probability theory and PGMs can enhance your understanding of generative models and help you develop a more comprehensive perspective on their principles and applications.</p>
<h3 id="explain-bayesian-neural-network-using-simple-pytorch-code">explain bayesian neural network using simple pytorch code</h3>
<p>A Bayesian Neural Network (BNN) is a neural network that incorporates uncertainty by treating the weights and biases as random variables with prior distributions. In this example, we&rsquo;ll demonstrate how to create a simple BNN for regression using PyTorch. We&rsquo;ll use a toy dataset, the <code>sin(x)</code> function with added noise, for illustration.</p>
<p>First, let&rsquo;s import the necessary libraries and generate the toy dataset:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span><span class="lnt">82
</span><span class="lnt">83
</span><span class="lnt">84
</span><span class="lnt">85
</span><span class="lnt">86
</span><span class="lnt">87
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-mysql" data-lang="mysql"><span class="line"><span class="cl"><span class="n">import</span><span class="w"> </span><span class="n">torch</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">import</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">nn</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">import</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">optim</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">import</span><span class="w"> </span><span class="n">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">np</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">import</span><span class="w"> </span><span class="n">matplotlib</span><span class="p">.</span><span class="n">pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">plt</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Generate toy dataset
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">def</span><span class="w"> </span><span class="nf">generate_toy_data</span><span class="p">(</span><span class="n">num_points</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">generate_toy_data</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">x_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">y_tensor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Next, we define a simple neural network with one hidden layer and a `tanh` activation function:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">class</span><span class="w"> </span><span class="nf">SimpleBNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_dim</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nf">super</span><span class="p">(</span><span class="n">SimpleBNN</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">fc1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="nf">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">fc2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nn</span><span class="p">.</span><span class="nf">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_dim</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">input_dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">hidden_dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">output_dim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">SimpleBNN</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="w"> </span><span class="n">hidden_dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_dim</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Now, we&#39;ll define the prior distribution for the weights and biases. In this example, we&#39;ll use Gaussian distributions with mean 0 and standard deviation 1:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">def</span><span class="w"> </span><span class="nf">gaussian_prior</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">log_prob</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">w</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">log_prob</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">def</span><span class="w"> </span><span class="nf">compute_log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">log_prior</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">():</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="n">log_prior</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nf">gaussian_prior</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">mu</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">log_prior</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># For the likelihood, we&#39;ll use a Gaussian distribution with the mean given by the model&#39;s output and a fixed standard deviation of 0.5:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">def</span><span class="w"> </span><span class="nf">compute_log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">y_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">log_likelihood</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">y_pred</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">0</span><span class="p">.</span><span class="mi">5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">log_likelihood</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Now, we&#39;ll define the loss function as the negative log-posterior, which is the sum of the negative log-prior and the negative log-likelihood:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">def</span><span class="w"> </span><span class="nf">loss_function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">log_prior</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">compute_log_prior</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">log_likelihood</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">compute_log_likelihood</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="p">(</span><span class="n">log_prior</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">log_likelihood</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">loss</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Finally, we&#39;ll train the model using stochastic gradient descent:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">optimizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">optim</span><span class="p">.</span><span class="nf">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span><span class="w"> </span><span class="n">lr</span><span class="o">=</span><span class="mi">0</span><span class="p">.</span><span class="mi">01</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">num_epochs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="k">for</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">loss_function</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">x_tensor</span><span class="p">,</span><span class="w"> </span><span class="n">y_tensor</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">	</span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="mi">100</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span><span class="w">         
</span></span></span><span class="line"><span class="cl"><span class="w">		</span><span class="nf">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&#34;Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}&#34;</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Now that the model is trained, let&#39;s use it for prediction and plot the results:
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="c1"># Test data for prediction
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="n">x_test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">y_test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">model</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Training Data&#34;</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="w"> </span><span class="n">y_test</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">,</span><span class="w"> </span><span class="n">label</span><span class="o">=</span><span class="s2">&#34;Predictions&#34;</span><span class="p">)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="n">plt</span><span class="p">.</span><span class="k">show</span><span class="p">()</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>This code demonstrates a simple example of a Bayesian Neural Network using PyTorch. Note that this is not a fully Bayesian approach, as we are using point estimates of the weights and biases instead of sampling from their posterior distributions. To implement a more accurate Bayesian approach, you could use techniques such as Markov Chain Monte Carlo (MCMC) or Variational Inference.</p>
<p>Additionally, more sophisticated BNN libraries like Pyro (built on top of PyTorch) can be used to implement BNNs with more advanced features and better handling of the Bayesian aspect.</p>
<h3 id="show-a-more-accurate-bayesian-approach-you-mentioned-above-for-the-same-example">show a more accurate bayesian approach you mentioned above for the same example</h3>
<p>In this example, we&rsquo;ll use the Pyro library, which is built on top of PyTorch, to implement a more accurate Bayesian approach for our toy dataset. We&rsquo;ll use the same dataset and neural network architecture as before.</p>
<p>First, let&rsquo;s import the necessary libraries and generate the toy dataset:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">  1
</span><span class="lnt">  2
</span><span class="lnt">  3
</span><span class="lnt">  4
</span><span class="lnt">  5
</span><span class="lnt">  6
</span><span class="lnt">  7
</span><span class="lnt">  8
</span><span class="lnt">  9
</span><span class="lnt"> 10
</span><span class="lnt"> 11
</span><span class="lnt"> 12
</span><span class="lnt"> 13
</span><span class="lnt"> 14
</span><span class="lnt"> 15
</span><span class="lnt"> 16
</span><span class="lnt"> 17
</span><span class="lnt"> 18
</span><span class="lnt"> 19
</span><span class="lnt"> 20
</span><span class="lnt"> 21
</span><span class="lnt"> 22
</span><span class="lnt"> 23
</span><span class="lnt"> 24
</span><span class="lnt"> 25
</span><span class="lnt"> 26
</span><span class="lnt"> 27
</span><span class="lnt"> 28
</span><span class="lnt"> 29
</span><span class="lnt"> 30
</span><span class="lnt"> 31
</span><span class="lnt"> 32
</span><span class="lnt"> 33
</span><span class="lnt"> 34
</span><span class="lnt"> 35
</span><span class="lnt"> 36
</span><span class="lnt"> 37
</span><span class="lnt"> 38
</span><span class="lnt"> 39
</span><span class="lnt"> 40
</span><span class="lnt"> 41
</span><span class="lnt"> 42
</span><span class="lnt"> 43
</span><span class="lnt"> 44
</span><span class="lnt"> 45
</span><span class="lnt"> 46
</span><span class="lnt"> 47
</span><span class="lnt"> 48
</span><span class="lnt"> 49
</span><span class="lnt"> 50
</span><span class="lnt"> 51
</span><span class="lnt"> 52
</span><span class="lnt"> 53
</span><span class="lnt"> 54
</span><span class="lnt"> 55
</span><span class="lnt"> 56
</span><span class="lnt"> 57
</span><span class="lnt"> 58
</span><span class="lnt"> 59
</span><span class="lnt"> 60
</span><span class="lnt"> 61
</span><span class="lnt"> 62
</span><span class="lnt"> 63
</span><span class="lnt"> 64
</span><span class="lnt"> 65
</span><span class="lnt"> 66
</span><span class="lnt"> 67
</span><span class="lnt"> 68
</span><span class="lnt"> 69
</span><span class="lnt"> 70
</span><span class="lnt"> 71
</span><span class="lnt"> 72
</span><span class="lnt"> 73
</span><span class="lnt"> 74
</span><span class="lnt"> 75
</span><span class="lnt"> 76
</span><span class="lnt"> 77
</span><span class="lnt"> 78
</span><span class="lnt"> 79
</span><span class="lnt"> 80
</span><span class="lnt"> 81
</span><span class="lnt"> 82
</span><span class="lnt"> 83
</span><span class="lnt"> 84
</span><span class="lnt"> 85
</span><span class="lnt"> 86
</span><span class="lnt"> 87
</span><span class="lnt"> 88
</span><span class="lnt"> 89
</span><span class="lnt"> 90
</span><span class="lnt"> 91
</span><span class="lnt"> 92
</span><span class="lnt"> 93
</span><span class="lnt"> 94
</span><span class="lnt"> 95
</span><span class="lnt"> 96
</span><span class="lnt"> 97
</span><span class="lnt"> 98
</span><span class="lnt"> 99
</span><span class="lnt">100
</span><span class="lnt">101
</span><span class="lnt">102
</span><span class="lnt">103
</span><span class="lnt">104
</span><span class="lnt">105
</span><span class="lnt">106
</span><span class="lnt">107
</span><span class="lnt">108
</span><span class="lnt">109
</span><span class="lnt">110
</span><span class="lnt">111
</span><span class="lnt">112
</span><span class="lnt">113
</span><span class="lnt">114
</span><span class="lnt">115
</span><span class="lnt">116
</span><span class="lnt">117
</span><span class="lnt">118
</span><span class="lnt">119
</span><span class="lnt">120
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-v" data-lang="v"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">torch</span><span class="p">.</span><span class="nv">nn</span> <span class="k">as</span> <span class="nv">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">numpy</span> <span class="k">as</span> <span class="nv">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">matplotlib</span><span class="p">.</span><span class="nv">pyplot</span> <span class="k">as</span> <span class="nv">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">pyro</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nv">pyro</span><span class="p">.</span><span class="nv">distributions</span> <span class="k">as</span> <span class="nv">dist</span>
</span></span><span class="line"><span class="cl"><span class="nv">from</span> <span class="nv">pyro</span><span class="p">.</span><span class="nv">infer</span> <span class="kn">import</span> <span class="nc">SVI</span><span class="p">,</span> <span class="nc">Trace_ELBO</span>
</span></span><span class="line"><span class="cl"><span class="nv">from</span> <span class="nv">pyro</span><span class="p">.</span><span class="nv">optim</span> <span class="kn">import</span> <span class="nc">Adam</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">#</span> <span class="nc">Generate</span> <span class="nv">toy</span> <span class="nv">dataset</span>
</span></span><span class="line"><span class="cl"><span class="nv">def</span> <span class="nf">generate_toy_data</span><span class="p">(</span><span class="nv">num_points</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nv">np</span><span class="p">.</span><span class="nv">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">x</span> <span class="o">=</span> <span class="nv">np</span><span class="p">.</span><span class="nv">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="nv">size</span><span class="o">=</span><span class="p">(</span><span class="nv">num_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nv">y</span> <span class="o">=</span> <span class="nv">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="nv">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="nv">np</span><span class="p">.</span><span class="nv">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="nv">size</span><span class="o">=</span><span class="p">(</span><span class="nv">num_points</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nv">x</span><span class="p">,</span> <span class="nv">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">x</span><span class="p">,</span> <span class="nv">y</span> <span class="o">=</span> <span class="nf">generate_toy_data</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nv">x_tensor</span> <span class="o">=</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nv">x</span><span class="p">,</span> <span class="nv">dtype</span><span class="o">=</span><span class="nv">torch</span><span class="p">.</span><span class="nv">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">y_tensor</span> <span class="o">=</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nv">y</span><span class="p">,</span> <span class="nv">dtype</span><span class="o">=</span><span class="nv">torch</span><span class="p">.</span><span class="nv">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">#</span> <span class="nc">Next</span><span class="p">,</span> <span class="nv">let</span><span class="s1">&#39;s define the same neural network architecture as before:
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">class SimpleBNN(nn.Module):
</span></span></span><span class="line"><span class="cl"><span class="s1">    def __init__(self, input_dim, hidden_dim, output_dim):
</span></span></span><span class="line"><span class="cl"><span class="s1">        super(SimpleBNN, self).__init__()
</span></span></span><span class="line"><span class="cl"><span class="s1">        self.fc1 = nn.Linear(input_dim, hidden_dim)
</span></span></span><span class="line"><span class="cl"><span class="s1">        self.fc2 = nn.Linear(hidden_dim, output_dim)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    def forward(self, x):
</span></span></span><span class="line"><span class="cl"><span class="s1">        x = torch.tanh(self.fc1(x))
</span></span></span><span class="line"><span class="cl"><span class="s1">        x = self.fc2(x)
</span></span></span><span class="line"><span class="cl"><span class="s1">        return x
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1"># Now we&#39;</span><span class="nv">ll</span> <span class="nv">define</span> <span class="nv">the</span> <span class="nv">model</span> <span class="nv">and</span> <span class="nv">guide</span> <span class="nv">functions</span> <span class="k">for</span> <span class="nc">Pyro</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">def</span> <span class="nf">model</span><span class="p">(</span><span class="nv">x</span><span class="p">,</span> <span class="nv">y</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="o">#</span> <span class="nc">Set</span> <span class="nv">prior</span> <span class="nv">distributions</span> <span class="k">for</span> <span class="nv">the</span> <span class="nv">weights</span> <span class="nv">and</span> <span class="nv">biases</span>
</span></span><span class="line"><span class="cl">    <span class="nv">fc1_weight_prior</span> <span class="o">=</span> <span class="nv">dist</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="nv">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">weight</span><span class="p">),</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">weight</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nv">fc1_bias_prior</span> <span class="o">=</span> <span class="nv">dist</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="nv">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">bias</span><span class="p">),</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">bias</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nv">fc2_weight_prior</span> <span class="o">=</span> <span class="nv">dist</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="nv">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">weight</span><span class="p">),</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">weight</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="nv">fc2_bias_prior</span> <span class="o">=</span> <span class="nv">dist</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="nv">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">bias</span><span class="p">),</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="nv">model</span><span class="p">.</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">bias</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">priors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fc1.weight&#39;</span><span class="p">:</span> <span class="nv">fc1_weight_prior</span><span class="p">,</span> <span class="s1">&#39;fc1.bias&#39;</span><span class="p">:</span> <span class="nv">fc1_bias_prior</span><span class="p">,</span> <span class="s1">&#39;fc2.weight&#39;</span><span class="p">:</span> <span class="nv">fc2_weight_prior</span><span class="p">,</span> <span class="s1">&#39;fc2.bias&#39;</span><span class="p">:</span> <span class="nv">fc2_bias_prior</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="nv">lifted_module</span> <span class="o">=</span> <span class="nv">pyro</span><span class="p">.</span><span class="nf">random_module</span><span class="p">(</span><span class="s2">&#34;module&#34;</span><span class="p">,</span> <span class="nv">model</span><span class="p">,</span> <span class="nv">priors</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nv">lifted_model</span> <span class="o">=</span> <span class="nf">lifted_module</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nv">with</span> <span class="nv">pyro</span><span class="p">.</span><span class="nf">plate</span><span class="p">(</span><span class="s2">&#34;map&#34;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nv">x</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="o">#</span> <span class="nc">Model</span><span class="s1">&#39;s output
</span></span></span><span class="line"><span class="cl"><span class="s1">        prediction_mean = lifted_model(x).squeeze(-1)
</span></span></span><span class="line"><span class="cl"><span class="s1">        # Likelihood
</span></span></span><span class="line"><span class="cl"><span class="s1">        pyro.sample(&#34;obs&#34;, dist.Normal(prediction_mean, 0.5), obs=y)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">def guide(x, y):
</span></span></span><span class="line"><span class="cl"><span class="s1">    # Variational distributions for the weights and biases
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_weight_loc = torch.randn_like(model.fc1.weight)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_weight_scale = torch.rand_like(model.fc1.weight)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_bias_loc = torch.randn_like(model.fc1.bias)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_bias_scale = torch.rand_like(model.fc1.bias)
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_weight_loc = torch.randn_like(model.fc2.weight)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_weight_scale = torch.rand_like(model.fc2.weight)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_bias_loc = torch.randn_like(model.fc2.bias)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_bias_scale = torch.rand_like(model.fc2.bias)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_weight_q = pyro.param(&#34;fc1_weight_q&#34;, fc1_weight_loc)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_weight_scale_q = pyro.param(&#34;fc1_weight_scale_q&#34;, fc1_weight_scale, constraint=dist.constraints.positive)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_bias_q = pyro.param(&#34;fc1_bias_q&#34;, fc1_bias_loc)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_bias_scale_q = pyro.param(&#34;fc1_bias_scale_q&#34;, fc1_bias_scale, constraint=dist.constraints.positive)
</span></span></span><span class="line"><span class="cl"><span class="s1">    
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_weight_q = pyro.param(&#34;fc2_weight_q&#34;, fc2_weight_loc)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_weight_scale_q = pyro.param(&#34;fc2_weight_scale_q&#34;, fc2_weight_scale, constraint=dist.constraints.positive)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_bias_q = pyro.param(&#34;fc2_bias_q&#34;, fc2_bias_loc)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_bias_scale_q = pyro.param(&#34;fc2_bias_scale_q&#34;, fc2_bias_scale, constraint=dist.constraints.positive)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_weight = dist.Normal(fc1_weight_q, fc1_weight_scale_q)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc1_bias = dist.Normal(fc1_bias_q, fc1_bias_scale_q)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_weight = dist.Normal(fc2_weight_q, fc2_weight_scale_q)
</span></span></span><span class="line"><span class="cl"><span class="s1">    fc2_bias = dist.Normal(fc2_bias_q, fc2_bias_scale_q)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">    posterior = {&#39;</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">weight</span><span class="s1">&#39;: fc1_weight, &#39;</span><span class="nv">fc1</span><span class="p">.</span><span class="nv">bias</span><span class="s1">&#39;: fc1_bias, &#39;</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">weight</span><span class="s1">&#39;: fc2_weight, &#39;</span><span class="nv">fc2</span><span class="p">.</span><span class="nv">bias</span><span class="s1">&#39;: fc2_bias}
</span></span></span><span class="line"><span class="cl"><span class="s1">    lifted_module = pyro.random_module(&#34;module&#34;, model, posterior)
</span></span></span><span class="line"><span class="cl"><span class="s1">    return lifted_module()
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1"># Instantiate the model
</span></span></span><span class="line"><span class="cl"><span class="s1">input_dim = 1
</span></span></span><span class="line"><span class="cl"><span class="s1">hidden_dim = 10
</span></span></span><span class="line"><span class="cl"><span class="s1">output_dim = 1
</span></span></span><span class="line"><span class="cl"><span class="s1">model = SimpleBNN(input_dim, hidden_dim, output_dim)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1"># Set up the optimizer and the inference algorithm (SVI)
</span></span></span><span class="line"><span class="cl"><span class="s1">optimizer = Adam({&#34;lr&#34;: 0.01})
</span></span></span><span class="line"><span class="cl"><span class="s1">svi = SVI(model, guide, optimizer, loss=Trace_ELBO())
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1"># Training loop
</span></span></span><span class="line"><span class="cl"><span class="s1">num_epochs = 1000
</span></span></span><span class="line"><span class="cl"><span class="s1">for epoch in range(num_epochs):
</span></span></span><span class="line"><span class="cl"><span class="s1">    loss = svi.step(x_tensor, y_tensor)
</span></span></span><span class="line"><span class="cl"><span class="s1">    if (epoch + 1) % 100 == 0:
</span></span></span><span class="line"><span class="cl"><span class="s1">        print(f&#34;Epoch [{epoch+1}/{num_epochs}], Loss: {loss/len(x_tensor)}&#34;)
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1">
</span></span></span><span class="line"><span class="cl"><span class="s1"># Finally, let&#39;</span><span class="nv">s</span> <span class="nv">use</span> <span class="nv">the</span> <span class="nv">trained</span> <span class="nv">model</span> <span class="k">for</span> <span class="nv">prediction</span> <span class="nv">and</span> <span class="nv">plot</span> <span class="nv">the</span> <span class="nl">results</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">def</span> <span class="nf">predict</span><span class="p">(</span><span class="nv">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nv">sampled_models</span> <span class="o">=</span> <span class="p">[</span><span class="nf">guide</span><span class="p">(</span><span class="nc">None</span><span class="p">,</span> <span class="nc">None</span><span class="p">)</span> <span class="k">for</span> <span class="nv">_</span> <span class="k">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl">    <span class="nv">y_preds</span> <span class="o">=</span> <span class="p">[</span><span class="nf">model</span><span class="p">(</span><span class="nv">x</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="nv">model</span> <span class="k">in</span> <span class="nv">sampled_models</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="nv">np</span><span class="p">.</span><span class="nb">array</span><span class="p">(</span><span class="nv">y_preds</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">x_test</span> <span class="o">=</span> <span class="nv">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="nv">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="nv">num</span><span class="o">=</span><span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="nv">dtype</span><span class="o">=</span><span class="nv">torch</span><span class="p">.</span><span class="nv">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">y_test</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="nv">x_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">y_mean</span> <span class="o">=</span> <span class="nv">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="nv">y_test</span><span class="p">,</span> <span class="nv">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">y_std</span> <span class="o">=</span> <span class="nv">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="nv">y_test</span><span class="p">,</span> <span class="nv">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nv">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="nv">x</span><span class="p">,</span> <span class="nv">y</span><span class="p">,</span> <span class="nv">label</span><span class="o">=</span><span class="s2">&#34;Training Data&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nv">x_test</span><span class="p">,</span> <span class="nv">y_mean</span><span class="p">,</span> <span class="nv">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="nv">label</span><span class="o">=</span><span class="s2">&#34;Predictions&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="nv">x_test</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="nv">y_mean</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="o">-</span><span class="nv">y_std</span><span class="p">,</span> <span class="nv">y_mean</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()</span><span class="o">+</span><span class="nv">y_std</span><span class="p">,</span> <span class="nv">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="nv">color</span><span class="o">=</span><span class="s2">&#34;red&#34;</span><span class="p">,</span> <span class="nv">label</span><span class="o">=</span><span class="s2">&#34;Uncertainty&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nv">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nv">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>In this example, we&rsquo;ve used Pyro to implement a more accurate Bayesian approach to our neural network. We&rsquo;ve defined the model and guide functions, and used Stochastic Variational Inference (SVI) for training. The predictions now include uncertainty, as we sample multiple models from the variational posterior and average their predictions.</p>
<ul>
<li>

<a target=“_blank” href="https://medium.com/paper-club/understanding-pyros-model-and-guide-a-love-story-7f3aa0603886" rel="noopener">https://medium.com/paper-club/understanding-pyros-model-and-guide-a-love-story-7f3aa0603886</a></li>
<li>Bayesian Neural Network: 

<a target=“_blank” href="https://num.pyro.ai/en/stable/examples/bnn.html" rel="noopener">https://num.pyro.ai/en/stable/examples/bnn.html</a></li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/QS/Week10/PCA-Notes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/QS/Week10/PCA-Notes/</guid><description>PCA Algo The algorithm for PCA can be broken down into the following steps:
Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all features are on the same scale.</description><content:encoded><![CDATA[<h2 id="pca-algo">PCA Algo</h2>
<p>

<img src="/Screenshot%202023-04-30%20at%2010.25.38%20PM.png" alt="Screenshot 2023-04-30 at 10.25.38 PM.png"  /></p>
<p>The algorithm for PCA can be broken down into the following steps:</p>
<ol>
<li>Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all features are on the same scale.</li>
</ol>
<p>**2.  Compute the covariance matrix: Calculate the covariance matrix of the standardized data.</p>
<p><strong>3.  Compute the eigenvectors and eigenvalues of the covariance matrix: Use eigenvalue decomposition to find the eigenvectors and eigenvalues of the covariance matrix.</strong></p>
<p><strong>4.  Sort the eigenvectors by decreasing eigenvalues: Arrange the eigenvectors in order of decreasing eigenvalues to identify the principal components.</strong></p>
<p><strong>5.  Choose the number of principal components: Determine how many principal components to retain based on the amount of variance explained and/or the desired dimensionality reduction.</strong></p>
<ol start="6">
<li>Project the data onto the principal components: Multiply the standardized data by the matrix of eigenvectors corresponding to the top k principal components to obtain the k-dimensional representation of the data.
<ol>
<li>In part 4. of the screenshot -&gt; do this instead cuz it does not highlight the fact that we are using M principal component i.e. the M principal eigen values for representing the data.</li>
</ol>
</li>
</ol>
<p>The key step in this algorithm is step 3, which involves computing the eigenvectors and eigenvalues of the covariance matrix. This can be done using eigenvalue decomposition, which decomposes the covariance matrix into a product of eigenvectors and eigenvalues.</p>
<p>Mathematically, this can be expressed as:</p>
<p>$\Sigma=Q\Lambda Q^{-1}$</p>
<p>where $\Sigma$ is the covariance matrix, $Q$ is the matrix of eigenvectors, and $\Lambda$ is the diagonal matrix of eigenvalues.</p>
<p>To compute the eigenvectors and eigenvalues, we can use an algorithm such as the power iteration method, which iteratively estimates the largest eigenvector and eigenvalue. Once the largest eigenvector has been found, it can be removed from the data and the process can be repeated to find the next largest eigenvector and eigenvalue.</p>
<p>Overall, eigenvalue decomposition is a critical part of the PCA algorithm as it enables us to find the eigenvectors and eigenvalues of the covariance matrix, which in turn allows us to identify the principal components of the data.</p>
<h3 id="here-is-a-step-by-step-process-for-performing-pca-and-spatial-whitening-on-a-given-dataset">Here is a step-by-step process for performing PCA and spatial whitening on a given dataset:</h3>
<ol>
<li>Given a dataset $X$ of $n$ samples each having $d$ dimensions, compute the mean of the data and subtract it from each sample to center the data:</li>
</ol>
<p>$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i \
x_i \leftarrow x_i - \bar{x}
$$</p>
<ol start="2">
<li>Compute the covariance matrix $\Sigma$ of the centered data:</li>
</ol>
<p>$$
\Sigma = \frac{1}{n} \sum_{i=1}^{n} x_i x_i^T
$$</p>
<ol start="3">
<li>Compute the eigendecomposition of the covariance matrix $\Sigma$:</li>
</ol>
<p>$$
\Sigma = U \Lambda U^T
$$</p>
<p>where $\Lambda$ is a diagonal matrix of the eigenvalues and $U$ is a matrix of the corresponding eigenvectors. Note that the eigenvectors in $U$ are already orthonormal.</p>
<ol start="4">
<li>Sort the eigenvectors in $U$ in descending order of their corresponding eigenvalues in $\Lambda$ to obtain a matrix $W$:</li>
</ol>
<p>$$
W = [u_1, u_2, &hellip;, u_d]
$$</p>
<p>where $u_i$ is the eigenvector corresponding to the $i$th largest eigenvalue.</p>
<ol start="5">
<li>Project the centered data onto the new basis defined by $W$ to obtain the transformed data $Z$:</li>
</ol>
<p>$$
Z = XW
$$
<strong>Now -&gt; The step 5 in the previous answer only guarantees zero covariance between dimensions. To ensure unit variance along all dimensions, the covariance matrix of the whitened data needs to be normalized to the identity matrix</strong></p>
<ol start="6">
<li>Compute the whitening matrix $W_{white}$ as:</li>
</ol>
<p>$$
W_{white} = \Lambda^{-1/2} U^T
$$</p>
<ol start="7">
<li>Compute the whitened data $Z_{white}$ as:</li>
</ol>
<p>$$
Z_{white} = ZW_{white}
$$</p>
<p>Now, the whitened data $Z_{white}$ has a covariance matrix equal to the identity matrix, indicating that the dimensions are uncorrelated and have unit variance.</p>
<p>Note that the spacial whitening is applied after performing PCA, as it is necessary to first transform the data into a new basis before computing the whitening matrix.</p>
<p>In Summary:</p>
<p><strong>Spatial whitening is typically applied after performing PCA</strong>. This is because PCA decorrelates the data, but it does not necessarily produce data with unit variance along all dimensions. By applying spatial whitening after PCA, we can ensure that the resulting transformed data has unit variance along all dimensions, which can be useful for certain applications such as gradient descent optimization.</p>
<p>After performing PCA, the eigenvalues of the covariance matrix are used to normalize the data, resulting in a new set of transformed data points. These transformed data points are then further modified through the process of spatial whitening, which results in data that has unit variance along all dimensions.</p>
<h1 id="independent-component-analysis-ica">Independent Component Analysis (ICA)</h1>
<p>PCA aims to retain the max variance in the components. However, these components may not be meaningful for the purpose of for example classification; as the component which has maximum variance, may not be the direction in which we have maximum separation of the data.</p>
<ul>
<li>
<p>Non-Gaussianity: Non-Gaussianity refers to the property of a distribution that does not follow a Gaussian or normal distribution.</p>
</li>
<li>
<p><strong>Independent Component Analysis (ICA)</strong> is a technique that is particularly suited for dealing with non-Gaussian data. The <em>basic idea behind ICA is to identify a set of independent sources that underlie the observed data, by exploiting the non-Gaussianity of the sources</em>.</p>
<ul>
<li>The assumption is that the data can be expressed as a linear combination of these sources, and that the sources are mutually independent.</li>
</ul>
</li>
</ul>
<p>Independent Component Analysis (ICA) is a technique used to extract independent signals from a set of mixed signals. It is similar to PCA but it assumes that the sources are not only linearly correlated but also statistically independent. ICA is often used in signal processing and machine learning applications such as image and speech recognition.</p>
<p><strong>ICA can be performed on a set of mixed signals by following these steps:</strong></p>
<p>ICA algorithm does not require PCA, but it can be useful to apply PCA or spacial whitening as preprocessing steps before performing ICA. The purpose of these preprocessing steps is to decorrelate the data and reduce the dimensionality of the problem, which can make the ICA algorithm more computationally efficient and more effective.</p>
<p>Here is the complete process for performing ICA with PCA and spacial whitening:</p>
<ol>
<li>Center the data: Subtract the mean of each feature from each data point to center the data around zero.</li>
</ol>
<p>$$x_{centered} = x - \mu_x$$</p>
<ol start="2">
<li>Perform PCA: Compute the principal components of the centered data using eigenvalue decomposition of the covariance matrix.</li>
</ol>
<p>$$\Sigma = U \Lambda U^T$$</p>
<p>where $\Sigma$ is the covariance matrix, $U$ is the matrix of eigenvectors, and $\Lambda$ is the diagonal matrix of eigenvalues.</p>
<ol start="3">
<li>Whiten the data: Transform the centered data into a new space where the covariance matrix is the identity matrix by multiplying it by the inverse square root of the diagonal matrix of eigenvalues and the transpose of the matrix of eigenvectors.</li>
</ol>
<p>$$x_{whitened} = \Lambda^{-1/2} U^T x_{centered}$$</p>
<ol start="4">
<li>
<p>Apply ICA to the pre-processed data. This involves finding a set of linear transformations, represented by a matrix $\mathbf{W}$, that decorrelates the columns of $x_{whitened}$ and maximizes the non-Gaussianity of the transformed data.</p>
</li>
<li>
<p>Retrieve the original sources: Compute the original sources by multiplying the mixing matrix by the whitened data.</p>
</li>
</ol>
<p>Originally we have $$ x = \mathbf{A}s $$, where A is an unknow invertible transform, we aim to learn $\textbf{W}$ which is $\mathbf{A^{-1}}$, such that$$ s = \mathbf{W} x_{whitened}$$
Note that step 2 involves the transpose of the matrix of eigenvectors, $U^T$, which is used in step 3 to transform the data into the whitened space. This transpose operation is necessary to ensure that the columns of $U$ form an orthonormal basis for the whitened space.</p>
<h3 id="ambiguities-in-ica">Ambiguities in ICA</h3>
<ol>
<li><strong>Ambiguity in determining the variances of the independent components</strong></li>
</ol>
<p>In ICA, the variances of the estimated independent components are not uniquely determined, and this is known as the variance indeterminacy problem. This ambiguity arises because the independent components can be scaled by arbitrary factors without affecting their independence.</p>
<p>Mathematically, if we have a set of independent components $s_1, s_2, &hellip;, s_k$ and their corresponding mixing coefficients $A_{i,j}$, then we can scale each independent component $s_i$ by an arbitrary factor $\alpha_i$ to obtain a new set of independent components $s_1&rsquo;, s_2&rsquo;, &hellip;, s_k&rsquo;$:</p>
<p>$$s_i&rsquo; = \alpha_i s_i$$</p>
<p>The corresponding mixing coefficients for the new set of independent components will be:</p>
<p>$$A&rsquo;<em>{i,j} = \frac{A</em>{i,j}}{\alpha_i}$$</p>
<p>It can be shown that the new set of independent components $s_1&rsquo;, s_2&rsquo;, &hellip;, s_k&rsquo;$ is also independent and satisfies the same linear mixing model as the original set of independent components $s_1, s_2, &hellip;, s_k$.</p>
<p>Therefore, the variance of the independent components cannot be uniquely determined because any scaling factor applied to the independent components will affect their variances. However, in practice, the scaling factor can be normalized to make the variance of the independent components equal to one or any other desired value, making the indeterminacy of the variances less of a concern. <strong>(The step we take during whitenening step, helps resolve this as it ensures unit variances across all the independent components)</strong></p>
<ol start="2">
<li><strong>Ambiguity of order of independent components</strong></li>
</ol>
<p>The ambiguity of order of independent components refers to the fact that the order in which the independent components are estimated by the ICA algorithm is arbitrary. In other words, the independent components can be returned in any order, and this order may not be meaningful or interpretable.</p>
<p>This ambiguity can be resolved by using additional information or criteria to order the independent components. For example, if the independent components represent audio signals, they can be ordered based on their frequency content or their energy levels. Alternatively, if the independent components represent images, they can be ordered based on their spatial frequency content or their edge information.</p>
<p>Another approach is to use clustering algorithms to group independent components that are similar or belong to the same source. This can help to identify the underlying sources of the independent components and to assign them to specific sources in a meaningful way.</p>
<p>Overall, the ambiguity of order of independent components is a common challenge in ICA, but it can be mitigated by using additional information and criteria to order and group the independent components.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/QS/Week6/QS_W6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/QS/Week6/QS_W6/</guid><description>1. b: By differentiating, show that a necessary condition to minimize P (error) is that θ satisfies p(θ|c1)P (c1) = p(θ|c2)P (c2)
Leibniz&amp;rsquo;s rule for differentiating an integral is a method that allows us to differentiate an integral that depends on a parameter.</description><content:encoded><![CDATA[<p><strong>1. b: By differentiating, show that a necessary condition to minimize P (error) is that θ satisfies  p(θ|c1)P (c1) = p(θ|c2)P (c2)</strong></p>
<p>Leibniz&rsquo;s rule for differentiating an integral is a method that allows us to differentiate an integral that depends on a parameter. It is named after the German mathematician Gottfried Wilhelm Leibniz.</p>
<p>The rule states that if we have an integral of the form:</p>
<p>$F(t) = \int_{a(t)}^{b(t)} f(x,t) dx$</p>
<p>where $a(t)$ and $b(t)$ are functions of the parameter $t$, and if the integrand $f(x,t)$ and the functions $a(t)$ and $b(t)$ are continuous, then we can differentiate $F(t)$ with respect to $t$ by first differentiating the limits of integration with respect to $t$, and then integrating the result with respect to $x$. That is:</p>
<p>$\frac{d}{dt} F(t) = \int_{a(t)}^{b(t)} \frac{\partial}{\partial t} f(x,t) dx + f(b(t),t)\frac{db}{dt} - f(a(t),t)\frac{da}{dt}$</p>
<p>Here, $\frac{\partial}{\partial t} f(x,t)$ represents the partial derivative of the integrand with respect to $t$.</p>
<p>To apply this rule to differentiate the expression for P(error) with respect to $\theta$, we can first write the expression as:</p>
<p>$P(error) = P(c1) \int_{-\infty}^{\theta} p(x|c1) dx + P(c2) \int_{\theta}^{\infty} p(x|c2) dx$</p>
<p>Here, $a(\theta) = -\infty$, $b(\theta) = \theta$, and $f(x,\theta) = p(x|c1)$ for the first integral, and $a(\theta) = \theta$, $b(\theta) = \infty$, and $f(x,\theta) = p(x|c2)$ for the second integral.</p>
<p>Using Leibniz&rsquo;s rule, we can differentiate P(error) with respect to $\theta$ as follows:</p>
<p>$\frac{d}{d\theta} P(error) = P(c1) \frac{\partial}{\partial \theta} \int_{-\infty}^{\theta} p(x|c1) dx + P(c2) \frac{\partial}{\partial \theta} \int_{\theta}^{\infty} p(x|c2) dx + P(c2) p(\theta|c2) - P(c1) p(\theta|c1)$</p>
<p>Evaluating the partial derivatives using the fundamental theorem of calculus, we get:</p>
<p>$\frac{d}{d\theta} P(error) = P(c1) p(\theta|c1) - P(c2) p(\theta|c2)$</p>
<p>This is the desired result, which shows that the derivative of P(error) with respect to $\theta$ is equal to the difference between the probability density functions of the two classes evaluated at $\theta$, weighted by the prior probabilities of each class.</p>
<p>My simple answer:</p>
<p>

<img src="/Screenshot%202023-04-29%20at%208.13.56%20PM.png" alt="Screenshot 2023-04-29 at 8.13.56 PM.png"  /></p>
<p><strong>1.c: Explain why this equation does not necessarily define a unique θ or even one that minimizes  P (error). Illustrate your answer with examples.</strong></p>
<p>The equation $p(\theta|c1)P(c1) = p(\theta|c2)P(c2)$ is a necessary condition for finding the value of $\theta$ that minimizes the probability of error $P(error)$ in a two-category one-dimensional problem with the decision rule &ldquo;choose $c1$ if $x &gt; \theta$, otherwise choose $c2$.&rdquo; However, this equation does not necessarily define a unique value of $\theta$ or even one that minimizes $P(error)$.</p>
<p><strong>We can also have multiple minimum (local)</strong></p>
<p>There are a few reasons for this:</p>
<ol>
<li>
<p><strong>The equation is only a necessary condition, not a sufficient condition, for finding the value of $\theta$ that minimizes $P(error)$. Even if the equation is satisfied, there may be other values of $\theta$ that give lower values of $P(error)$.</strong></p>
</li>
<li>
<p>The equation assumes that the probability density functions $p(x|c1)$ and $p(x|c2)$ are known and can be differentiated with respect to $\theta$. In practice, these functions may be unknown or difficult to estimate accurately, which can make it challenging to find the value of $\theta$ that minimizes $P(error)$.</p>
</li>
<li>
<p>The equation assumes that the prior probabilities $P(c1)$ and $P(c2)$ are known and accurately reflect the true class probabilities. In practice, these probabilities may be unknown or incorrectly specified, which can affect the value of $\theta$ that minimizes $P(error)$.</p>
</li>
</ol>
<p>T**o illustrate these points, let&rsquo;s consider an example. Suppose we have a two-category one-dimensional problem with the following probability density functions:</p>
<p>$p(x|c1) = N(x; 0, 1)$</p>
<p>$p(x|c2) = N(x; 2, 1)$</p>
<p>where $N(x; \mu, \sigma^2)$ is the normal (Gaussian) probability density function with mean $\mu$ and variance $\sigma^2$.</p>
<p>If we assume equal prior probabilities $P(c1) = P(c2) = 0.5$, then the equation $p(\theta|c1)P(c1) = p(\theta|c2)P(c2)$ simplifies to:</p>
<p>$N(\theta; 0, 1) = N(\theta; 2, 1)$</p>
<p>This equation has two solutions: $\theta = 0$ and $\theta = 2$. However, neither of these values minimizes $P(error)$, which can be shown to be minimized at $\theta = 1$.**</p>
<p>This example illustrates how the necessary condition $p(\theta|c1)P(c1) = p(\theta|c2)P(c2)$ does not necessarily define a unique value of $\theta or one that minimizes $P(error)$. In this case, the equation is satisfied at two different values of $\theta$, but neither of these values gives the optimal decision rule. Other factors, such as the shape of the probability density functions and the prior probabilities, can also influence the value of $\theta$ that minimizes $P(error)$.</p>
<p><strong>2. Suppose we have two normal distributions with the same covariance but different means.</strong>
(a) Explain what is meant by a Bayes Classifier.</p>
<p>A Bayes classifier is a classification system that minimises the probability of an<br>
incorrect classification.</p>
<p>(b) Derive an expression for the Bayesian Decision Boundary (BDB) in terms of the prior proba-<br>
bilities P (c1) and P (c2), the means μ1 and μ2 and the common covariance Σ.</p>
<p>3.a Show that the true error cannot decrease if we first project the distributions to a lower dimensional space and then classify them. [Hint: you can think of this geometrically first. If you want to show it mathematically it helps to think of the data lying on a 2D space.]</p>
<p>Geometrically, projecting the data to a lower-dimensional space corresponds to mapping the data points onto a lower-dimensional subspace of the original feature space. This can be thought of as collapsing the data points onto a lower-dimensional plane or line.</p>
<p>Now, suppose we have two arbitrary distributions $p(\mathbf{x}|c_1)$ and $p(\mathbf{x}|c_2)$ with known priors $P(c_1)$ and $P(c_2)$ in a $d$-dimensional feature space, and we want to classify them using a linear decision boundary. If we project the distributions onto a lower-dimensional subspace (i.e., a lower-dimensional plane or line), then the decision boundary will also be a linear boundary in the lower-dimensional space, and we can classify the data points as belonging to one of two classes based on which side of the boundary they lie.</p>
<p>However, projecting the data to a lower-dimensional space can never increase the distance between the means of the two distributions, since the projection essentially collapses the data onto a lower-dimensional plane or line. As a result, the linear decision boundary in the lower-dimensional space can never be further away from the means than the linear decision boundary in the original space. This means that the classification error in the lower-dimensional space can never be lower than the classification error in the original space.</p>
<p>Mathematically, we can show this as follows. Let $\mathbf{x}_1$ and $\mathbf{x}_2$ be two data points sampled from the two distributions, with means $\boldsymbol{\mu}_1$ and $\boldsymbol{\mu}_2$, respectively. Let $\mathbf{w}$ be a $d$-dimensional vector that defines the projection onto the lower-dimensional space, and let $\mathbf{y}_1 = \mathbf{w}^T\mathbf{x}_1$ and $\mathbf{y}_2 = \mathbf{w}^T\mathbf{x}_2$ be the corresponding projected data points. Then, the distance between the means of the projected distributions is given by:</p>
<p>$|\boldsymbol{\mu}_{\mathbf{y}<em>1} - \boldsymbol{\mu}</em>{\mathbf{y}_2}| = |\mathbf{w}^T\boldsymbol{\mu}_1 - \mathbf{w}^T\boldsymbol{\mu}_2| = |\mathbf{w}^T(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)|$</p>
<p>Since $\mathbf{w}$ is a $d$-dimensional vector, $|\mathbf{w}^T(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)|$ is the projection of the vector $(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)$ onto the subspace defined by $\mathbf{w}$. By the Cauchy-Schwarz inequality, this projection is bounded by the length of $(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)$, i.e.,</p>
<p>$|\mathbf{w}^T(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2)| \leq ||\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2||$</p>
<p>where $||\cdot||$ denotes the Euclidean norm. Therefore, the distance between the means of the projected distributions is always less than or equal to the distance between the means of the original distributions.</p>
<p>Since the classification error is proportional to the distance between the means of the distributions, it follows that the true error cannot decrease if we first project the distributions to a lower-dimensional space and then classify them. In other words , the classification error in the lower-dimensional space can never be lower than the classification error in the original space. This is true for any choice of the projection vector $\mathbf{w}$ and any choice of the lower-dimensional subspace.</p>
<p>Therefore, if we want to minimize the classification error, we should always use the original high-dimensional feature space for classification, unless we have a good reason to believe that the relevant information in the data lies in a lower-dimensional subspace. In practice, we can use dimensionality reduction techniques, such as principal component analysis (PCA) or linear discriminant analysis (LDA), to identify the most important dimensions or subspace that capture the relevant information in the data. However, we should always keep in mind that projecting the data to a lower-dimensional space can never improve the classification accuracy beyond what is achievable in the original high-dimensional space.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/QS/Week7/Bayesian-Learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/QS/Week7/Bayesian-Learning/</guid><description>Expectations as Integrals In probability theory and statistics, expectations are often expressed as integrals. The expectation of a function $g(x)$ with respect to a probability distribution $p(x)$ is defined as:</description><content:encoded><![CDATA[<h2 id="expectations-as-integrals">Expectations as Integrals</h2>
<p>In probability theory and statistics, expectations are often expressed as integrals. The expectation of a function $g(x)$ with respect to a probability distribution $p(x)$ is defined as:</p>
<p>$\mathbb{E}[g(x)] = \int g(x)p(x)dx$</p>
<p>This integral represents the average value of $g(x)$ over the probability distribution $p(x)$. The integral is taken over the entire domain of $x$ and the function $g(x)$ is weighted by the probability density function $p(x)$.</p>
<p>For example, the mean of a random variable $X$ with probability density function $f(x)$ can be expressed as:</p>
<p>$\mathbb{E}[X] = \int x f(x)dx$</p>
<p>Similarly, the variance of $X$ can be expressed as:</p>
<p>$\text{Var}(X) = \mathbb{E}[(X-\mu)^2] = \int (x-\mu)^2 f(x)dx$</p>
<p>where $\mu$ is the mean of $X$.</p>
<p>Expectations as integrals allow us to compute various properties of probability distributions, such as mean, variance, and higher moments, and to perform statistical inference, such as hypothesis testing and parameter estimation, based on probability distributions. They also provide a way to generalize the concept of average or expected value beyond simple arithmetic means.</p>
<h2 id="making-predictions-with-bayesian-learning-posterior-mean">Making Predictions with Bayesian Learning: Posterior Mean</h2>
<p>In Bayesian learning, the goal is to compute the posterior distribution over the model parameters given the observed data. Once we have obtained the posterior distribution, we can use it to make predictions for new, unseen data points.</p>
<p>One way to make predictions using Bayesian learning is to use the posterior mean of the model parameters as the predicted value. The posterior mean is a weighted average of the possible parameter values, where the weights are given by the posterior distribution.</p>
<p>Suppose we have a model with parameters $\theta$ and observed data $\mathcal{D}$. We can compute the posterior distribution over $\theta$ using Bayes&rsquo; rule:</p>
<p>$p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})}$</p>
<p>where $p(\mathcal{D}|\theta)$ is the likelihood function, $p(\theta)$ is the prior distribution over the parameters, and $p(\mathcal{D})$ is the marginal likelihood, which acts as a normalizing constant.</p>
<p>**Once we have obtained the posterior distribution $p(\theta|\mathcal{D})$, we can use the posterior mean of $\theta$ as the predicted value for a new data point $\mathbf{x}$. That is,</p>
<p>$\hat{y}(\mathbf{x}) = \mathbb{E}_{p(\theta|\mathcal{D})}[y(\mathbf{x},\theta)]$</p>
<p><strong>where $y(\mathbf{x},\theta)$ is the predicted output for $\mathbf{x}$ given the parameter value $\theta$, and $\mathbb{E}_{p(\theta|\mathcal{D})}$ denotes the expectation with respect to the posterior distribution over $\theta$.</strong></p>
<p>In practice, computing the posterior mean may involve numerical integration or Monte Carlo methods, depending on the complexity of the model and the posterior distribution. However, the posterior mean provides a principled way to make predictions that takes into account the uncertainty in the model parameters and the observed data.</p>
<h2 id="making-predictions-with-bayesian-learning-maximum-a-priori">Making Predictions with Bayesian Learning: Maximum A Priori</h2>
<p>In Bayesian learning, we often seek to estimate the posterior distribution over the model parameters given the observed data. One way to make predictions using Bayesian learning is to use the maximum a priori (MAP) estimate of the parameters.</p>
<p>The MAP estimate of the parameters $\theta$ is the value that maximizes the posterior distribution $p(\theta|\mathcal{D})$. Mathematically, we can write:</p>
<p>$$
\theta_{\text{MAP}} = \operatorname*{argmax}<em>{\theta} p(\theta\mid D) = \operatorname*{argmax}</em>{\theta} p(D\mid\theta)p(\theta)
$$</p>
<p>where $\mathcal{D}$ represents the observed data, $p(\mathcal{D}|\theta)$ is the likelihood function, and $p(\theta)$ is the prior distribution over the parameters.</p>
<p>Once we have obtained the MAP estimate of the parameters, we can use it to make predictions for new, unseen data points. For example, in a regression problem, we can predict the output value $\hat{y}$ for a new input $\mathbf{x}$ as follows:</p>
<p>$$
\hat{y} = f(\mathbf{x};\theta_{\text{MAP}})
$$</p>
<p>where $f(\mathbf{x};\theta)$ is the regression function that maps inputs to outputs using the parameters $\theta$.</p>
<p>The MAP estimate can be viewed as a point estimate of the parameters, since it provides a single value for the parameters rather than a distribution over possible values. It can be more computationally efficient than computing the full posterior distribution, especially for large-scale problems.</p>
<p>However, the MAP estimate may not fully capture the uncertainty in the model parameters and can be sensitive to the choice of prior distribution. In addition, it can be affected by overfitting if the prior distribution is not well-suited to the data. Therefore, it is important to consider the limitations and assumptions of the MAP estimate and to compare it with other methods of Bayesian learning, such as computing the full posterior distribution or using Bayesian model averaging.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/QS/Week7/QS_W7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/QS/Week7/QS_W7/</guid><description>1.b Show that the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.
MAP vs. ML Estimates under Nonlinear Transformations The maximum likelihood (ML) estimate of the model parameters $\theta$ is the value that maximizes the likelihood function $p(D\mid\theta)$, where $D$ represents the observed data.</description><content:encoded><![CDATA[<p>1.b <strong>Show that the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.</strong></p>
<h5 id="map-vs-ml-estimates-under-nonlinear-transformations">MAP vs. ML Estimates under Nonlinear Transformations</h5>
<p>The maximum likelihood (ML) estimate of the model parameters $\theta$ is the value that maximizes the likelihood function $p(D\mid\theta)$, where $D$ represents the observed data. Mathematically, we can write:</p>
<p>$$
\theta_{\text{ML}} = \operatorname*{argmax}_{\theta} p(D\mid\theta)
$$</p>
<p>The maximum a posteriori (MAP) estimate of the parameters $\theta$ is the value that maximizes the posterior distribution $p(\theta\mid D)$. Mathematically, we can write:</p>
<p>$$
\theta_{\text{MAP}} = \operatorname*{argmax}<em>{\theta} p(\theta\mid D) = \operatorname*{argmax}</em>{\theta} p(D\mid\theta)p(\theta)
$$</p>
<p>Suppose we have a nonlinear transform $g$ of the parameter space such that $\theta = g(\phi)$, where $\phi$ represents the transformed parameters. If $g$ is invertible, we can write $\phi = g^{-1}(\theta)$.</p>
<p>The likelihood function under the transformed parameter space is given by $p(D\mid\phi)$. The ML estimate of the transformed parameters $\phi$ is the value that maximizes the likelihood function, which we can write as:</p>
<p>$$
\phi_{\text{ML}} = \operatorname*{argmax}_{\phi} p(D\mid\phi)
$$</p>
<p>However, the MAP estimate of the transformed parameters $\phi$ is given by:</p>
<p>$$
\phi_{\text{MAP}} = \operatorname*{argmax}<em>{\phi} p(\phi\mid D) = \operatorname*{argmax}</em>{\phi} p(D\mid\phi)p(\phi)
$$</p>
<p>Now, we can use the <strong>change of variables formula</strong> for probability densities to write:</p>
<p>(look for the change of variables concept below)</p>
<p>$$
p(D\mid\phi) = p(D\mid\theta) \left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|^{-1}
$$</p>
<p>where $\left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|$ is the absolute value of the determinant of the Jacobian matrix of $g$ with respect to $\phi$.</p>
<p>Substituting this into the expressions for the ML and MAP estimates, we get:</p>
<p>$$
\phi_{\text{ML}} = g^{-1}\left(\operatorname*{argmax}_{\theta} p(D\mid\theta)\right)
$$</p>
<p>and</p>
<p>$$
\phi_{\text{MAP}} = g^{-1}\left(\operatorname*{argmax}_{\theta} p(D\mid\theta)p(g(\phi))\left|\det\left(\frac{\partial g(\phi)}{\partial\phi}\right)\right|^{-1}\right)
$$</p>
<p>We can see that the ML estimate of the transformed parameters $\phi$ is simply the inverse transform of the ML estimate of the original parameters $\theta$. However, the MAP estimate of the transformed parameters $\phi$ depends on both the prior distribution over the transformed parameters $p(\phi)$ and the Jacobian of the transformation. Therefore, the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.</p>
<p><strong>Change of Variables Formula for Probability Densities</strong></p>
<p>The change of variables formula for probability densities is a formula that allows us to transform a probability density function from one variable to another. It is used when we want to work with a probability distribution in a new set of variables.</p>
<p>Suppose we have a probability density function $p_Y(y)$ over the random variable $Y$, and we want to transform it to a probability density function $p_X(x)$ over the random variable $X$ using a one-to-one function $g: X \rightarrow Y$. Then, the change of variables formula for probability densities states that:</p>
<p>$$
p_X(x) = p_Y(g(x)) \left|\frac{dg(x)}{dx}\right|
$$</p>
<p>where $\left|\frac{dg(x)}{dx}\right|$ is the absolute value of the derivative of the function $g$ with respect to $x$.</p>
<p>Intuitively, this formula tells us that the probability density of $X$ at $x$ is equal to the probability density of $Y$ at $g(x)$ multiplied by the factor that accounts for the stretching or compression of the probability density as we change variables.</p>
<p>To use the formula, we first find the inverse function of $g$, which we can use to express $y$ in terms of $x$. Then, we substitute this expression for $y$ into the original probability density function $p_Y(y)$ to get $p_Y(g(x))$. Finally, we multiply this by the factor $\left|\frac{dg(x)}{dx}\right|$ to get the probability density function $p_X(x)$ over the variable $X$.</p>
<p>The change of variables formula is commonly used in Bayesian inference and machine learning to transform probability distributions between different parameterizations or to transform data from one feature space to another.</p>
<ol start="3">
<li><strong>Let x be drawn from a uniform density; calculate the maximum likelihood estimate for θ given the data: D = {0.1, 0.4, 0.2, 0.8, 0.45}.</strong></li>
</ol>
<p>We know that $x$ is drawn from a uniform density with parameter $\theta$ over the interval $[0, 1]$. The probability density function of $x$ is given by:</p>
<p>$$
p(x | \theta) =
\begin{cases}
\frac{1}{\theta} &amp; 0 \leq x \leq \theta \
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>Given the data $D = {0.1, 0.4, 0.2, 0.8, 0.45}$, the likelihood function is:</p>
<p>$$
L(\theta | D) = \prod_{i=1}^n p(x_i | \theta) =
\begin{cases}
\frac{1}{\theta^n} &amp; x_{(1)} \leq \theta \leq x_{(n)} \
0 &amp; \text{otherwise}
\end{cases}
$$</p>
<p>where $x_{(1)}$ and $x_{(n)}$ are the minimum and maximum values in the dataset, respectively.</p>
<p>To find the maximum likelihood estimate of $\theta$, we need to find the value of $\theta$ that maximizes the likelihood function. Since the likelihood function is a decreasing function of $\theta$ for $\theta &lt; x_{(n)}$, and an increasing function of $\theta$ for $\theta &gt; x_{(1)}$, the maximum likelihood estimate of $\theta$ is given by:</p>
<p>$$
\hat{\theta}<em>{ML} = x</em>{(n)}
$$</p>
<p>In this case, the maximum value in the dataset is 0.8, so the maximum likelihood estimate of $\theta$ is $\hat{\theta}_{ML} = 0.8$.</p>
<p><strong>NOTE: Why are we not taking derivatives in this case? Can we also not solve for it by taking derivative of likelihood function?</strong></p>
<p><em>In this particular case, the likelihood function is not differentiable everywhere, so we cannot take its derivative to find the maximum likelihood estimate of $\theta$.</em></p>
<p>However, we can observe that the likelihood function is a decreasing function of $\theta$ for $\theta &lt; x_{(n)}$ and an increasing function of $\theta$ for $\theta &gt; x_{(1)}$. This means that the maximum likelihood estimate of $\theta$ must lie in the interval $[x_{(1)}, x_{(n)}]$. We can then compare the likelihood of the two endpoints of this interval to find the maximum likelihood estimate. Since the likelihood is 0 outside of this interval, we can be sure that the maximum likelihood estimate lies within it.</p>
<p>In this case, the endpoint $x_{(n)}$ has the highest likelihood among all the possible values of $\theta$ within the interval, so the maximum likelihood estimate is $\hat{\theta}<em>{ML} = x</em>{(n)} = 0.8$.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/QS/Week9/QS_W9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/QS/Week9/QS_W9/</guid><description>Q2.
We can use the law of total probability to express the joint probability of the observation and the current state in terms of the previous state as:
$P(V_t, \omega(t) = j | \omega_0) = \sum_i P(V_t, \omega(t) = j, \omega(t-1) = i | \omega_0)$</description><content:encoded><![CDATA[<p>Q2.</p>
<p>We can use the law of total probability to express the joint probability of the observation and the current state in terms of the previous state as:</p>
<p>$P(V_t, \omega(t) = j | \omega_0) = \sum_i P(V_t, \omega(t) = j, \omega(t-1) = i | \omega_0)$</p>
<p>Using the Markov assumption, we can simplify this to:</p>
<p>$P(V_t, \omega(t) = j | \omega_0) = \sum_i P(V_t | \omega(t) = j, \omega(t-1) = i, \omega_0) P(\omega(t) = j | \omega(t-1) = i, \omega_0) P(\omega(t-1) = i | \omega_0)$</p>
<p>We can rewrite the first term using the emission probability $b_j(v_t)$, and the second term using the transition probability $a_{i,j}$:</p>
<p>$P(V_t, \omega(t) = j | \omega_0) = \sum_i b_j(v_t) a_{i,j} P(\omega(t-1) = i | \omega_0)$</p>
<p>Substituting this into the expression for $\alpha_j(t)$, we have:</p>
<p>$$
\begin{align}
&amp;\quad \alpha_j(t) = P(V_1, \dots, V_t, \omega(t) = j | \omega_0) \
&amp;\quad = \sum_i P(V_t, \omega(t) = j, \omega(t-1) = i | V_1, \dots, V_{t-1}, \omega_0) P(V_1, \dots, V_{t-1}, \omega(t-1) = i | \omega_0) \
&amp;\quad = \sum_i P(V_t | \omega(t) = j, \omega(t-1) = i, V_1, \dots, V_{t-1}, \omega_0) P(\omega(t) = j | \omega(t-1) = i, V_1, \dots, V_{t-1}, \omega_0) P(V_1, \dots, V_{t-1}, \omega(t-1) = i | \omega_0) \
&amp;\quad = \sum_i P(V_t | \omega(t) = j) P(\omega(t) = j | \omega(t-1) = i) P(V_1, \dots, V_{t-1}, \omega(t-1) = i | \omega_0) \
&amp;\quad = \sum_i b_j(v_t) a_{i,j} \alpha_i(t-1)
\end{align} $$</p>
<p>So we have recursively defined $\alpha_j(t)$ as a function of the previous time step $\alpha_i(t-1)$, the transition probability $a_{i,j}$, and the emission probability $b_j(v_t)$.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/Track-Weeks/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/Track-Weeks/</guid><description> Week 1 Classification Part 1 Introduction to the biological neuron that served as inspiration for the Perceptron Linear classification and simple linear discriminant functions, and the concept of hyperplanes Multicategory classification, linear machines and generalized linear discriminants Normalisation, solution regions, the margin Week 2 Classification Part 2 Convex function, set and hull The Perceptron Criterion and Algorithm The MSE Classification Procedure Support Vector Machines Week 3 Neural Network Multilayer Neural Networks: forward operation and expressive power Multilayer Neural Networks: Back-propagation setup Multilayer Neural Networks: Back-propagation algorithm Multilayer Neural Networks: Training protocols Week 4 CNN Convolutional Neural Networks and Deep Learning Optimization Reviewing Background for Optimisation Bracketing Methods for 1-d Optimisation Gradient Descent Newton’s Method Week 5 Optimization Levenberg Marquardt Gradient descent with line minima Conjugate Gradient Descent Week 6 Bayes Decision Theory Supervised vs unsupervised learning; probabilistic view to learning Bayes formula, Bayes decision rule and probability of error Examples of statistical classification from a Bayesian view Week 7 Model Learning Maximum likelihood estimation; non-linear regression; Maximum likelihood fitting of Gaussians Overfitting; bias-variance dilemma Learning models with Bayesian; fitting 1D Gaussians; the Bayes – Maximum likelihood link Week 8 Clustering K-means clustering; k-means for vector quantization; optimality of k-means Relation to matrix factorisation; k-medoids algorithm Mixture of Gaussians and EM algorithm Week 9 HMM First-order Markov models; first-order hidden Markov models; different models Three fundamental problems; evaluation problem; isolated word speech recognition Evaluation problem; decoding problem; learning problem Week 10 Principal Component Analysis (PCA) Principal component analysis; importance and implications; subspace projections Relation to matrix factorisation; PCA example Independent Component Analysis (ICA) Independent component analysis; differences with PCA</description><content:encoded><![CDATA[<ul>
<li><input checked="" disabled="" type="checkbox"> Week 1
<ul>
<li><input checked="" disabled="" type="checkbox"> Classification Part 1
<ul>
<li>Introduction to the biological neuron that served as inspiration for the Perceptron</li>
<li>Linear classification and simple linear discriminant functions, and the concept of hyperplanes</li>
<li>Multicategory classification, linear machines and generalized linear discriminants</li>
<li>Normalisation, solution regions, the margin</li>
</ul>
</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> Week 2
<ul>
<li><input checked="" disabled="" type="checkbox"> Classification Part 2
<ul>
<li>Convex function, set and hull</li>
<li>The Perceptron Criterion and Algorithm</li>
<li>The MSE Classification Procedure</li>
<li>Support Vector Machines</li>
</ul>
</li>
</ul>
</li>
<li><input checked="" disabled="" type="checkbox"> Week 3
<ul>
<li><input checked="" disabled="" type="checkbox"> Neural Network
<ul>
<li>Multilayer Neural Networks: forward operation and expressive power</li>
<li>Multilayer Neural Networks: Back-propagation setup</li>
<li>Multilayer Neural Networks: Back-propagation algorithm</li>
<li>Multilayer Neural Networks: Training protocols</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 4
<ul>
<li><input disabled="" type="checkbox"> CNN
<ul>
<li>Convolutional Neural Networks and Deep Learning</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Optimization
<ul>
<li>Reviewing Background for Optimisation</li>
<li>Bracketing Methods for 1-d Optimisation</li>
<li>Gradient Descent</li>
<li>Newton’s Method</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 5
<ul>
<li><input disabled="" type="checkbox"> Optimization
<ul>
<li>Levenberg Marquardt</li>
<li>Gradient descent with line minima</li>
<li>Conjugate Gradient Descent</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 6
<ul>
<li><input disabled="" type="checkbox"> Bayes Decision Theory
<ul>
<li>Supervised vs unsupervised learning; probabilistic view to learning</li>
<li>Bayes formula, Bayes decision rule and probability of error</li>
<li>Examples of statistical classification from a Bayesian view</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 7
<ul>
<li><input disabled="" type="checkbox"> Model Learning
<ul>
<li>Maximum likelihood estimation; non-linear regression; Maximum likelihood fitting of Gaussians</li>
<li>Overfitting; bias-variance dilemma</li>
<li>Learning models with Bayesian; fitting 1D Gaussians; the Bayes – Maximum likelihood link</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 8
<ul>
<li><input disabled="" type="checkbox"> Clustering
<ul>
<li>K-means clustering; k-means for vector quantization; optimality of k-means</li>
<li>Relation to matrix factorisation; k-medoids algorithm</li>
<li>Mixture of Gaussians and EM algorithm</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 9
<ul>
<li><input disabled="" type="checkbox"> HMM
<ul>
<li>First-order Markov models; first-order hidden Markov models; different models</li>
<li>Three fundamental problems; evaluation problem; isolated word speech recognition</li>
<li>Evaluation problem; decoding problem; learning problem</li>
</ul>
</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Week 10
<ul>
<li><input disabled="" type="checkbox"> Principal Component Analysis (PCA)
<ul>
<li>Principal component analysis; importance and implications; subspace projections</li>
<li>Relation to matrix factorisation; PCA example</li>
</ul>
</li>
<li><input disabled="" type="checkbox"> Independent Component Analysis (ICA)
<ul>
<li>Independent component analysis; differences with PCA</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/MLSP/Week1/Notes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/MLSP/Week1/Notes/</guid><description>Linear Discriminant Functions Linear discriminant functions, also known as linear classifiers, are mathematical functions used in classification tasks to separate data points from different classes based on their feature values. These functions model a linear decision boundary or hyperplane that separates the classes in the feature space.</description><content:encoded><![CDATA[<h1 id="linear-discriminant-functions">Linear Discriminant Functions</h1>
<p><strong>Linear discriminant functions</strong>, also known as <strong>linear classifiers</strong>, are mathematical functions used in classification tasks to separate data points from different classes based on their feature values. These functions <strong>model a linear decision boundary or hyperplane that separates the classes in the feature space</strong>.</p>
<p>Here&rsquo;s an overview of linear discriminant functions:</p>
<ol>
<li>
<p><strong>Representation</strong>: A linear discriminant function is typically represented as a linear combination of the input features. It takes the form: $f(x) = w^T x + b$ where $f(x$) represents the predicted class label for a given input vector $x, w$ is the weight vector, and $b$ is the bias or intercept term.</p>
</li>
<li>
<p><strong>Decision Boundary</strong>: The linear discriminant function defines a decision boundary or hyperplane in the feature space. The decision boundary is the locus of points where the function output changes, determining the classification of the input points. In a binary classification problem, the decision boundary separates the data points of one class from the other class.</p>
</li>
</ol>
<p><strong>Linear discriminant functions assume that the decision boundaries between classes are linear.</strong> While they may not capture complex relationships present in the data, linear classifiers can be effective when classes are separable by linear decision boundaries or in situations where interpretability and simplicity are important.</p>
<p>Overall, linear discriminant functions play a crucial role in classification tasks by separating data points based on linear decision boundaries or hyperplanes, enabling the prediction of class labels for unseen data.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/NLU/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/NLU/</guid><description> Week 1 (Machine Translation) Introduction Machine Translation Conditional Language Models (with n-grams) Week 2 (Language Models) Feedforward Language Models RNN LSTM GRU Week 3 Seq2Seq Models with attention Transformers Week 4 Word Embeddings Pretrained Language Models Week 5 Prompting Evaluation of MT Week 6 Open Vocabulary Models Low Resource MT NLP Ethics Week 7 Bias in Embeddings and Language Models Summarization 1 Summarization 2 Week 8 Summarization 3 Week 9 Neural Parsing Unsupervised Parsing Week 10 Movie Summarization Tutorials Tutorial 1 - Language Models Tutorial 2 - Neural Network Language Models Tutorial 3 - Transformers Tutorial 4 - Ethics in NLP Tutorial 5 - Summarization</description><content:encoded><![CDATA[<h2 id="week-1-machine-translationweek1">





<a
  href="/UoE/NLU/Week1/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week1/">Week 1 (Machine Translation)</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Introduction</li>
<li><input checked="" disabled="" type="checkbox"> Machine Translation</li>
<li><input checked="" disabled="" type="checkbox"> Conditional Language Models (with n-grams)</li>
</ul>
<h2 id="week-2-language-modelsweek2">





<a
  href="/UoE/NLU/Week2/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week2/">Week 2 (Language Models)</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Feedforward Language Models</li>
<li><input checked="" disabled="" type="checkbox"> RNN</li>
<li><input checked="" disabled="" type="checkbox"> LSTM</li>
<li><input checked="" disabled="" type="checkbox"> GRU</li>
</ul>
<h2 id="week-3week3">





<a
  href="/UoE/NLU/Week3/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week3/">Week 3</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Seq2Seq Models with attention</li>
<li><input disabled="" type="checkbox"> Transformers</li>
</ul>
<h2 id="week-4week4">





<a
  href="/UoE/NLU/Week4/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week4/">Week 4</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Word Embeddings</li>
<li><input checked="" disabled="" type="checkbox"> Pretrained Language Models</li>
</ul>
<h2 id="week-5week5">





<a
  href="/UoE/NLU/Week5/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week5/">Week 5</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Prompting</li>
<li><input checked="" disabled="" type="checkbox"> Evaluation of MT</li>
</ul>
<h2 id="week-6week6">





<a
  href="/UoE/NLU/Week6/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week6/">Week 6</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Open Vocabulary Models</li>
<li><input checked="" disabled="" type="checkbox"> Low Resource MT</li>
<li><input checked="" disabled="" type="checkbox"> NLP Ethics</li>
</ul>
<h2 id="week-7week7">





<a
  href="/UoE/NLU/Week7/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week7/">Week 7</a></h2>
<ul>
<li><input disabled="" type="checkbox"> Bias in Embeddings and Language Models</li>
<li><input checked="" disabled="" type="checkbox"> Summarization 1</li>
<li><input checked="" disabled="" type="checkbox"> Summarization 2</li>
</ul>
<h2 id="week-8week7">





<a
  href="/UoE/NLU/Week7/"
  rel="noopener" class="internal-link"
  data-src="/UoE/NLU/Week7/">Week 8</a></h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Summarization 3</li>
</ul>
<h2 id="week-9">Week 9</h2>
<ul>
<li><input disabled="" type="checkbox"> Neural Parsing</li>
<li><input disabled="" type="checkbox"> Unsupervised Parsing</li>
</ul>
<h2 id="week-10">Week 10</h2>
<ul>
<li><input disabled="" type="checkbox"> Movie Summarization</li>
</ul>
<h2 id="tutorials">Tutorials</h2>
<ul>
<li><input disabled="" type="checkbox"> Tutorial 1 - Language Models</li>
<li><input disabled="" type="checkbox"> Tutorial 2 - Neural Network Language Models</li>
<li><input disabled="" type="checkbox"> Tutorial 3 - Transformers</li>
<li><input disabled="" type="checkbox"> Tutorial 4 - Ethics in NLP</li>
<li><input disabled="" type="checkbox"> Tutorial 5 - Summarization</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week1/</guid><description>Introduction Readings:
The Future of Computational Linguistics: Beyond the Alchemy Church and Lieberman (21)
Why has deep learning taken over NLP?
Universal Function Approximators Representation Learning Multi-task learning Works with wide class of data: strings, labels, trees, graphs, tables, images deep learning solves the difficulties of applying machine learning to NLP&amp;hellip; But it does not solve NLP!</description><content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Readings:</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://www.frontiersin.org/articles/10.3389/frai.2021.625341/full" rel="noopener">The Future of Computational Linguistics: Beyond the Alchemy</a> Church and Lieberman (21)</p>
</li>
<li></li>
<li>
<p>Why has deep learning taken over NLP?</p>
<ul>
<li>Universal Function Approximators</li>
<li>Representation Learning</li>
<li>Multi-task learning</li>
<li>Works with wide class of data: <strong>strings, labels, trees, graphs, tables, images</strong></li>
<li>deep learning solves the difficulties of applying machine learning to NLP&hellip; <em>But it does not solve NLP!</em></li>
</ul>
</li>
<li>
<p>Problems with DL:</p>
<ul>
<li>Energy - carbon cost increasing exponentially</li>
<li>Ethical practice lags technical practice - <em>Privacy, Bias, Justice</em> etc.</li>
</ul>
</li>
</ul>
<p><strong>Fundamental Methods of the Course</strong></p>
<p>Our primary tool will be probabilistic models parameterized by</p>
<p>deep learning architectures such as:<br>
• feed-forward neural networks<br>
• recurrent neural networks<br>
• transformers</p>
<p>&hellip; applied primarily to structured prediction tasks in NLP.</p>
<p>The second half of the course will focus on the application of deep models to a variety of core
<strong>NLP tasks:</strong><br>
• machine translation<br>
• word embeddings<br>
• pretrained language models<br>
• syntactic parsing<br>
• semantic parsing<br>
<strong>And applications:</strong><br>
• paraphrasing<br>
• question answering<br>
• summarization<br>
• data-to-text generation</p>
<h1 id="machine-translation">Machine Translation</h1>
<p>Readings:</p>
<ul>
<li>Background Reading: 

<a target=“_blank” href="https://kevincrawfordknight.github.io/papers/aimag97.pdf" rel="noopener">Automating Knowledge Acquisition for Machine Translation</a>, Knight.</li>
</ul>
<p><strong>Challenges in MT:</strong></p>
<ul>
<li>Words are ambiguous (<strong>French: Banque</strong> / <strong>English: Bank</strong>, <strong>French: rive</strong> / <strong>English: Bank</strong>)</li>
<li>Words have complex morphology
<ul>
<li>Tense</li>
<li>Singular/Plural</li>
<li>Morphology:
<ul>
<li>Finnish: ostoskeskuksessa<br>
ostos#keskus+N+Sg+Loc:in<br>
shopping#center+N+Sg+Loc:in<br>
English: ‘in the shopping center’</li>
</ul>
</li>
</ul>
</li>
<li>Word order matters (reordering can change the meaning of sentence)</li>
<li>Every word counts (can&rsquo;t omit word during translation)</li>
<li>Sentences are long sometimes and complex</li>
</ul>
<p><strong>Cues which might be helpful for translation:</strong></p>
<ul>
<li>Pairs of words that occur consecutively in the target language (bigrams)</li>
<li>Pairs of source and target words that occur frequentyl occur together in translations</li>
</ul>
<p><strong>How to model these cues? what are the ways they can fail?</strong>
Let&rsquo;s look at them!!</p>
<h1 id="conditional-language-models-with-n-grams">Conditional Language Models (with n-grams)</h1>
<p>Readings:</p>
<ul>
<li>

<a target=“_blank” href="https://www.learn.ed.ac.uk/bbcswebdav/pid-7935785-dt-content-rid-30666816_1/xid-30666816_1" rel="noopener">Word Alignment and the Expectation Maximization Algorithm,</a> Lopez.</li>
<li> 

<a target=“_blank” href="https://arxiv.org/abs/1703.01619" rel="noopener">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a></li>
</ul>
<p><strong>How to derive an n-gram language model</strong></p>
<p>To define the probability $P(w) = P(w1 . . . w_{|w|})$ for a given sequence of words w, we can use an n-gram language model. An n-gram model estimates the probability of a word sequence by approximating it as the product of probabilities of individual words given their previous n-1 words, where n is a positive integer (usually 1, 2, or 3).</p>
<p>Specifically, we can define P(w) as the product of conditional probabilities of each word given its previous n-1 words:</p>
<p>$$P(w) = P(w_1, w_2, \dots, w_{|w|}) \approx \prod_{i=1}^{|w|} P(w_i | w_{i-1}, w_{i-2}, \dots, w_{i-n+1})$$</p>
<p>where $P(wi | w(i-1), w(i-2), &hellip;, w(i-n+1))$ is the conditional probability of the ith word given its previous n-1 words, and the approximation holds by the Markov assumption that the probability of a word only depends on its previous n-1 words.</p>
<p><strong>Maximizing likelihood by counting n-grams</strong></p>
<p>counting n-grams is one way to estimate the probabilities in an n-gram language model, and it aims to maximize the likelihood of the observed data. The likelihood function L estimates the probability of the observed data given the model parameters (i.e., the n-gram probabilities):</p>
<p>$$L(P(w_1, w_2, &hellip;, w_{|w|})) = \prod_{i=1}^{|w|} P(w_i | w_{i-1}, w_{i-2}, &hellip;, w_{i-n+1})$$
$$
\hat{\theta} = \arg\max_\theta P(D | \theta)
$$</p>
<p>where $w_1, w_2, &hellip;, w_n$ is the observed data. The goal of estimating the n-gram probabilities is to maximize the likelihood function L, given the observed data.</p>
<p>Counting n-grams is a simple and effective method to estimate the n-gram probabilities, where we count the frequency of each n-gram in the training data and normalize it by the count of its preceding (n-1)-gram. However, this method has a <strong>limitation that it assigns zero probability to any n-gram that did not occur in the training data (i.e., zero-frequency problem), which can lead to poor performance in handling unseen data.</strong></p>
<p>To overcome this limitation, <strong>smoothing techniques are often used to estimate the probabilities of unseen n-grams.</strong> Smoothing methods aim to assign non-zero probabilities to unseen n-grams by borrowing probabilities from other n-grams or using prior knowledge.</p>
<p><strong>Predition using n-gram LM</strong></p>
<p>If we have a sequence of words $w_1 . . . w_k$, then we can use the language model to predict the next word $w_{k+1}$:
$$w^*<em>{k+1} = \operatorname{argmax}</em>{k+1} P(w_{k+1} | w_1, w_2, \dots, w_k)$$</p>
<h3 id="can-we-model-machine-translation-with-n-gram">Can we model Machine Translation with n-gram?</h3>
<p>Yes, we can: but there is a lot of machinery involved such as:
Some issue:</p>
<ul>
<li>Length mismatch b/w source and target language.</li>
<li>Change in word order b/w source and target languages</li>
</ul>
<p><strong>Machine translations is Conditional Language Modeling:</strong></p>
<ul>
<li>We will have one-word or phrase from one language mapped to the other language</li>
<li>We will need to model this alignment. Now this alignment is actually unknown and unobserved, so HMM can be a good choice of model here.</li>
</ul>
<p>$$MT = P(|y|||x|)\prod_{i=1}^{|y|} P(z_i||x|)P(y_i|x_{z_i})$$
Here: $P(|y|||x|)$  means for all pairs of sentence lengths |y|, |x|. $P(y_i|x_i)$ is the pairs of all co-occuring words in both languages. $P(z_i||x|)$ probability of how $y_i$ aligns with $x_i$ according to some distribution.</p>
<p>Also: $P(z_i||x|)$ is the <em>transition probability</em> and $P(y_i|x_{z_i})$ is the <em>emission probability</em>. $z$ is the latent variable here which is unobserved. We can use <strong>Expectation-Maximization</strong> to estimate the MLE $\hat{\theta}$  .</p>
<p>Now for <strong>decoding</strong>:
$$
P(y|x) = \frac{P(x|y) P(y)}{P(x)} \propto  P(y) P(x|y)
$$
where: $P(y)$ is <strong>language model</strong> and $P(x|y)$ is the <strong>translation model.</strong></p>
<p><strong>Why care about n-grams? Aren’t they obsolete?</strong></p>
<ol>
<li>
<p>Many of these ideas turn up again in neural models.</p>
<p>• All machine learning maximizes some <strong>objective function</strong>.<br>
• Neural models still use <strong>beam search</strong>.<br>
• <strong>Latent variables</strong> are common in <strong>unsupervised learning</strong>.<br>
• <strong>Alignment</strong> directly inspired <strong>neural attention</strong>.<br>
• Neural models exploit same signals, though more powerful.</p>
</li>
<li>
<p>Older models are still often useful in <strong>low-data settings</strong>.</p>
</li>
<li>
<p>An extension of the model in this lecture translates  n-grams to n-grams: <strong>phrase-based translation</strong>. It is still  used by Google for some languages, despite move to  neural MT in 2017.</p>
</li>
<li>
<p>Understanding the tradeoffs of working with Markov assumptions will help you appreciate the fact that neural models usually make them go away!</p>
</li>
</ol>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week10/</guid><description>Movie Summarization Readings Movie Summarization via Sparse Graph Construction. Papalampidi, Keller, Lapata, 2021.
Motivation
Summarize full-length movies by creating shorter videos containing their most informative scenes. Key Concept
Turning Points Key events in a movie that describe its storyline Problem Formulation</description><content:encoded><![CDATA[<h1 id="movie-summarization">Movie Summarization</h1>
<p>Readings


<a target=“_blank” href="https://arxiv.org/abs/2012.07536" rel="noopener">Movie Summarization via Sparse Graph Construction.</a> Papalampidi, Keller, Lapata, 2021.</p>
<ul>
<li>
<p><strong>Motivation</strong></p>
<ul>
<li>Summarize full-length movies by creating shorter videos containing their most informative scenes.</li>
</ul>
</li>
<li>
<p><strong>Key Concept</strong></p>
<ul>
<li><strong>Turning Points</strong>
<ul>
<li>Key events in a movie  that describe its storyline</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Problem Formulation</strong></p>
<ul>
<li><strong>Data</strong>
<ul>
<li>Let $D$ denote a screenplay consisting of a sequence of scenes $D = {s_1, s_2, \ldots, s_n}$. We aim at selecting a smaller subset $D&rsquo; = {s_i, \ldots, s_k}$ consisting of the most informative scenes describing the movie&rsquo;s storyline.</li>
</ul>
</li>
<li><strong>Objective</strong>
<ul>
<li>Our objective is to assign a binary label $y_i$ to each scene $s_i$ denoting whether it is part of the summary.</li>
</ul>
</li>
<li><strong>Process</strong>
<ul>
<li><strong>Classification Problem:</strong>
<ul>
<li>For each scene $s_i \in D$ we assign a binary label $y_{it}$ denoting whether it represents turning point $t$.</li>
<li>Specifically, we calculate probabilities $p(y_{it}|s_i, D, \theta)$ quantifying the extent to which $s_i$ acts as the $t^{th}$ TP, where $t \in [1, 5]$ (and $\theta$ are model parameters).</li>
</ul>
</li>
</ul>
</li>
<li><strong>Inference</strong>
<ul>
<li>During inference, we compose a summary by selecting $l$ consecutive scenes that lie on the peak of the posterior distribution $\arg\max_{i=1}^N p(y_{it}|s_i, D, \theta)$ for each TP.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Modeling Process</strong>
*</p>
</li>
<li>
<p><strong>What does the graphs mean?</strong></p>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week2/</guid><description>Feedforward Language Model Reading:
A Course in Machine Learning, chapter 4, Daumé. Chapter 7 of Speech and Language Processing, 3rd edition, Jurafsky &amp;amp; Martin. Representing n-gram probabilities with neural networks For neural networks: $\mathbf{x}$ : input vector $\mathbf{h}$ : hidden layer $\mathbf{y}$ : output vector Vector Rrepresentations:</description><content:encoded><![CDATA[<h1 id="feedforward-language-model">Feedforward Language Model</h1>
<p>Reading:</p>
<ul>
<li>

<a target=“_blank” href="http://ciml.info/" rel="noopener">A Course in Machine Learning</a>, chapter 4, Daumé.</li>
<li>Chapter 7 of 

<a target=“_blank” href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener">Speech and Language Processing</a>, 3rd edition, Jurafsky &amp; Martin.</li>
</ul>
<h3 id="representing-n-gram-probabilities-with-neural-networks">Representing n-gram probabilities with neural networks</h3>
<ul>
<li><strong>For neural networks:</strong>
<ul>
<li>$\mathbf{x}$ : input vector</li>
<li>$\mathbf{h}$ : hidden layer</li>
<li>$\mathbf{y}$ : output vector</li>
</ul>
</li>
</ul>
<p><strong>Vector Rrepresentations:</strong></p>
<ul>
<li>one-hot encoding</li>
<li>probability distribution as vector
<ul>
<li>Softmax: turns any vector into a probability distribution</li>
<li>Input: a vector $\mathbf{v}$ of length $|\mathbf{v}|$</li>
<li>Output: softmax($\mathbf{v}$) is a distribution $\mathbf{u}$ over $\mathbf{|v|}$ where: $$u_i = \frac{exp (v_i)}{\sum_{j=1}^{|v|} exp(v_j)}$$</li>
</ul>
</li>
<li><strong>Logistic Regression:</strong>$$ \mathbf{y} = softmax(\mathbf{Wx + b}) $$</li>
<li>$\mathbf{W}$ and $\mathbf{b}$ are parameters of the model</li>
<li>Input $\mathbf{x}$ and output $\mathbf{y}$ have different dimension, which determine the dimension of $\mathbf{W}$ and $\mathbf{b}$
<ul>
<li>$\mathbf{W}$  = $|\mathbf{y}| \times |\mathbf{x}|$</li>
<li>$\mathbf{b}$ is bias = $|\mathbf{y}|$</li>
</ul>
</li>
</ul>
<h3 id="learning-and-objective-functions">Learning and Objective Functions</h3>
<p><strong>Two views</strong>
* <strong>Maximize Likelihood</strong> : Attempt to put as much as probability on the target output
* <strong>Minimize Error</strong> : Minimizes the <strong>cross-entropy</strong> loss, which penalizes the model for the proportion of the output probability mass that it does not assign to the target output.
* Gradient descent: good framework, however, limited in power in practice, use the following to make it more efficient
* with momentum
* individual learning rates for each dimension
* adaptive learning rate
* decoupling step length from partial derivatives</p>
<h1 id="recurrent-networks-for-language-modeling-rnn">Recurrent Networks for Language Modeling (RNN)</h1>
<p>Readings</p>
<ul>
<li>Sections 4 and 5 of 

<a target=“_blank” href="https://arxiv.org/abs/1703.01619" rel="noopener">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a>, Neubig</li>
</ul>
<p>So far we have seen n-gram and Feedforward networks:</p>
<ul>
<li>both have fixed context width, however, linguistic dependencies can be arbitrarily long. This is where RNN come in!</li>
</ul>
<p><strong>- Why not to use a standard network for sequence tasks? There are two problems:</strong></p>
<ul>
<li>
<p><strong>Inputs, outputs can be different lengths in different examples</strong>.</p>
<ul>
<li>This can be solved for normal NNs by paddings with the maximum lengths but it&rsquo;s not a good solution.</li>
</ul>
</li>
<li>
<p><strong>Doesn&rsquo;t share features learned across different positions of text/sequence</strong>.</p>
<ul>
<li>Using a feature sharing like in CNNs can significantly reduce the number of parameters in your model. That&rsquo;s what we will do in RNNs.</li>
</ul>
</li>
<li>
<p>Recurrent neural network doesn&rsquo;t have either of the two mentioned problems.</p>
</li>
<li>
<p><strong>Glossary</strong></p>
<ul>
<li>$x_i$ : the input word transformed into one hot encoding</li>
<li>$y_i$ : the output probability distribution</li>
<li>$\mathbf{U}$ : the weight matrix of the recurrent layer</li>
<li>$\mathbf{V}$ : the weight matrix between the input layer and the hidden layer</li>
<li>$\mathbf{W}$ : the weight matrix between the hidden layer and the output layer</li>
<li>$\sigma$ : the sigmoid activation function</li>
<li>$\mathbf{h}$ : the hidden layer</li>
</ul>
</li>
<li>
<p>A diagram showing a RNN (this in an unfolded representation or a complete computational graph)
*
* No markov assumption is made here: $$
\begin{align}
&amp;\quad P(x_{i+1}|x_1, &hellip;,x_i) = y_i \
&amp;\quad y_i = softmax(\mathbf{Wh_i + b_2}) \
&amp;\quad h_i = \sigma(\mathbf{Vx_i + Uh_{i-1} + b_1}) \
&amp;\quad x_i = onehot(\mathbf{x_i})
\end{align}
$$</p>
<ul>
<li><strong>Training:</strong>
<ul>
<li>SGD with cross entropy</li>
<li><strong>BPTT (backpropagation through time)</strong> can be done through arbitrary number of steps $\tau$. This makes RNN a deep neural network with $\tau$ depth, which can be arbitrarily large
<ul>
<li>In practice we implement <strong>Truncated BPTT</strong>, i.e. we only update gradients only through a limited number of steps.</li>
<li>If we have large number of steps $\tau$, derivatives which are backpropagated becomes vanishingly small, resulting in no weight update. This problem is called <strong>Vanishing Gradient Problem</strong> which prevents the RNN from learning long term dependencies.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="long-short-term-memory-networks-lstms">Long Short-term Memory Networks (LSTMs)</h1>
<p>Readings:</p>
<ul>
<li>Section 6 of 

<a target=“_blank” href="https://arxiv.org/abs/1703.01619" rel="noopener">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a>, Graham Neubig. </li>
<li>

<a target=“_blank” href="http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf" rel="noopener">Backpropagation through Time</a>, Jiang Guo.</li>
</ul>
<p>Helpful Resources: 

<a target=“_blank” href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p>LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) that is designed to handle the vanishing gradient problem in traditional RNNs. LSTMs can effectively capture long-term dependencies in sequential data by allowing information to flow through &ldquo;memory cells&rdquo; that can be selectively read from or written to at each time step.</p>
<p>Here are the main steps in LSTMs:</p>
<ul>
<li>
<p>First we have a  <strong>Forget gate</strong>, which  determines how much of the previous memory cell value should be retained, from cell state $C_{t-1}$</p>
<p>$f_t = \sigma(W_f[x_t, h_{t-1}] + b_f)$</p>
</li>
<li>
<p>Next step, is to decide what new information we&rsquo;re going to store in the cell steate.  <strong>Input gate:</strong> determines how much of the new input should be incorporated into the memory cell. <strong>Candidate memory cell value:</strong> the new value to be potentially added to the memory cell</p>
<p>$i_t = \sigma(W_i[x_t, h_{t-1}] + b_i)$
$\tilde{C}<em>t = \tanh(W_c[x_t, h</em>{t-1}] + b_c)$</p>
</li>
</ul>
<p>It&rsquo;s now time to update the old cell state, $C_{t-1}$ into the new cell state $C_{t}$. <strong>Memory cell update:</strong> combines the input, forget, and candidate values to update the memory cell value</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
</span></span><span class="line"><span class="cl">![[LSTM3-focus-C.png]]
</span></span></code></pre></td></tr></table>
</div>
</div><ul>
<li>
<p>Finally, we need to decide what we’re going to output. <strong>Output gate:</strong> determines how much of the current memory cell value should be output. <strong>Hidden state output:</strong> applies the output gate to the updated memory cell value to compute the hidden state output at the current time step</p>
<p>$o_t = \sigma(W_o[x_t, h_{t-1}] + b_o)$
$h_t = o_t * \tanh(C_t)$</p>
</li>
</ul>
<p>where $x_t$ is the input at time step $t, h_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, \sigma is the sigmoid activation function, and * denotes element-wise multiplication.</p>
<p>These equations allow the LSTM to selectively forget or remember information at each time step, based on the input and the current state of the memory cell. By doing so, LSTMs can effectively capture long-term dependencies in sequential data, making them well-suited for tasks such as natural language processing and speech recognition.</p>
<p><strong>How does LSTM solve the gradient problem?</strong></p>
<ul>
<li>the memory cell is linear, so its gradient doesn’t vanish;
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
</span></span></code></pre></td></tr></table>
</div>
</div></li>
<li>
<p>This is linear.</p>
</li>
<li>
<p>If $f_t$ is 1 and $i_t$ is 0 then we can simply pass the activation from previous cell state $C_{t-1}$ ; this means LSTM block can retain information indefinitely.</p>
</li>
</ul>
</li>
<li>All the gates are trainable; the LSTM block learns when to accept input, produce output and forget information</li>
</ul>
<h1 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h1>
<p>GRUs are similar to LSTMs in that they are also designed to handle the vanishing gradient problem in traditional RNNs, but they have a simpler architecture with fewer parameters. Like LSTMs, GRUs use &ldquo;gates&rdquo; to selectively filter information flow through the network. However, they have a more streamlined approach to gating, with just two gates: an update gate and a reset gate.</p>
<p>Here are the main equations used in GRUs:</p>
<ul>
<li>
<p><strong>Update gate:</strong> determines how much of the previous hidden state should be retained</p>
<p>$z_t = \sigma(W_z[x_t, h_{t-1}] + b_z)$</p>
</li>
<li>
<p><strong>Reset gate:</strong> determines how much of the previous hidden state should be ignored</p>
<p>$r_t = \sigma(W_r[x_t, h_{t-1}] + b_r)$</p>
</li>
<li>
<p><strong>Candidate hidden state output:</strong> a new value to be potentially added to the hidden state output</p>
<p>$\tilde{h}<em>t = \tanh(W_h[x_t, r_t * h</em>{t-1}] + b_h)$</p>
</li>
<li>
<p><strong>Hidden state output:</strong> a linear combination of the current and candidate hidden state values, controlled by the update gate</p>
<p>$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$</p>
</li>
</ul>
<p>where $x_t$ is the input at time step $t, h_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, and \sigma is the sigmoid activation function. * denotes element-wise multiplication.</p>
<p>These equations allow the GRU to selectively update or ignore information at each time step, based on the input and the previous hidden state. The update gate $z_t$ controls how much of the new candidate hidden state value $\tilde{h}<em>t$ should be added to the previous hidden state value $h</em>{t-1}$, while the reset gate $r_t$ controls how much of the previous hidden state value should be ignored.</p>
<p>Overall, the GRU architecture is simpler than that of the LSTM, but it can still effectively capture long-term dependencies in sequential data. By selectively updating or ignoring information at each time step, the GRU can adapt to changes in the input and capture complex patterns in the data.</p>
<h3 id="difference-bw-gru-vs-lstm">Difference b/w GRU v/s LSTM</h3>
<p>One of the main differences is that GRUs have a simpler architecture with fewer parameters than LSTMs. <strong>GRUs use two gates, an update gate and a reset gate, to selectively update or ignore information in the hidden state</strong>, whereas LSTMs have <strong>three gates, an input gate, a forget gate, and an output gate, as well as a memory cell.</strong></p>
<p>Another difference is in the way that the hidden state is updated. In GRUs, <strong>the candidate hidden state value is computed using a single matrix multiplication of the input and the previous hidden state</strong>, while in LSTMs, the <strong>candidate memory cell value is computed separately from the input and the previous hidden state, using two matrix multiplications and an element-wise multiplication.</strong>
In GRUs, the candidate hidden state is computed as the element-wise product of the reset gate $r_t$ and the previous hidden state $h_{t-1}$, which is then concatenated with the input $x_t$ and transformed by a weight matrix W_h and an activation function tanh. In contrast, in LSTMs, the candidate memory cell $\tilde{C}<em>t$ is computed separately from the input $x_t$ and the previous hidden state $h</em>{t-1}$, using a weight matrix $W_C$ and an activation function $tanh$.
Furthermore, the memory cell $C_t$ in LSTMs is updated using both the forget gate $f_t$ (which controls how much of the previous memory cell state to retain) and the input gate $i_t$ (which controls how much of the new candidate memory cell value to use). The hidden state $h_t$ is then computed using the output gate $o_t$ (which controls how much of the memory cell state to output) and the updated memory cell $C_t$. In GRUs, the hidden state is computed directly using the candidate hidden state and the update gate $z_t$.</p>
<p>The differences in architecture between GRUs and LSTMs have implications for their performance in different tasks. For example, GRUs may be more suitable for tasks that require faster training and less complex models, while LSTMs may be more suitable for tasks that require modeling more complex dependencies and handling longer sequences of data.</p>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week4/</guid><description>Word Embeddings Readings:
Efficient estimation of word representations in vector space. Mikolov et al., NIPS Workshop 2013.
Contextual word representations: A contextual introduction. Smith, 2019. This paper provides a conceptual overview of word embeddings, and explains why contextualized embeddings are an important innovation</description><content:encoded><![CDATA[<h1 id="word-embeddings">Word Embeddings</h1>
<p>Readings:</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1301.3781" rel="noopener">Efficient estimation of word representations in vector space</a>. Mikolov et al., NIPS Workshop 2013.</p>
</li>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1902.06006" rel="noopener">Contextual word representations: A contextual</a> 

<a target=“_blank” href="https://arxiv.org/abs/1902.06006" rel="noopener">introduction.</a> Smith, 2019. This paper provides a conceptual overview of word embeddings, and explains why contextualized embeddings are an important innovation</p>
</li>
<li>
<p><strong>Pre-training</strong>: train a generic source model (e.g., VGG-16) on a standard, large dataset (e.g., ImageNet). You can think of pre-training as a way of initializing the parameters of your target model to good values.</p>
</li>
<li>
<p><strong>Finetuning</strong>: then take the resulting model, keep its parameters, and replace the output layer to suit the new task. Now train this target model on the dataset for the new task. Transfer learning by finetuning.</p>
</li>
</ul>
<p>While finetuning, keep following things in mind:</p>
<ul>
<li>
<p>we typically have fewer output classes in the finetuning class</p>
</li>
<li>
<p>use smaller learning rate</p>
</li>
<li>
<p>finetune only a handful of layer, while keeping other fixed: <strong>weight freezing</strong></p>
</li>
<li>
<p><strong>source model in finetuning is usually a neural language model</strong>, however the target task is very different from that, as the target task is rarely <strong>Next-Word-Prediction</strong></p>
</li>
<li>
<p>We can all weights of embedding layer for the target model to be trained - this is a limited form of <strong>finetuning.</strong>  - We can do full-scale finetuning for NLP, which is possible because of contextualized word embeddings.</p>
</li>
<li>
<p><strong>Static Word Embeddings</strong>:</p>
<ul>
<li>Eg: <strong>Word2Vec</strong>, <strong>Glove</strong>, <strong>FastText</strong></li>
<li>They assign fixed vector to each word; <strong>context independent</strong></li>
<li>Used for initializing the embedding layers of a target model; i.e. for feature extraction</li>
<li><strong>Not designed for finetuning</strong></li>
<li>efficient and can be trained from scratch with humble resources</li>
</ul>
</li>
<li>
<p><strong>Contextualized Word Embeddings</strong></p>
<ul>
<li>Eg: Bert, GPT etc.</li>
<li>Often called pretrained language model or **LLM (large language models)</li>
<li>assign a vector to a word that depends on its context: i.e. on its preceding and following words - <strong>context dependent</strong></li>
<li>can be used in target model to initialize embeddings like static embeddings</li>
<li><strong>designed to be finetuned</strong> - we re-train some of the weights of the embedding model for the target task.</li>
<li><strong>requires considerable resources to train from scratch</strong>; finetuning is efficient way of using them</li>
</ul>
</li>
</ul>
<h1 id="static-word-embeddings">Static Word Embeddings</h1>
<ul>
<li>
<p><strong>Word2Vec : Continuous Bag-of-words model (CBOW)</strong></p>
</li>
<li></li>
<li>
<p><strong>CBOW</strong>: uses words within a context window to predict the current word (5 window size is used)</p>
</li>
<li>
<p>architecture:</p>
<ul>
<li>has only a single, linear hidden layer</li>
<li>Weights for differnt position are shared</li>
</ul>
</li>
<li>
<p><strong>Word2Vec: Skipgram</strong></p>
</li>
<li></li>
<li>
<p><strong>Skipgram:</strong> uses the current word to predict the context words</p>
</li>
<li>
<p>architecture:</p>
<ul>
<li>flipped version of CBOW</li>
</ul>
</li>
<li>
<p><strong>Details:</strong></p>
<ul>
<li>
<p>Objective function maximizes the probability of the corpus D (set of all word and context pairs extracted from text):</p>
<ul>
<li>
<p>$$</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">  \arg\max_\theta \prod_{(w,c) \epsilon D} p(c|w;\theta)
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
<p>$$</p>
</li>
<li>
<p>Here: $p(c|w;\theta)$  is defined as:
$$
p(c|w;\theta) = \frac{exp(v_c . v_w)}{\sum_{c^<code> \epsilon C}exp(V_{c^</code>} . v_w)}
$$</p>
</li>
<li>
<p>$v_c$, $v_w$ are vector representation of $c$ and $w$ and $C$ is the set of all available contexts</p>
</li>
<li>
<p>$W$ - word vocabulary</p>
</li>
<li>
<p>$C$ - Context vocabulary</p>
</li>
<li>
<p>$d$ - embedding dim</p>
</li>
<li>
<p>each $w  \epsilon  W$  is associated with vector $v_w \epsilon \mathbb{R}^d$</p>
</li>
<li>
<p>each $c \epsilon C$  is associated with vector $v_c \epsilon \mathbb{R}^d$</p>
</li>
<li>
<p>NOTE: the formulation is impractical due to $\sum_{c^<code> \epsilon C}exp(V_{c^</code>} . v_w)$ over all context $c^`$ . This makes the softmax too expensive to compute for large vocabulary. Negative sampling helps us make the sum tractable.</p>
</li>
</ul>
</li>
<li>
<p><strong>Negative Sampling</strong></p>
<ul>
<li>Check if you have time: 

<a target=“_blank” href="https://arxiv.org/pdf/1402.3722.pdf" rel="noopener">word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method</a> - has the derivation of negative sampling objective and good information on skip-gram model.</li>
<li>The idea is to not evaluate the full output layer based on the hidden layer.
<ul>
<li>Treat this as independent logistic regression classifiers (i.e. <strong>replace softmax objective with a binary classification objective</strong>)
<ul>
<li>The model is trained to distinguish
<ul>
<li>the positive class / <strong>target word</strong></li>
<li>a few randomly sampled neurons or <strong>randomly sampled negative words</strong></li>
</ul>
</li>
<li>This make training speech independent of vocabulary size and this can be parallelized</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Detailed derivation of Negative sampling Objective:</strong></p>
</li>
</ul>
<p>Consider a pair $(w, c)$ of word and context. Did this pair come from the   training data? Let’s denote by $p(D = 1|w, c)$ the probability that (w, c) came   from the corpus data. Correspondingly, $p(D = 0|w, c) = 1 − p(D = 1|w, c)$ will   be the probability that $(w, c)$ did not come from the corpus data. As before,  assume there are parameters θ controlling the distribution: $p(D = 1|w, c; θ)$.</p>
<p>$$
\begin{align}
&amp;\quad \arg\max_{\theta} \prod_{(w,c)\in D} p(D=1|w,c;\theta)\ &amp;\quad = \arg\max_{\theta} \log \prod_{(w,c)\in D} p(D=1|w,c;\theta)\ &amp;\quad = \arg\max_{\theta} \sum_{(w,c)\in D} \log p(D=1|w,c;\theta)
\end{align}
$$
where $p(D=1|w,c;\theta)$ can be defined using softmax:
$$ \begin{align}</p>
<p>&amp;\quad p(D=1|w,c;\theta) = \frac{1}{1+e^{-v_c \cdot v_w}} \
\end{align}
$$
Leading to the objective:
$$\begin{align}
&amp;\quad \Rightarrow \arg\max_{\theta} \sum_{(w,c)\in D} \log \frac{1}{1+e^{-v_c \cdot v_w}}
\end{align}
$$
This objective has a trivial solution if we set $θ$ such that $p(D = 1|w, c; θ) = 1$   for every pair $(w, c)$. This can be easily achieved by setting θ such that $v_c = v_w$  and $v_c · v_w = K$ for all $v_c$, vw , where K is large enough number (practically, we   get a probability of 1 as soon as $K ≈ 40)$.<br>
We need a mechanism that prevents all the vectors from having the same  value, by disallowing some $(w, c)$ combinations. One way to do so, is to present  the model with some $(w, c)$ pairs for which $p(D = 1|w, c; θ)$ must be low, i.e.  pairs which are not in the data. This is achieved by generating the set $D^′$  of random $(w, c)$ pairs, assuming they are all incorrect (the name “negative-sampling” stems from the set $D^′$ of randomly sampled negative examples). The  optimization objective now becomes:</p>
<p>$$
\begin{align}
&amp;\quad= \arg\max_{\theta} \prod_{(w,c)\in D} p(D=1|c,w;\theta) \prod_{(w,c)\in D&rsquo;} (1 - p(D=1|c,w;\theta)) \
&amp;\quad= \arg\max_{\theta} \prod_{(w,c)\in D} p(D=1|c,w;\theta) \prod_{(w,c)\in D&rsquo;} (1 - p(D=1|c,w;\theta)) \
&amp;\quad= \arg\max_{\theta} \sum_{(w,c)\in D} \log p(D=1|c,w;\theta) + \sum_{(w,c)\in D&rsquo;} \log(1-p(D=1|w,c;\theta)) \
&amp;\quad= \arg\max_{\theta} \sum_{(w,c)\in D} \log \frac{1}{1+e^{-v_c\cdot v_w}} + \sum_{(w,c)\in D&rsquo;} \log \frac{1}{1+e^{v_c\cdot v_w}} \
&amp;\quad= \arg\max_{\theta} \sum_{(w,c)\in D} \log \frac{1}{1+e^{-v_c\cdot v_w}} + \sum_{(w,c)\in D&rsquo;} \log \frac{1}{1+e^{v_c\cdot v_w}}
\end{align}
$$</p>
<p>If we let $σ(x) = \frac{1}{1+e^{−x}}$ we get:
$$
\begin{align}
&amp;\quad
\arg\max_{\theta} \sum_{(w,c)\in D} \log \frac{1}{1+e^{-v_c\cdot v_w}} + \sum_{(w,c)\in D&rsquo;} \log \frac{1}{1+e^{v_c\cdot v_w}} \
&amp;\quad= \arg\max_{\theta} \sum_{(w,c)\in D} \log \sigma(v_c\cdot v_w) + \sum_{(w,c)\in D&rsquo;} \log \sigma(-v_c\cdot v_w)
\end{align}
$$
This is similar to what we have in Mikolov&rsquo;s paper, however they assume $D^\cdot \epsilon D$  . i.e. the $(w,c)$ negative samples are taken from the dataset itself. hence their equation is</p>
<p>$$
\arg\max_{\theta} \sum_{(w,c)\in D} \log p(c|w) = \sum_{(w,c)\in D} \left( \log e^{v_c\cdot v_w} - \log \sum_{c&rsquo;} e^{v_{c&rsquo;}\cdot v_w} \right)
$$</p>
<hr>
<blockquote>
<p><strong>why does CBOW do better on syntactic tasks, given that word position isn&rsquo;t a factor, and skip-gram do better on semantic tasks?</strong></p>
</blockquote>
<p>Some types of syntactic tasks may not require, long-term context information or word order information, let&rsquo;s take these examples:</p>
<ol>
<li>
<p>&ldquo;The dogs chase the cat&rdquo;</p>
</li>
<li>
<p>&ldquo;The big dog barks&rdquo;</p>
</li>
<li>
<p>&ldquo;The dog that chases the cat barks loudly&rdquo;</p>
</li>
</ol>
<p>In case 1) the relationship between the word &ldquo;dogs&rdquo; is in the local context of the verb &ldquo;chase&rdquo;, and the verb can be inferred to be in the plural form to match the subject.</p>
<p>In case 2) the word &ldquo;dog&rdquo; is in the local context of the adjective &ldquo;big&rdquo;, and the adjective can be inferred to be in the singular masculine form to match the noun.</p>
<p>Now, given that word order information is not present in CBOW, this grammatical information may be inferred indirectly from the distributional semantics of the words which often occur together. Maybe because some tasks such as subject-verb agreement or noun-adjective agreement, can be performed based on local context without positional information, CBOW learns to focus on the syntactic properties instead of semantic relations between words. (<em>Neural network has a tendency to take shortcuts</em>).</p>
<p>However, in case 3) where the agreement relationship is more complex or depends on longer-range dependencies, positional information may be necessary to capture the relationship accurately. In this case, CBOW may not perform well and skip-gram which better captures semantic relationships may serve better.</p>
<p>now; why does skip-gram do better on semantic tasks?</p>
<p>It is probably because of the architecture; skip-gram predicts the neighbouring words, which requires more semantic information. By predicting the context words based on the target word, skip-gram is able to capture broader semantic relationships between words that may not be captured by CBOW.</p>
<p>Hopefully, this helps.</p>
<p>This is also a good answer on SO: 

<a target=“_blank” href="https://ai.stackexchange.com/a/18637" rel="noopener">https://ai.stackexchange.com/a/18637</a> if you want to check out.</p>
<hr>
<h1 id="pretrained-language-models">Pretrained Language Models</h1>
<p>Readings:</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1810.04805" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. Devlin et al., NAACL 2019.</p>
</li>
<li>
<p>Using transformers, we can build language models that represent context very well: <strong>contextualized embeddings</strong>.</p>
</li>
<li>
<p>These language models can be <strong>used for feature extraction</strong>, but also in a <strong>pre-training/finetuning setup</strong>.</p>
</li>
</ul>
<h3 id="bert-httpsaclanthologyorgn19-1423pdf">

<a target=“_blank” href="https://aclanthology.org/N19-1423.pdf" rel="noopener">Bert </a></h3>
<ul>
<li>
<p>Bert (Bidirectional Encoder Representation from Transformer)</p>
</li>
<li>
<p><strong>Purpose</strong> :</p>
<ul>
<li><strong>designed for pre-training</strong> deep bidirectional representations from unlabeled text;</li>
<li><strong>conditions on left and right context</strong> in all layers;</li>
<li>pre-trained model can be <strong>finetuned with one additional output layer for many tasks</strong> (e.g., NLI, QA, sentiment);</li>
<li>for many tasks, no modifications to the Bert architecture are required;</li>
</ul>
</li>
<li>
<p><strong>Architecture</strong>:</p>
<ul>
<li>Uses bidirectional representation - i.e. <strong>uses the left and right context both</strong>
<ul>
<li>In contrast:
<ul>
<li>GPT - uses only left-to-right context only (unidirectional)</li>
<li>ELMO - uses shallow concatenation of independently trained left-to-right and right-to-left LSTMs</li>
</ul>
</li>
</ul>
</li>
<li><strong>Input / Output Representation</strong>:
<ul>
<li><strong>Input sequence:</strong> can be &lt;Question, Answer&gt; pair, single sentence, or any other string of tokens;</li>
<li><strong>Token Types:</strong>
<ul>
<li>30,000 token vocabulary, represented as <strong>WordPiece embeddings</strong> (handles OOV words);</li>
<li>first token is always [<strong>CLS</strong>]: aggregate sentence representation for classification tasks;</li>
<li>sentence pairs separated by [<strong>SEP</strong>] token; and by <strong>segment embeddings</strong>;
<ul>
<li><strong>segment embedding</strong>: segment embeddings are identical across all sub-words of a segment and only differ if input strings containing multiple  segments are fed into the model simultaneously (e.g. an  input containing two sentences separated by a [SEP]  token).</li>
<li>the intuition behind segment embeddings is less obvious, especially  for unsegmented (i.e. single sentence) input
<ul>
<li>While absent in the original Transformer, <strong>BERT further  adds segment embeddings to the non-contextualized input representation</strong>. In practice, this means that the input is combined with a sequence of segment IDs (a lookup of the sequence “0 0 &hellip; 0 1 &hellip; 1 1” ), which are found  in a lookup table of size 2 × embedding dimensionality (768 in BERTbase).</li>
<li>Actually gets applied for the NSP task. Two sentences are fed as input to BERT and the model is asked to discriminate  usbetween the true next sentence and a randomly sampled alternative sentence occurring 50% of the time. For this task, it is important for the model to be able to distinguish between the two input sentences, hence segment embeddings are used.</li>
</ul>
</li>
<li>

<a target=“_blank” href="https://aclanthology.org/2022.lrec-1.152.pdf" rel="noopener">https://aclanthology.org/2022.lrec-1.152.pdf</a></li>
</ul>
</li>
<li>token position represented by <strong>position embeddings</strong>.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Pretraining and Finetuning approaches</strong>:</p>
<ul>
<li>
<p>The main reason why Bert is famous - Pretraining-Finetuning (though GPT can also be used in the same way)</p>
</li>
<li>
<p><strong>Pre-training BERT</strong>:</p>
<ul>
<li>
<p><strong>Task1: Masked LM</strong></p>
<ul>
<li><strong>Try to predict the masked-tokens (this is simlar to skip-gram, where we predict the surrounding words)</strong></li>
<li>We simply <strong>mask some percentage of the input  tokens at random</strong>, and then predict those masked  tokens. We refer to this procedure as a “masked  LM” (MLM), although it is often referred to as a  <strong>Cloze task</strong> in the literature.</li>
<li>mask 15% of the tokens in the input sequence; train the model to predict these using the context word
<ul>
<li>This however creates a mismatch between pre-training and fine-tuning, since the [MASK] token does not appear during fine-tuning.
<ul>
<li>do not always replace masked words with [MASK], instead choose 15% of token positions at random for prediction;
<ul>
<li>if $i^{th}$ token is chosen, we replace the $i^{th}$ token with:
<ol>
<li>the [MASK] token 80% of the time;</li>
<li>a random token 10% of the time;</li>
<li>the unchanged $i^{th}$ token 10% of the time.</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li>Now use $T_i$ to predict original token with cross entropy loss.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Task2: Next Sentence Prediction (NSP)</strong></p>
<ul>
<li>Tasks like Question-Answering (QA) and NLI are based on relationship b/w two sentences, which is not directly captured by language modeling.</li>
<li>Solution: Pre-train for a binarized NSP task; trivially this can be generated on any monolingual corpus. If we have two sentences $A$ and $B$ such that $B$ follows $A$
<ul>
<li>50% of the time $B$ is the actual next setence.</li>
<li>50% of the time $B$ is a random sentence from the corpus.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Pretraining-Finetuning can be understood as:</p>
<ul>
<li>(1) sentence pairs in <strong>paraphrasing</strong>,</li>
<li>(2) hypothesis-premise pairs in <strong>entailment</strong>,</li>
<li>(3)  question-passage pairs in <strong>question answering</strong></li>
<li>(4) a degenerate $text-∅$ pair in text classification or <strong>sequence tagging</strong>.</li>
</ul>
</li>
<li>
<p>Token Level Task v/s Full sentence task:</p>
<ul>
<li><strong>Token Level:</strong>
<ul>
<li>At the output, the token representations are fed into an output layer for token-level tasks, such as <strong>sequence tagging</strong> (eg: NER, POS) or <strong>question answering</strong>  (predicts span),</li>
</ul>
</li>
<li><strong>Sentence Level:</strong>
<ul>
<li>the $[CLS]$ representation is fed into an output layer for classification, such as <strong>entailment</strong> or <strong>sentiment analysis</strong> (predict class label)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week5/</guid><description>Language models are unsupervised multitask learners: Moving from single task to multi-task | this time by using textual prompts.
Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution $p(output|input)$.</description><content:encoded><![CDATA[<h3 id="language-models-are-unsupervised-multitask-learners">Language models are unsupervised multitask learners:</h3>
<p><strong>Moving from single task to multi-task | this time by using textual prompts.</strong></p>
<ul>
<li>
<p>Learning to perform a single task can be expressed in a  probabilistic framework as estimating a conditional distribution $p(output|input)$. Since a general system should be  able to perform many different tasks, even for the same  input, it should condition not only on the input but also  on the task to be performed. That is, it should model  $p(output|input, task)$.</p>
<p>For example, a translation training example can be written as the sequence</p>
<ul>
<li>$(\text{translate to french}, \text{english text}, \text{french text})$ .</li>
<li>In order to help it infer that  this is the desired task, we condition the language model  on a context of example pairs of the format $\text{english  sentence} = \text{french sentence}$ and then after a final prompt of $\text{english sentence =}$ . The model samples and gives translation.
Likewise, a reading comprehension training example can  be written as</li>
<li>$(\text{answer the question}, document,  question, answer)$</li>
</ul>
</li>
</ul>
<p><strong>Conclusion:</strong>
When a large language model is trained on a sufficiently  large and diverse dataset it is able to perform well across  many domains and datasets. The diversity of tasks the model is able to  perform in a zero-shot setting suggests that high-capacity  models trained to maximize the likelihood of a sufficiently  varied text corpus begin to learn how to perform a surprising  amount of tasks without the need for explicit supervision</p>
<h1 id="prompting">Prompting</h1>
<p>Readings:</p>
<ul>
<li>Sections 1-3 

<a target=“_blank” href="https://arxiv.org/abs/2107.13586" rel="noopener">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language</a> 

<a target=“_blank” href="https://arxiv.org/abs/2107.13586" rel="noopener">Processing</a>, Liu et al. (2021)</li>
<li>

<a target=“_blank” href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener">Language models are unsupervised multitask learners</a>, Radford et al. (2019)</li>
<li>

<a target=“_blank” href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener">Improving Language Understanding by Generative Pre-Training</a>- GPT</li>
</ul>
<h3 id="a-short-overview-of-change-in-language-modeling-paradigm">A short overview of change in language modeling paradigm</h3>
<ul>
<li>At the start we used to have <code>fully-supervised learning</code> modeling. Here, we were doing task specific supervised modeling
<ul>
<li><code>feature-engineering</code>: domain knowledge requirement</li>
<li>then, we moved towards <code>architecture-engineering</code>, for automatic learning of features for the supervised task.</li>
</ul>
</li>
<li>Then we moved towards <code>pretrain-finetune</code> paradigm. <code>pre-train</code> a LM on a general purpose dataset as that is available in abundance. Adapt the <code>pre-trained</code> LM to downstream tasks by introducing additional parameters and <code>finetune</code> using task-specific objective functions.
<ul>
<li>focus is shifted to <code>objective-engineering</code> i.e. the training objectives to be used at both <code>pretraining</code> and <code>finetuning</code> stage. We add a pre-training task which is similar to downstream task, this improves performance on the downstream task later.</li>
</ul>
</li>
<li>Now we are moving towards <code>pre-train, prompt, predict</code> paradigm. Instead of adapting pre-trained LMs to downstream tasks via <code>objective-engineering</code>, we are reformulating the downstream tasks to look more like those solved during the original LM training with the help of textual <code>prompt</code>
<ul>
<li>Eg:<code>“I felt so ___” </code>, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt <code>“English: I missed the bus today. French: ”</code>), an LM may be able to fill in the blank with a French translation.</li>
</ul>
</li>
</ul>
<h3 id="prompting-basics">Prompting Basics</h3>
<ul>
<li>
<p><strong>Terminologies</strong>:</p>
<ul>
<li><code>prefix prompt</code>: variety of prompt where the input text comes entirely before $\bf{z}$</li>
<li><code>cloze prompt</code>: the first variety of prompt with a slot to fill in the middle of the text</li>
</ul>
</li>
<li>
<p><strong>Prompt Addition</strong>: $f_{prompt}(x)$ is applied on $\bf{x}$ to to generate $\mathbf{x}&rsquo; = f_{prompt}(x)$</p>
<ol>
<li>Apply a template, which is a textual string that has two slots: an input slot [X] for input x and an answer slot [Z] for an intermediate generated answer text z that will later be mapped into y.</li>
<li>Fill slot [X] with the input text $\bf{x}$.</li>
</ol>
</li>
<li>
<p><strong>Answer Search</strong>:</p>
<ul>
<li>we search for the highest-scoring text $\bf{z}ˆ$ that maximizes the score of the LM. We first define $Z$ as a set of permissible values for $\bf{z}$.
$$
\hat{z} = \underset{z \epsilon Z}{search} P(f_{fill}(x&rsquo;, z);\theta)
$$</li>
<li>$Z$ could take variety of input:
<ul>
<li><strong>classification</strong>: could be a small subset of the words <code>{“excellent”, “good”, “OK”, “bad”, “horrible”}</code> or <code>{++, +, ~, -, --}</code></li>
<li><strong>regression</strong>: continuous values, constants</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Answer Mapping</strong>: we would like to go from the highest-scoring answer $zˆ$ to the highest-scoring output $yˆ$. This is trivial for cases, where answer itself is the output, however for cases where multiple result could result in the same output, we need a mapping function:</p>
<ul>
<li>sentiment-bearing words (e.g. “excellent”, “fabulous”, “wonderful”) to represent a single class (e.g. “++”)</li>
</ul>
</li>
<li>
<p><strong>Design Considerations for Prompting</strong></p>
</li>
</ul>
<h3 id="pre-trained-language-models">Pre-trained Language Models</h3>
<p>Systematic view of various pre-trained LMs:</p>
<ul>
<li><strong>main training objective</strong>
<ul>
<li>auxiliary training objective</li>
</ul>
</li>
<li><strong>type of text noising</strong></li>
<li><strong>Directionality</strong>: attention mask</li>
</ul>
<h4 id="main-training-objective">Main Training Objective</h4>
<p>The main training objective of the pre-trained LMs plays an important role in determining its applicability to particular prompting tasks.</p>
<ul>
<li>
<p><strong>Standard Language Model (SLM)</strong></p>
<ul>
<li>Autoregressive prediction (left to right)
<ul>
<li>These are particularly suitable for <code>prefix prompts</code></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Denoising Objective</strong>:</p>
<ul>
<li>Noising function: $\tilde{f} = f_{noise}(x)$</li>
<li>Task to predict: $P(x|\tilde{x})$</li>
<li>These types of reconstruction objectives are suitable for <code>cloze prompts</code></li>
<li>Two common types of denoising objectives
<ul>
<li><strong>Corrupted Text Reconstruction (CTR)</strong>: the processed text to its uncorrupted state by calculating <em>loss over only the noised parts</em> of the input sentence</li>
<li><strong>Full Text Reconstruction (FTR)</strong>: reconstruct the text by <em>calculating the loss over the entirety of the input texts</em> whether it has been noised or not</li>
</ul>
</li>
<li><strong>Noising Functions</strong>
<ul>
<li>the specific type of corruption applied to obtain the noised text $\tilde{x}$ has an effect on the efficacy of the learning algorithm</li>
<li><strong>prior knowledge can be incorporated by controlling the type of noise</strong>, e.g. <em>the noise could focus on entities of a sentence, which allows us to learn a pre-trained model with particularly high predictive performance for entities</em></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>SLM</strong> or <strong>FTR</strong> objectives are maybe more suitable for <em>generation tasks</em></p>
</li>
<li>
<p>tasks such as <em>classification</em> can be formulated using models trained with any of these objectives</p>
</li>
<li>
<p><strong>Auxiliary Training Objective</strong>:</p>
<ul>
<li>improve models’ ability to perform certain varieties of downstream tasks.</li>
<li><strong>Next Sentence Prediction</strong>: Next Sentence Prediction: do two segments appear consecutively - better sentence representations - <code>BERT</code></li>
<li><strong>Discourse Relation Prediction</strong>: predict rhetorical relations between sentences - better semantics - <code>ERNIE [Sun et al., 2020]</code></li>
<li><strong>Image Region Prediction</strong>: predict the masked regions of an image - for better visual-linguistic tasks - <code>VL-BERT [Su et al., 2020]</code></li>
</ul>
</li>
</ul>
<h4 id="directionality-type-of-attention-masking">Directionality (Type of attention masking)</h4>
<ul>
<li>
<p>pre-trained LM can be different based on the directionality of the calculation of representations</p>
</li>
<li>
<p><strong>Bidirectional:</strong> full attention no masking</p>
</li>
<li>
<p><strong>Left-to-right:</strong> diagonal attention masking</p>
</li>
<li>
<p>Mix the two strategies</p>
</li>
</ul>
<h4 id="typical-pre-training-methods">Typical Pre-training Methods</h4>
<p>Following is a representation of popular pre-training methods:</p>
<ul>
<li><strong>Left-to-Right Language Model</strong>
<ul>
<li>Popular backbone for many prompting methods. Representative examples of modern pre-trained left-to-right LMs include <strong>GPT-3</strong> , and <strong>GPT-Neo</strong></li>
<li>Generally large and difficult to train - generally not available to public, thus <code>pretraining and finetuning</code>  is generally not possible</li>
<li>Useful for <strong>generative tasks</strong></li>
</ul>
</li>
<li><strong>Masked Language Models</strong>
<ul>
<li>Take advantage of full context. When the focus is shifted on generating optimal representation for downstream tasks.</li>
<li>BERT is a popular example which aims to predict masked text pieces based on surrounded context</li>
<li>In prompting methods, MLMs are generally most suitable for <strong>natural language understanding or analysis tasks</strong> (e.g., text classification, natural language inference , and extractive question answering).</li>
<li>Suitable for <code>cloze prompting</code>.</li>
<li><code>pretraining-finetuning</code> is generally possible</li>
</ul>
</li>
<li><strong>Prefix and Encoder-Decoder</strong>
<ul>
<li>Useful for conditional text-generation tasks such as <strong>translation</strong> and <strong>summarization</strong>
<ul>
<li>such tasks need a pre-trained model both capable of endcoding the text and generating the output</li>
</ul>
</li>
<li>(1) using an encoder with <strong>fully-connected mask</strong> (full-attention, no masking) to encode the source $x$ first and then (2) decode the target $y$ <strong>auto-regressively</strong> (from the left to right)</li>
<li><strong>In Prefix-LM</strong>: Encoder-Decoder weights are shared. So same parameters are used to encode $x$ and $y$
<ul>
<li>Eg: UniLM 1-2, ERNIE-M</li>
</ul>
</li>
<li><strong>In Encoder-Decoder</strong>: Weights are different for E &amp; D. $x$ is encoded using encoder weight whereas, $y$ is encoded using decoder weight.
<ul>
<li>Eg: T5, BART</li>
</ul>
</li>
<li>These models were typically used for <strong>text generation purposes</strong>, however, recently they are <strong>being used for non-generation tasks</strong> such as QA, Information Extraction etc.</li>
</ul>
</li>
</ul>
<h3 id="prompt-engineering">Prompt Engineering</h3>
<ul>
<li>Creating a promtping function $f{prompt}(x)$</li>
<li>Manual template engineering</li>
<li>Automated template learning of discrete prompts:
<ul>
<li>Prompt mining ”[X] middle words [Z]”</li>
<li>Paraphrase existing prompts - select the ones with highest accuracy</li>
</ul>
</li>
<li>Continuous prompts: perform prompting directly in the embedding space of the model
<ul>
<li>Initialise with discrete prompt, fne tune on task</li>
<li>Template embeddings have their own parameters that can be tuned</li>
</ul>
</li>
</ul>
<h3 id="training">Training</h3>
<ul>
<li>Promptless fne-tuning (BERT, ELMO)</li>
<li>Tuning free prompting: zero-shot (GPT3)</li>
<li>Fix prompt tune LM (T5)</li>
<li>Additional prompt parameteres:
<ul>
<li>Fix LM tune prompt</li>
<li>Tune LM and prompt (high resource)</li>
</ul>
</li>
</ul>
<h3 id="t5-text-to-text-transfer-transformer">T5 (Text-to-Text Transfer Transformer)</h3>
<p>Readings:</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></p>
</li>
<li>
<p>Model Size: up to 11B parameters - BERT-large is 330M</p>
</li>
<li>
<p>Amount of training data: 120B words of data</p>
</li>
<li>
<p>Domain/Cleanness of training data</p>
</li>
<li>
<p>Pretraining objective</p>
</li>
<li>
<p>Finetuning recipe</p>
</li>
<li>
<p><strong>Conclusion</strong>: Scaling up model size and training data really helps Really easy to use pretrained model for multiple tasks using prompts!</p>
</li>
</ul>
<h1 id="evaluating-machine-translation-systems">Evaluating Machine Translation Systems</h1>
<p>Readings:</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/P02-1040.pdf" rel="noopener">Bleu: a method for automatic evaluation of machine translation</a>, Papenini et al. (2002)</p>
</li>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/2020.emnlp-main.213/" rel="noopener">COMET: A neural framework for MT evaluation</a>, Rei et al. (2020)</p>
</li>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/2021.wmt-1.57/" rel="noopener">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation,</a> Kocmi et al. (2021)</p>
</li>
<li>
<p><strong>Why do we need to evaluate machine translation systems?</strong></p>
<ul>
<li>Decide which of two (or more) systems to use.</li>
<li>Evaluate incremental changes to systems.
<ul>
<li>Does a new idea make it better or worse?</li>
<li>Does it change things in the intended way?</li>
</ul>
</li>
<li>Decide whether a system is appropriate for a given use case.
<ul>
<li>Understanding a restaurant menu. - allergy (life/death)</li>
<li>Understanding a news about safety of a city you are visiting.</li>
<li>Translating legal notices of a product you are selling. (MS - 150 languages, need to get client specifications right)</li>
<li>Negotiating a peace treaty</li>
</ul>
</li>
<li>Different level/severity of consequences:
<ul>
<li>assimilate - understand what is written / do some analysis</li>
<li>disseminate - BBC news</li>
</ul>
</li>
</ul>
</li>
<li>
<p>There can be different translations from different translators. A good translation is both <strong>adequate</strong> and <strong>fuent</strong></p>
<ul>
<li><strong>Adequate</strong>: Does the output convey the same meaning as the input sentence? Is part of the message lost, added, or distorted?</li>
<li><strong>Fluent</strong>: Is the output good fuent English? Is is grammatically correct? Does it use appropriate words and idioms?</li>
</ul>
</li>
<li>
<p>Axis of quality of MT:</p>
<ul>
<li>Adequacy:
<ul>
<li>Earlier evaluated like this:
<ul>
<li><code>5) all meaning 4) most meaning 3) much meaning 2) little meaning 1) none</code></li>
</ul>
</li>
</ul>
</li>
<li>Fluency:
<ul>
<li>Earlier evaluated like this:
<ul>
<li><code>5) flawless english 4) good english 3) non-native english 2) dis-fluent english 1) incomprehensible</code></li>
</ul>
</li>
</ul>
</li>
<li>Rating</li>
<li>This is subjective, inconsistent, and non-reproducible.</li>
<li>High inter-annotator disagreeability</li>
</ul>
</li>
<li>
<p><strong>Goals for Evaluation Metrics</strong></p>
<ul>
<li><strong>Low cost</strong>: reduce time and money spent on carrying out evaluation</li>
<li><strong>Tunable</strong>: automatically optimize system performance towards metric</li>
<li><strong>Meaningful</strong>: score should give intuitive interpretation of translation quality</li>
<li><strong>Consistent</strong>: repeated use of metric should give same results</li>
<li><strong>Correct</strong>: metric must rank better systems higher</li>
</ul>
</li>
<li>
<p><strong>Measuring agreement between evaluators</strong>
$$\kappa = \frac{\rho{(A)} - \rho{(E)}}{ 1 - \rho{(E)}}$$</p>
<ul>
<li>$\rho{(A)}$ is proportion of times that evaluators agree</li>
<li>$\rho{(E)}$ is proportion of times that they would agree by chance</li>
<li>Adequacy, Fluecy are very hard to measure, low but positive agreement empirically</li>
<li>Agreement on rating is relatively higher, but still low</li>
</ul>
</li>
<li>
<p><strong>Can we evaluate automatically?</strong></p>
<ul>
<li>Idea: Use human annotated reference to automatically compare the translations</li>
<li>Requirement: Need an automatic metric</li>
</ul>
</li>
<li>
<p><strong>Automatic metrics</strong></p>
<ul>
<li>
<p>Idea 1:</p>
<ul>
<li><strong>Precision</strong>: $\frac{\text{# of correct words}}{\text{# of output words}}$</li>
<li><strong>Recall</strong>: $\frac{\text{# of correct words}}{\text{# of reference words}}$</li>
<li><strong>F-Measure</strong>: $\frac{2 (\text{precision x recall})}{\text{precision + recall}}$</li>
<li>Eg:
<ul>
<li>System 1: Victory is <code>the opening</code> game <code>is always important</code></li>
<li>Reference: It <code>is always important</code> to win <code>the opening</code> match</li>
<li>System 2: the it opening important is match always win to</li>
<li>For System 1: P $( 5 / 8)$ R $(5 / 9)$ F1 $(0.58)$</li>
<li>For System 2: P $(9 / 9)$ R $( 9/ 9)$ F1 $(1)$</li>
</ul>
</li>
<li><strong>We see the issue that : These do not take word order into account</strong></li>
</ul>
</li>
<li>
<p>Idea 2:</p>
<ul>
<li>
<p>Count all of the n-grams that match</p>
</li>
<li>
<p><strong>BLEU</strong> score - computes precision for n-grams of size 1-4 against multiple reference.</p>
<ul>
<li>
<p>Recall is not defined in this setting</p>
</li>
<li>
<p>BLEU compares system length to an <em>effective reference length</em> and penalize if too short</p>
</li>
<li>
<p>Formula: $$\text{BLEU} = BP \cdot \exp(\sum_{n=1}^{N}w_n \log p_n)$$</p>
</li>
<li>
<p>where BP is the brevity penalty,  - Add brevity penalty (for too short translations)</p>
</li>
<li>
<p>$w_n$ is the weight for n-gram precision, and</p>
</li>
<li>
<p>$p_n$ is the n-gram precision.</p>
</li>
<li>
<p>Another formula: $$
\begin{equation*}
\text{BLEU} = \min\left(1, \frac{\text{output-length}}{\text{reference-length}}\right) \left( \prod_{i=1}^4 \text{precision}_i \right)^{\frac{1}{4}}
\end{equation*}$$</p>
</li>
</ul>
</li>
<li>
<p>Typically computed over the entire corpus, not single sentences</p>
</li>
<li>
<p>To account for variability, use multiple reference translations<br>
– n-grams may match in any of the references<br>
– closest reference length used</p>
<ul>
<li>Used for:
<ul>
<li>Machine Translation, Image Captioning, Text Summarization, Speech Recognition</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Cons:</p>
<ul>
<li>Human translators score low on BLEU (possibly because of higher variability, different word choices)</li>
<li>Ignore relevance of words - names and core concepts more important than determiners and punctuation</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Idea 3:</p>
<ul>
<li>Using trained metrics</li>
<li><strong>COMET</strong>  - Crosslingual Optimized Metric for Evaluation of Translation</li>
<li>Pros:
<ul>
<li>It has shown good correlation with human judgments of translation quality</li>
<li>It is designed to be language-independent and can be used to evaluate translations across multiple languages.</li>
<li>It takes into account the semantic similarity between the source and target sentences, which can be more meaningful than just comparing n-grams.</li>
<li>It has been shown to be more reliable and consistent than other automatic evaluation metrics, such as BLEU.</li>
</ul>
</li>
<li>Cons:
<ul>
<li>Scores are meaningless - absolute values are not informative</li>
<li>It requires a pre-trained cross-lingual sentence encoder, which can be computationally expensive to train.</li>
<li>It may not always align sentences correctly at the semantic level, which could lead to incorrect similarity scores.</li>
<li>It may not be as interpretable as other metrics, such as BLEU, which simply count n-gram matches.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week6/</guid><description>Open Vocabulary Models Readings
Neural Machine Translation of Rare Words with Subword Units, Sennrich et al. (2016)
BPE-Dropout: Simple and Effective Subword Regularization, Provilkov et al. (2020)
We are encoding rare and unknown words as sequences of subword units.</description><content:encoded><![CDATA[<h1 id="open-vocabulary-models">Open Vocabulary Models</h1>
<p>Readings</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://www.aclweb.org/anthology/P16-1162/" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>, Sennrich et al. (2016)</p>
</li>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/2020.acl-main.170.pdf" rel="noopener">BPE-Dropout: Simple and Effective Subword Regularization</a>, Provilkov et al. (2020)</p>
</li>
</ul>
<p>We are encoding rare and unknown words as sequences of subword units. This is based on  the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).</p>
<p><strong>Problem:</strong></p>
<p><strong>how do we represent text?</strong></p>
<ul>
<li>
<p>1-hot encoding</p>
<ul>
<li>lookup of word embedding for input</li>
<li>probability distribution over vocabulary for output</li>
</ul>
</li>
<li>
<p>large vocabularies</p>
<ul>
<li>increase network size</li>
<li>decrease training and decoding speed</li>
</ul>
</li>
<li>
<p>typical network vocabulary size: 10,000–100,000 symbols</p>
</li>
</ul>
<p><strong>NLU and NLG are open-vocabulary problems</strong></p>
<ul>
<li>many training corpora contain millions of word types</li>
<li>productive word formation processes (compounding; derivation) allow formation and understanding of unseen words
<ul>
<li>names, numbers are morphologically simple, but open word classes</li>
</ul>
</li>
<li>Rest of this class we are going to focus on translation</li>
</ul>
<p><strong>Some common approaches which do not work:</strong></p>
<ul>
<li><strong>Ignoring rare words:</strong>
<ul>
<li>replace out-of-vocabulary words with UNK</li>
<li>a vocabulary of 50,000 words covers 95% of text  which gets you 95% of the way; if you only care about automatic metrics</li>
<li>However:
<ul>
<li>rare words are generally high information words:
<ul>
<li>Eg: We we miss the name below; we won&rsquo;t know the <code>subject</code> of the sentence
<ul>
<li><strong>source</strong>: Mr <code>Gallagher</code> has offered a ray of hope.</li>
<li><strong>reference</strong>: Herr <code>Gallagher</code> hat einen hoffnungsstrahl ausgesandt .</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Some initial approaches, which are not good enough:</strong></p>
<ul>
<li><strong>Approximative Softmax</strong>
<ul>
<li>compute softmax over &ldquo;active&rdquo; subset of vocabulary (20 subsets)
<ul>
<li>→ smaller weight matrix, faster softmax</li>
</ul>
</li>
<li>at training time: vocabulary based on words occurring in training set partition</li>
<li>at test time: determine likely target words based on source text (using cheap method like translation dictionary)</li>
<li>limitations
<ul>
<li>allows larger vocabulary, but still not open (completely new words don&rsquo;t have any representation)</li>
<li>network may not learn good representation of rare words (eg. a word seen only once in 20 sets, will not have a good representation)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Using back-off models</strong>
<ul>
<li>replace rare words with <strong>UNK</strong> at training time</li>
<li>when system produces <strong>UNK</strong>, align <strong>UNK</strong> to source word, and translate this with back-off method</li>
<li>limitations:
<ul>
<li><strong>compounds</strong>: hard to model 1-to-many relationships  (assumes that there is a 1-1 mapping of words in source and target languages)</li>
<li><strong>morphology</strong>: hard to predict inflection with back-off dictionary (eg. in languages like turkish with complex morphology)</li>
<li><strong>names</strong>: if alphabets differ, we need transliteration</li>
<li><strong>alignment</strong>: attention model unreliable (if you use alignment, to map words, the alignment from attention could be unreliable)</li>
</ul>
</li>
</ul>
</li>
<li><strong>character-based translations with phrase based models</strong> - good results for closely related languages</li>
<li><strong>segmentation algorithm for phrase-based SMT are too conservative</strong> - we need aggressive segmentation for open-word vocabulary (compact, without need for back-off dictionary)</li>
</ul>
<p><strong>Important approaches, which work and are generally good:</strong></p>
<ul>
<li><strong>Subword NMT</strong>
<ul>
<li>
<p><strong>Subword Translation:</strong></p>
<ul>
<li>Idea: Rare words are potentially translateable through smaller units</li>
<li>Subword segmentation can also avoid the information bottleneck of a fixed-length representation - i.e. in character models when we have too long words, joining embeddings could loose information.</li>
<li>Potential category or words:
<ul>
<li><strong>named entities</strong>: Between languages that share an alphabet, names can often be copied from source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example:
<ul>
<li>Barack Obama (English; German)</li>
<li>バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)</li>
</ul>
</li>
<li><strong>cognates and loanwords</strong>: Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient . Example:
<ul>
<li>claustrophobia (English)</li>
<li>Klaustrophobie (German)</li>
</ul>
</li>
<li><strong>morphologically complex words</strong>: Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example:
<ul>
<li>solar system (English)</li>
<li>Sonnensystem (Sonne + System) (German)</li>
<li>Naprendszer (Nap + Rendszer) (Hungarian)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Bye Pair Encoding (BPE)</strong> (A method that actually works)</p>
<ul>
<li>
<p>merge frequent pairs of characters or character sequences.</p>
</li>
<li>
<p><strong>why BPE</strong>?</p>
<ul>
<li><strong>open-vocabulary</strong>: operations learned on training set can be applied to unknown words</li>
<li><strong>compression of frequent character sequences improves efficiency</strong> → trade-off between text length and vocabulary size</li>
</ul>
</li>
<li>
<p><strong>Algorithm</strong>:</p>
</li>
<li>
<p>two strategies:</p>
<ul>
<li>apply BPE separately for source and target language</li>
<li><strong>apply BPE jointly for source and target language</strong> (<strong>Shared BPE</strong>):
<ul>
<li>translitration is used such that both language have same characters</li>
<li><strong>Big Idea:</strong> <em>If we apply BPE independently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units.</em></li>
</ul>
</li>
</ul>
</li>
<li>
<p>Example:</p>
</li>
</ul>
</li>
<li>
<p><strong>BPE-Dropout</strong></p>
<ul>
<li>
<p>Adding stochastic noise to increase model robustness</p>
</li>
<li>
<p><strong>Idea:</strong></p>
<ul>
<li>In BPE most frequent words are intact in vocabulary, learns how to compose with infrequent words</li>
<li>If we sometimes forget to merge, we will learn how words compose, and better transliteration</li>
<li>forget 1 in 10 times for most scripts, 6/10 in CKJ scripts</li>
</ul>
</li>
<li>
<p><strong>Algorithm:</strong></p>
</li>
<li>
<p>Consistently give 1+ BLEU scores across language pairs - widely used</p>
</li>
<li>
<p><code>-</code> (merge performed) ; <code>_</code> (red) (merge dropped) ; <code>_</code> (green) (merge performed)
*</p>
</li>
</ul>
</li>
</ul>
</li>
<li><strong>Character level NMT</strong>
<ul>
<li>Character-level Models:
<ul>
<li><strong>advantages</strong>:
<ul>
<li>(mostly) open-vocabulary</li>
<li>no heuristic or language-specific segmentation</li>
<li>neural network can conceivably learn from raw character sequences •</li>
</ul>
</li>
<li><strong>drawbacks</strong>:
<ul>
<li>increasing sequence length slows training/decoding (reported x2–x8 increase in training time)</li>
</ul>
</li>
<li>open questions
<ul>
<li>on which level should we represent meaning?</li>
<li>on which level should attention operate?</li>
<li><strong>The disappointing answer is:</strong> whichever gives the better downstream performance</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="low-resource-mt">Low Resource MT</h1>
<p>Readings</p>
<ul>
<li>

<a target=“_blank” href="https://arxiv.org/abs/2109.00486" rel="noopener">Survey of Low-Resource Machine Translation</a>, Haddow et al. (2021)</li>
<li>

<a target=“_blank” href="https://aclanthology.org/P16-1009.pdf" rel="noopener">Improving Neural Machine Translation Models with Monolinguagl Data</a></li>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/2103.12028.pdf" rel="noopener">Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</a></li>
<li>

<a target=“_blank” href="https://aclanthology.org/D16-1163.pdf" rel="noopener">Transfer Learning for Low-Resource Neural Machine Translation</a></li>
<li>

<a target=“_blank” href="https://aclanthology.org/W18-6325.pdf" rel="noopener">Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></li>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/1905.12688.pdf" rel="noopener">Choosing Transfer Languages for Cross-Lingual Learning</a>
<ul>
<li>

<a target=“_blank” href="https://github.com/neulab/langrank" rel="noopener">https://github.com/neulab/langrank</a></li>
<li>LangRank is a program to solve this task of automatically selecting optimal transfer languages, treating it as a ranking problem and building models that consider the aforementioned features to perform this prediction.</li>
</ul>
</li>
<li>

<a target=“_blank” href="https://aclanthology.org/2020.tacl-1.47.pdf" rel="noopener">Multilingual Denoising Pre-training for Neural Machine Translation</a> - mBART
<ul>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
</ul>
</li>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/1907.05019.pdf" rel="noopener">Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</a></li>
<li>

<a target=“_blank” href="https://arxiv.org/pdf/2108.03265.pdf" rel="noopener">Facebook AI’s WMT21 News Translation Task Submission</a></li>
<li>

<a target=“_blank” href="http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.458.pdf" rel="noopener">An Analysis of Massively Multilingual Neural Machine Translation for  Low-Resource Languages</a></li>
</ul>
<p><em><strong>“Low-resourced”-ness is a complex problem going beyond data availability and refects systemic problems in society.</strong></em> - Masakhane</p>
<ul>
<li>
<p><strong>Corpus Creation</strong></p>
<ul>
<li>Web crawling:
<ul>
<li>Extract text from websites identified as multilingual <code>-&gt;</code> Align documents then sentences <code>-&gt;</code> Collate, deduplicate and filter</li>
<li>For new languages: ask native speakers for websites where we can collect such parallel data</li>
</ul>
</li>
<li>Process of large scale extraction:
*</li>
<li>How can we extract parallel data?
<ul>
<li>
<p>Extraction from monolingual data:</p>
<ul>
<li>Large collections of monolingual data contain parallel sentences: <em><strong>Common Crawl, Internet Archive</strong></em></li>
<li>How do we detect these parallel data:
<ul>
<li>Map sentences into a common embedding space using eg. LASER</li>
<li>Nearest neighbours to find parallel sentences</li>
</ul>
</li>
<li>Eg: Datasets: <strong>Paracrawl, WikiMatrix, CCMatrix, Samanantar</strong></li>
</ul>
</li>
<li>
<p>Problems with large scale extraction:</p>
<ul>
<li><strong>Tools for low-resource languages are poor</strong>  - such techniques may work for resource rich languages, but for resource scarce languages the tools might not be evolved and may not succeed to give the required overall outcome of sufficient quality</li>
<li><strong>False positives can dominate</strong> - suppose we have very less parallel data - as we are working at petabyte of data, 1% of the false data will be too much false data.</li>
<li>The crawled resource itself may not have enough parallel data</li>
</ul>
</li>
<li>
<p>Quality of crawled data</p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>  * **ParaCrawl v7.1** seems to be best
</code></pre>
<ul>
<li>
<p><strong>Using Monolingual data for MT</strong></p>
<ul>
<li>By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task <code>English↔German (+2.8–3.7 BLEU)</code>, and for the low-resourced IWSLT 14 task  <code>Turkish→English (+2.1–3.4 BLEU)</code>, obtaining new state-of-the-art results</li>
<li></li>
<li>Iterated back translation for 2-3 iteration is sufficient, however this can fail if initial system is too weak.</li>
</ul>
</li>
<li>
<p><strong>Using Multilingual data for MT</strong></p>
<ul>
<li><strong>Big Idea:</strong>
<ul>
<li>First train a high-resource language pair (the parent model), then transfer some of the  learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs</li>
</ul>
</li>
<li><strong>How transfer learning was done exactly:</strong>
<ul>
<li>
<p>In the French–English to Uzbek–English example, as a result of the initialization, <strong>the English word embeddings from the parent model are copied</strong>, <em>but the Uzbek words are initially mapped to random French embeddings</em>. <strong>The parameters of the English  embeddings are then frozen</strong>, while <em>the Uzbek embeddings’ parameters are allowed to be modified, i.e. fine-tuned, during training of the child model</em>.</p>
</li>
<li>
<p>Parent and Child do not need to be related <code>-&gt;</code> 

<a target=“_blank” href="https://aclanthology.org/W18-6325.pdf" rel="noopener">Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></p>
</li>
<li>
<p>Extensive investigation of choice of parents <code>-&gt;</code> 

<a target=“_blank” href="https://arxiv.org/pdf/1905.12688.pdf" rel="noopener">Choosing Transfer Languages for Cross-Lingual Learning</a></p>
<ul>
<li>Data set size and lexical overlap important</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Transfer learning from Many Monolingual Corpora</strong></p>
<ul>
<li><strong>mBART</strong>:
<ul>
<li><strong>Data</strong>: 25 languages from Common Crawl <code>-&gt;</code> Then finetune parallel data separately for each task</li>
<li><strong>Architecture:</strong> Encoder-Decoder</li>
<li><strong>Learning Method:</strong> Our training data covers $K$ languages: $D = {D_1, . . . , D_K }$ where each $D_i$ is a collection  of monolingual documents in language $i$. We assume access to a noising function $g$, defined below, that corrupts text, and (2) train the model to predict the original text $X$ given $g(X)$. More formally, we aim to maximize $L_θ$:  $$ L_{\theta} = \sum_{D_i \epsilon D} \sum_{X \epsilon D_i} log P(X|g(X);\theta)$$</li>
<li><strong>Objective:</strong> loss over full text reconstruction (not just over masked spans)
<ul>
<li><strong>Noise</strong>:
<ul>
<li><strong>mask spans</strong> of text: 35% of words</li>
<li><strong>permute the order of sentences</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>Token Type:</strong> Language token for both source and target language</li>
<li><em>mBART</em>, <em>mBART50</em> <code>-&gt;</code> Basis of much practical work on low-resource MT</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Multilingual Models</strong>:</p>
<ul>
<li><strong>Idea:</strong> Handle all $N$ by $N$ translation directions with a single model (instead of $O(N^2)$</li>
<li>Usually 1-n or n-1</li>
<li>Use a small number of related langauges (As not all language pair gives good result - different linguistic properties have an effect, other possibilities as well)</li>
<li>Or go big: 103 languages 

<a target=“_blank” href="https://arxiv.org/pdf/1907.05019.pdf" rel="noopener">Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</a></li>
<li>There is a trade-off:
<ul>
<li>Transfer: benefit from addition of other languages</li>
<li>Interferance: performance is degraded due to having to  also learn to translate other languages</li>
</ul>
</li>
<li><strong>Pros</strong>: Benefits are more noticeable for the many-to-English and low-resource pairs</li>
<li><strong>Cons:</strong> High-resource pairs tend to be harmed</li>
<li><strong>Cons:</strong> Massive systems require capacity</li>
</ul>
</li>
<li>
<p><strong>Evaluation of Low-resrouce MT</strong></p>
<ul>
<li>Is automatic evaluation of low-resource languages harder?
<ul>
<li>Metrics are designed with high-resource langauges in mind</li>
<li>Metrics are less reliable on poor systems</li>
<li>Lack of good test sets and human evaluations for training metrics</li>
</ul>
</li>
<li>Human evaluation is preferable for low resource language</li>
<li>Researchers need to connect to language communities</li>
</ul>
</li>
</ul>
<h1 id="nlp-ethics">NLP Ethics</h1>
<p>Readings</p>
<ul>
<li>
<p>

<a target=“_blank” href="http://aclweb.org/anthology/P16-2096" rel="noopener">The Social Impact of Natural Language Processing</a>, Hovy and Spruit (2016)</p>
</li>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/2112.04359" rel="noopener">Ethical and social risks of harm from Language Models</a>   Weidinger et al. (2021)</p>
</li>
<li>
<p>

<a target=“_blank” href="https://www.scu.edu/media/ethics-center/technology-ethics/IntroToDataEthics.pdf" rel="noopener">An Introduction to Data Ethics</a>, Vallor and Rewak. </p>
</li>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1901.10002" rel="noopener">A Framework for Understanding Unintended Consequences of Machine Learning</a>, Suresh and Guttag (2019)</p>
</li>
<li>
<p><strong>NLP affects people&rsquo;s lives</strong></p>
<ul>
<li>We need to ask who is affected by an NLP experiment?</li>
<li>Have they consented to the experiment or participating in it? - <strong>Facebook&rsquo;s contagion experiment</strong></li>
<li>What derived information about you is collected which you have not consented to? they can be traced from their data</li>
</ul>
</li>
<li>
<p><strong>Who do these systems harm?</strong></p>
<ul>
<li>Who gets admitted.</li>
<li>Who gets hired.</li>
<li>Who gets promoted.</li>
<li>Who receives a loan.</li>
<li>Who receives treatment for medical problems.</li>
<li>Who receives the death penalty.</li>
</ul>
</li>
<li>
<p><strong>Type of Risks</strong></p>
<ul>
<li>
<p>The NLP model accurately refects natural speech, including unjust, toxic, and oppressive tendencies present in the training data.</p>
</li>
<li>
<p><strong>Discrimination, Exclusion and Toxicity</strong></p>
<ul>
<li><strong>Allocational (material) harm</strong>: discrimination
<ul>
<li>eg. Models that analyse CVs for recruitment can be less likely to recommend historically discriminated groups</li>
<li>eg: Accent challenge
*</li>
</ul>
</li>
<li><strong>Representative harm</strong>: exclusionary norms eg. Q: what is a family? A: a man and a woman who get married and have children, and social stereotypes (Dr. - Man | Nurse - Woman)
<ul>
<li>Audacious is to boldness as [religion] is to &hellip;
<ul>
<li>Muslim - Terrorism</li>
<li>Jewish - Money</li>
</ul>
</li>
</ul>
</li>
<li><strong>Ofensive Behaviour</strong>: generate toxic language</li>
</ul>
</li>
<li>
<p><strong>Information Hazards</strong> (Leads to Privacy and safety harms)</p>
<ul>
<li>The language models could be prompted to give the private information / safety critical information which is present in it</li>
</ul>
</li>
<li>
<p><strong>Misinformation Harms</strong></p>
<ul>
<li>The LM assigning high probabilities to false, misleading, nonsensical or poor quality information.</li>
<li><strong>Harms involve</strong>: people believing and acting on false information</li>
</ul>
</li>
<li>
<p><strong>Malicious Uses</strong></p>
<ul>
<li>From humans intentionally using the LM to cause harms</li>
<li>Types of harms:
<ul>
<li>Illegitimate surveillance and censorship
<ul>
<li>Saudi Arabia monitoring social media <code>-&gt;</code> persecutes dissidents without trial, often violently</li>
</ul>
</li>
<li>Facilitating fraud and impersonation scams</li>
<li>Cyber attacks</li>
<li>Misinformation campaigns</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>While solving a problem we can ask these questions:</strong></p>
<ul>
<li>Who are the stakeholders?</li>
<li>What could go wrong?</li>
<li>Who could benefit, and how?</li>
<li>Who could be harmed, and how?</li>
<li>What can you do to mitigate possible harms?</li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week7/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week7/</guid><description>Bias in Embeddings and Language Models Summarization Reading
Get To The Point: Summarization with Pointer-Generator Networks
Text Summarization with Pretrained Encoders
Planning with Learned Entity Prompts for Abstractive Summarization
Conditional Generation with a Question-Answering Blueprint</description><content:encoded><![CDATA[<h1 id="bias-in-embeddings-and-language-models">Bias in Embeddings and Language Models</h1>
<h1 id="summarization">Summarization</h1>
<p>Reading</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/P17-1099.pdf" rel="noopener">Get To The Point: Summarization with Pointer-Generator Networks</a></p>
</li>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/D19-1387.pdf" rel="noopener">Text Summarization with Pretrained Encoders</a></p>
</li>
<li>
<p>

<a target=“_blank” href="https://watermark.silverchair.com/tacl_a_00438.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAs4wggLKBgkqhkiG9w0BBwagggK7MIICtwIBADCCArAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQML9ZBJlXoc5EThXYTAgEQgIICgemuaxuFlkWgZQg6GOwSCTfcKUZzH3RAxrV0KI1W22jaZA1jnX_1-i6OMTPJnE01m3nN2E0Z-jCwiH0gNiti5XW-vBVBpWKPFG7UbZqkQgs4G00Sf7RY38nvFE1qPbMuZ1rgQkMOXrjfuLMPmt-_Uyc8Edbv-vQ2MadOzK-tXZlTteVkV22rVzwhN8ptYhiXv9Kxeyl-r2Oov4NEaB4AyoJ79weu95aC-92Wxl0ItCijpYER-kq2NkXLWDRRpfAaLiJYgKTGBct2VvSt4fF-a22Hr4jh0gyRGqwlsE0ADYop0Ods7kIxsyybOTa5Ohg_X7T66A0mkeA4TFAxb0hpUUPjKJK5ScoE7xB0Y3FlkiWSBz_SHAToUTUjIG-qIu3AqynQ1mPX1SvU52CHfxN8Z5HE81Kz_wpwOmf6hy8Mvg4VsF5cnyyaqDTwD4kEjBCGkzp9b3s9Ouw_SfrbczG-5F01HzzQy9fu4mynH3o2w7EXFnIvnAHxJ_hX_hqXNgwp6eRnUQffm-6AhwCuT3GV9jPkJ5j4KbBh8qVyrJ6DfUfYr-_21yqwd9DCYJP5V4-PPjsOy0ojPX_Kxbwcd6rRIHg1N2r9HrVZewLqMkm-tqpOMQ-e5W2WFqK_RE-zQcoYt94fUuWjSw-nrDWTGYkcbXFdBi0zQuFREhhL11kuH6an3vyfaru8Noy-fReEjL6lP4lvZ1_UvDdLMTZZloF9PENlGTFg59PTkc8TCLKh8hpbPvslbODrbsAva0HMA8lyIqsglvWHQ8a0ZC0Xzf8KNV9ttmQOrMyAm-W53JXRY2yCcorEEHOXaN4i5uZl4SxQ0q_W4UbPoNybbqVC66TdlCBO" rel="noopener">Planning with Learned Entity Prompts for Abstractive Summarization</a></p>
</li>
<li>
<p>

<a target=“_blank” href="https://www.researchgate.net/publication/361734006_Conditional_Generation_with_a_Question-Answering_Blueprint/fulltext/62c277363d26d6389e907d55/Conditional-Generation-with-a-Question-Answering-Blueprint.pdf?origin=publication_detail" rel="noopener">Conditional Generation with a Question-Answering Blueprint</a></p>
</li>
<li>
<p>

<a target=“_blank” href="https://aclanthology.org/2023.eacl-demo.13.pdf" rel="noopener">Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation</a></p>
</li>
<li>
<p>From both the examples below:</p>
<ul>
<li>We can say that NLG is subjective i.e. the summaries, captions etc. could be different from different person&rsquo;s perspective
<ul>
<li>eg: child migh need simpler sentences</li>
<li>person focused on diseases will say it&rsquo;s a covid-super spreader event etc.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>A Language model predicts <code>next word</code> given predicted words so far: $p(y_t|y_1,&hellip; y_{t-1})$</p>
</li>
<li>
<p>A conditional language model predicts <code>next word</code>, given the words so far, and also some other input $x$: $p(y_t|y_1,&hellip;,y_{t-1}, x)$ , where $x$ can be image, text or a database etc.</p>
<ul>
<li>For MT <code>-&gt;</code> $x=$ source sentence, $y=$ target sentence</li>
<li>For Summarization <code>-&gt;</code> $x=$ input text, $y=$ summarized text</li>
</ul>
</li>
<li>
<p><strong>Modeling Approach</strong></p>
<p>here $J=\frac{1}{T} \sum_{t=1}^{T} J_t$  is the loss function used to optimize the model</p>
</li>
<li>
<p><strong>Summarization: Task Definition</strong></p>
<ul>
<li>Given input text $x$, write summary $y$ which is shorter and contains main information of $x$.</li>
<li>Summarization can be single-document or multi-document. Typically multi-document summarization will have overlapping content i.e. same topic or many articles about same event
<ul>
<li><strong>Single-document</strong> means we write summary y of single document $x$.</li>
<li><strong>Multi-document</strong> means we write summary y of multiple documents $x_1,&hellip;, x_n$</li>
</ul>
</li>
<li>Generally multi-document summarization is difficult:
<ul>
<li>redundant content - but paraphrased so there might be repetition in summary</li>
<li>$x$ in the conditional probability formulation will be now very long i.e $m \times 500$, $m$ is number of document. This is more difficult to model.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Type of summarization</strong></p>
<ul>
<li>Extractive Summarization:
<ul>
<li>Select parts (typically sentences) of the original text to form a summary</li>
<li>Generally this is easier and restrictive (no paraphrasing)</li>
<li>Less Fluent, Less Coherent</li>
</ul>
</li>
<li>Abstractive Summarization:
<ul>
<li>Generate new text using NLG techniques</li>
<li>More difficult but more flexible (human-like)</li>
<li>Generally more fluent and coherent if done well</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Dataset for Summarization</strong></p>
<ul>
<li>CNN Daily Mail:
<ul>
<li>Pair of news articles (average 800 words) and summaries (aka story highlights), usually 3 or 4 sentences long (average 56 words)
<ul>
<li>highlights are written by journalists, in a compressed telegraphic manner</li>
<li>highlight need not form coherent summary - each highligh is relatively stand-alone, with little co-referencing (i.e. he, she, her, it etc.)</li>
</ul>
</li>
<li>CNN 100K pairs; Daily Mail 200K pairs</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Modeling Summarization Task</strong></p>
<ul>
<li>
<p><strong>Summarization with Sequence-to-Sequence Attentional Model</strong></p>
<ul>
<li>
<p>This is how the modeling is described here:</p>
<ul>
<li><strong>Token:</strong> Word (In summarization in this model, we used most common words for vocabulary with word as token <code>-&gt;</code> no BPE/wordpiece)</li>
<li><strong>Encoder:</strong> single-layer bidirectional LSTM produces a sequence of <code>hidden states</code> $h_i$.</li>
<li><strong>Decoder:</strong> single-layer unidirectional LSTM receives word embedding of previous word emitted by decoder and has <code>decoder state</code> $s_t$</li>
<li><strong>Attention distribution:</strong>  This tell the decoder which words are most important in the context vector and decoder knows where to focus
$$
\begin{align}
&amp;\quad e_i^t= v^T tanh(W_h h_i+W_s s_t + b_{attn}) \ &amp;\quad a^t = softmax(e^t)
\end{align}
$$</li>
<li><strong>Context Vector:</strong> weighted sum of encoder hiddher states $h_i^* = \sum_{i} a_i^t h_i$ ; this is what decoder will see from encoder.</li>
<li><strong>Vocabulary Distribution:</strong> probability distribution over all words in vocabulary, produced by decoder $P_{vocab} = \text{softmax}(V&rsquo;(V([s_t, h_t^*] + b) + b&rsquo;))$ ; $s_t$ is the decoder state; $V&rsquo;$ is not important so we can ignore</li>
<li><strong>Training Loss:</strong> for time step $t$ is negative log likelihood of target word $w_t^<em>$, $loss_t=-log P(w_t^</em>)$</li>
</ul>
</li>
<li>
<p>Now the first problem with this model is that there are words which are not present in our data; we use UNK tokens in to represent them, however, it is possible that we start using them repeatedly in our summaries.</p>
</li>
<li>
<p><strong>Pointer Generator Network</strong> comes to the rescue here; now instead of repeatedly using UNK tokens, we add a copying mechanism which is useful for rare words and phrases.</p>
<ul>
<li>this model allows us both <code>copying words by pointing</code>, and <code>generating words</code> from a fixed vocabulary</li>
<li>on each decoder step, we calculate $p_{gen}$, probability of <code>generating</code> next word (rather than copying it)</li>
<li>we learn $p_{gen}$ during training; this balances of copying v/s generating  $$
P(w) = p_{gen}P_{vocab}(w) + (1 - p_{gen}) \sum_{i:w_i=w}a_i^t
$$</li>
<li>so during prediction of next word in decoder; we look at  $p_{gen}$ ; which acts as a gate; which tells us if we generate a word from vocabulary or if we should look into the encoder and if there is a word which is should copy it.</li>
</ul>
</li>
<li>
<p>Pointer Generator fixes the problem of generating UNK; however now it does not know if it has mentioned a word before and it will keep repeating it in the summary, which mean our summary will be repeatitive now.</p>
</li>
<li>
<p>We introduce a mechanism to check for the coverage of a word; which ensure that we do not repeat words in summary. This is called <strong>Coverage Mechanism</strong></p>
<ul>
<li>We add a <strong>Coverage vector</strong> $c^t$, which tells us what has been attended so far:
<ul>
<li>$$c^t = \sum_{t&rsquo;=0}^{t-1} a^{t&rsquo;}$$</li>
<li>Use coverage vector as extra input to attention mechanism:$$
e_i^t = v^T tanh(W_h h_i + W_s s_t + W_c c_i^t + b_{attn})
$$</li>
<li><strong>Coverage loss</strong>: is added to training objective which penalizes overlap between coverage vector $c^t$ and new attention distribution $a^t$: $$
covloss_t = \sum_{i} min (a_i^t, c_i^t)
$$
<ul>
<li>this adds a penalty which promotes coverage if the word has not been attended to and discourages coverage from attending to already covered words</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Summarization with Pre-Trained Encoders</strong></p>
<ul>
<li>
<p>Pre-trained encoders like BERT are very successful in may NLU tasks, however for they were not setup for summarization. Why?</p>
<ul>
<li>BERT is trained on sentence level (sentence pairs), will it work on document?</li>
<li>Here&rsquo;s the encoder for BERT:</li>
<li>How to make it ready for document as input?
<ul>
<li>as we can see we have inserted [CLS] token at the start of each sentence; also sengment embeddings tells us where a new sentence start and end</li>
</ul>
</li>
</ul>
</li>
<li>
<p>However this change brings another challenge:</p>
<ul>
<li>there is a mismatch between encoder and decoder:
<ul>
<li>difference between how they are setup and optimized - our decoder has not seen any data whereas our encoder has seen a lot of data.</li>
<li>this implies that same training strategy won&rsquo;t work for both</li>
</ul>
</li>
<li>How to fix this mismatch?
<ul>
<li>fine-tune strategy
<ul>
<li>learning rate schedule (vaswani et al. 2017)
<ul>
<li>${lr} = \tilde{lr}.min(step^{-0.5}, step.warmup^{-1.5})$</li>
</ul>
</li>
</ul>
</li>
<li>we choose a <code>smaller learning rate</code> and <code>longer warming-up</code> for the encoder:
<ul>
<li>$\tilde{lr}_e = 2e^{-3}, warmup_e = 20,000$</li>
</ul>
</li>
<li>we choose <code>larger learning rate</code> and <code>shorter warming-up</code> for the decoder:
<ul>
<li>$\tilde{lr}_e = 0.1, warmup_e = 10,000$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Evaluation of Summarization</strong></p>
<ul>
<li>
<p><strong>ROUGE</strong> : Recall-Oriented Understudy for Gisting Evaluation$$
ROUGE-N = \frac{\sum\limits_{S\epsilon{ReferenceSummaries}}  \sum\limits_{gram_n \epsilon S}{Count_{match}(gram_n)}}{\sum\limits_{S\epsilon{ReferenceSummaries}}\sum\limits_{gram_n \epsilon S}{Count(gram_n)}}
$$</p>
</li>
<li>
<p>Like BLEU, it is based on n-gram overlap</p>
</li>
<li>
<p>ROUGE has no brevity penalty and is based on recall</p>
</li>
<li>
<p>Often F1 (combination of precision and recall)  and ROUGE is reported</p>
</li>
<li>
<p>Most commonly-reported ROUGE scores: <strong>ROUGE-1</strong> <code>unigram</code> overlap <strong>ROUGE-2</strong> <code>bigram</code> overlap, and <strong>ROUGE-L</strong> <code>longest common subsequence</code> overlap</p>
</li>
<li>
<p><strong>Summarization Results</strong></p>
</li>
</ul>
</li>
<li>
<p><strong>Conditional Generation: Objectives</strong></p>
<ul>
<li>
<p>Generate natural language towards a <code>communicative goal</code></p>
</li>
<li>
<p>which is <code>faithful</code> and can be <code>attributed</code> to is sources - <code>no hallucination</code></p>
</li>
<li>
<p>while users Iexplicitly <code>control</code> generation outcome - <code>style - short, long</code></p>
</li>
<li>
<p>Long-form QA</p>
<ul>
<li>some queries have long-form answer and they require <code>multiple</code> documents to answer - <code>hallucinations</code> and <code>attribution</code> can be more problematic</li>
</ul>
</li>
</ul>
</li>
<li>
<p>changes to summarization systems:</p>
<ul>
<li>Change the way entities are represented (Puduppully et al., 2019; Iso et al., 2019)</li>
<li>The decoder skips low-confidence tokens (Tian et al., 2019)</li>
<li>Encode documents hierarchically (Rhode et al., 2021)</li>
<li>Adopt sparse attention mechanisms (Child et al., 2019; Beltagy et al., 2020)</li>
<li>Introduce <code>planning</code> components (Puduppully et al., 2022; Narayan et al., 2022)</li>
</ul>
</li>
<li>
<p><strong>Planning with Entity Chains</strong> (Narayan et al., 2022)</p>
<ul>
<li></li>
</ul>
</li>
</ul>
<ul>
<li>
<p><strong>Big Idea:</strong> We extract entities and then creates a summary which chain them together</p>
<ul>
<li>however, the entities are context dependent - eg. <code>Titanic</code> - <code>the boat / the movie</code></li>
<li>hence; if we miss the context we end up getting a wrong summary</li>
<li><strong>how is this control/planning component added?</strong>
<ul>
<li>There is  <strong>Questions under discussion (QUD)</strong> theory of discourse structure which tells that during a discourse participants are mutually committed to resolving a partially structured <strong>set of questions</strong>  at a given point in time.</li>
<li>now; <strong>discourse</strong> has these questions/plans <strong>implicitly</strong> which are eventually turned into answers in a successful discourse</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Our plan is to turn the question into PLAN explicitly!!</strong> how? lets see!</p>
<ul>
<li><strong>Question-Answering Blueprints as Content Plans</strong>
<ul>
<li>
<p>Blueprints as intermediate (discrete) planning stage for conditional generation</p>
</li>
<li>
<p>Reduce faithfulness errors</p>
</li>
<li>
<p>Increase controllability</p>
</li>
<li>
<p>Are better for long-form inputs and outputs</p>
</li>
<li>
<p><strong>Blue Print Annotation</strong></p>
<ul>
<li>Large-scale QA generation for output (summary, answer)</li>
<li>QA selection and filtering for final blueprint</li>
<li><strong>Steps for QA blueprint annotation:</strong>
<ol>
<li>
<p>Question-Answering Overgeneration - Identify <strong>noun phrases</strong> and <strong>named entities</strong> as answer candidates</p>
</li>
<li>
<p><strong>Generate questions for each answer candidate</strong> using SQuAD trained Question Generation model (T5-11B) - try to generate as many questions as we want</p>
</li>
<li>
<p><strong>FIltering</strong> - Perform <strong>round-trip consistency check</strong> (Alberti et al., 2019)! - After getting QA normally , we ask the same questions again to the summary and if we get the same answer, then our question is selected</p>
</li>
<li>
<p><strong>Filtering</strong> - <strong>Rheme-based selection</strong> prioritizes new-information seeking questions</p>
</li>
<li>
<p><strong>Filtering</strong> - <strong>Coverage</strong> prioritizes the selection of informative QA pairs by selecting non-overlapping ones.</p>
</li>
</ol>
<ul>
<li><strong>QA Blueprint Annotation (Full picture)</strong></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Blueprint Model</strong></p>
<ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>    * **End-to-end Blueprint Model**
      
      * Treat the blueprint as **prefix** maybe like in T5 model `-&gt;` Decoder blueprint `-&gt;` decode output 
      * Model ouputs the blueprint and summary
      * **Issue:** The output sequence is too long and the decoder runs out of memory
    * **Multitask Blueprint Model**
      
      * Two tasks that model does:
        1. answer plan and output sequence
        1. answer plan and questions (output and blueprint are generated separately during inference)
      * Issue:  a) The output is conditioned only on answers and not on the questions b) we run the decoder and get the output sequence, to get the blueprint we have to rerun the model.
    * **Iterative Blueprint Model**
      
      * Issue: a) No global plan (only local planning) b) This is slow as everything is planned sentence by sentence.
  * **Evaluating Model:**
    
    * Datasets:
      * **AQuaMuse** (Kulkarni et al., 2002, 2021): **long-form question answering**, simulates search engine, answer based on multiple retrieved documents.
      * **WikiCatSum** (Perez-Beltrachini et al., 2019): **topic-focused multi-document summarization**, generate Wikipedia abstracts.
      * **SummScreen** (Chen et al., 2022): **dialogue summarization**, generate summaries of TV episodes (e.g., CSI, The Bing Bang Theory). `-&gt;` `Hardest dataset as it has very long documents.`
    * Import notes:
      * 
      * We found out that with blueprint methods, our summaries are much longer than earlier.
      * **Is this bad?**
        * Not necessarily as we will see below
        * First we evaluate using `ROUGE-N` and find out that all the blueprint based models (E2E, Multitask and Iterative) are similarly fluent as non-blueprint based model (LongT5):
    * But can we only evaluate using ROUGE? isn't there anything else. Yes there are few more things we can do!
      * **QA-based** metric: (QAFactEval; Fabbri et al, 2022)
        * **Big Idea:** We have QA pairs, let's do QA on summary and see if we get the same answer; if we get the same answer we can claim that our **summaries are more grounded**
        * **Is blueprint more grounded?** (yes it is)
        * 
      * **Textual Entailment based metric**
        * Quantify whether model summaries are **faithful** (*consistent text to the input document*) to input with textual entailment. $(t =&gt; h)$ **t** entails **h**?
          * if human reading $t$ will infer $h$ is most likely true$$
            F(s) = \frac{1}{n}\sum\limits\_{i=1}^n E(D, s_i)$$
          * E is a textual entailment model trained on public data (Honovich et al., 2022) 
          * $n$ is the number of sentences in the summary; $D$ is input document/s. 
          * $F(s)$ correlates well with human ratings $(\rho = 0.774)$.
          * We can see that for SumScreen the difference is most discerable even though it is low.  Yes -&gt; blueprint methods are more faithful
</code></pre>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/NLU/Week9/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/NLU/Week9/</guid><description>Neural Parsing Readings
Grammar as a Foreign Language, Vinyals et al., NeurIPS 2015. This is the encoder-decoder-based pasing model introduced in the lecture.
Constituency parsing with a self-attentive encoder. Kitaev and Klein, ACL 2018.</description><content:encoded><![CDATA[<h1 id="neural-parsing">Neural Parsing</h1>
<p>Readings</p>
<ul>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1412.7449" rel="noopener">Grammar as a Foreign Language</a>, Vinyals et al., NeurIPS 2015. This is the encoder-decoder-based pasing model introduced in the lecture.</p>
</li>
<li>
<p>

<a target=“_blank” href="https://arxiv.org/abs/1805.01052" rel="noopener">Constituency parsing with a self-attentive encoder.</a> Kitaev and Klein, ACL 2018. This is the transformer-based parsing model introduced in the lecture.</p>
</li>
<li>
<p><strong>The Big Idea</strong></p>
<ul>
<li>The big idea is that we can generate any structured representation that can be linearized.
<ul>
<li>Eg: syntactic parsing, where the input is a string of word and the output is a tree. It&rsquo;s a fairly high it&rsquo;s a highly structured object.</li>
<li>Eg: Trees, Graphs</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Parsing is the task of turning a sequence of words: (it helps disambiguate)</p>
<ul>
<li>(1) You saw a man with a telescope. into a syntax tree:</li>
</ul>
</li>
<li>
<p><strong>How can we use an encoder-decoder model for parsing?</strong></p>
<ul>
<li><strong>Input</strong>: is a string</li>
<li><strong>Outpu</strong>t: is not a string yet, we need to <strong>linearize the syntax tree</strong>:
<ul>
<li><code>(S (NP (Pro You ) ) (VP (V saw ) (NP (Det a ) (N man ) (PP (P with ) (Det a ) (N telescope ) ) ) ) )</code></li>
<li>We can even get rid of the words (is this a valid assumption?)
<ul>
<li><code>(S (NP Pro ) (VP V (NP Det N (PP P Det N ) ) ) )</code></li>
</ul>
</li>
<li>And we can make it <em>easier to process by annotating also the closing brackets</em>:
<ul>
<li>$(S\ (NP \ Pro)<em>{NP} (VP\ V(NP\ Det\ N(PP\ Det\ N)</em>{PP} )<em>{NP} )</em>{VP} )_S$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>What could go wrong?</strong></p>
<ul>
<li>
<p>How do we know if the generated tree is valid?**</p>
<ul>
<li>turns out that RNN learns to close open brackets most of the time</li>
</ul>
</li>
<li>
<p><strong>How about cross branches in syntax tree?</strong></p>
<ul>
<li>model learns it as well</li>
</ul>
</li>
<li>
<p><strong>Encoder-Decoder for Parsing</strong> 

<a target=“_blank” href="https://arxiv.org/abs/1412.7449" rel="noopener">Grammar as a Foreign Language</a>,</p>
<ul>
<li></li>
<li>To make it work properly, we need to do following:
<ul>
<li><strong>Add an end-of-sequence symbol</strong>, as output sequences can vary in length.</li>
<li><strong>Reverse the input string</strong>: results in small performance gain.</li>
<li>Make the <strong>network deeper</strong>. Vinyals et al. (2015) use three LSTM layers for both encoder and decoder.</li>
<li>Add <strong>attention</strong>. This essentially works like the encoder-decoder with attention we saw for MT (lecture 7).</li>
<li>Use pre-trained word embeddings as input (here: word2vec).</li>
<li><strong>Get lots of training data</strong>. Vinyals et al. (2015) use an existing parser (the Berkeley parser) to parse a large amount of text, which they then use as training data.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Potential Problems</strong></p>
<ul>
<li>How do we make sure that opening and closing brackets match? Else we won’t have a well-formed tree!
<ul>
<li>This is really rare (0.8–1.5% of sentences). And if it occurs, just fix the brackets in post-processing (add brackets to beginning or end of the sequence).</li>
</ul>
</li>
<li>How do we associate the words in the input with the leaves of the tree in the output?
<ul>
<li>You could just associate each input word with a PoS in the output, in sequence order. But in practice: only the tree is evaluated. Vinyals et al. (2015) replace all PoS tags with XX.</li>
</ul>
</li>
<li>The output sequence can be longer than the input sequence, isn’t this a problem?
<ul>
<li>encoder-decoder can match sequence length mismatch; there is no restriction on sequence length both input and generated</li>
</ul>
</li>
<li>How can I make sure that the model outputs the best overall sequence, not just the best symbol at each time step?
<ul>
<li>Use beam search to generate the output (as in MT). However, in practice, beam size has very little impact on performance. (this may not be the optimal one)</li>
<li>How about - viterbi decoding or probabilstic CYK  (guranteed optimal)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Evaluation</strong></p>
<ul>
<li>Training Corpora: (part of penn tree bank was used)
<ul>
<li><strong>Wall Street Journal (WSJ)</strong>: treebank with 40k manually annotated sentences.</li>
<li><strong>BerkeleyParser corpus</strong>: 90k sentences from WSJ and several other treebanks, and 11M sentences parsed with <strong>Berkeley Parser</strong>.</li>
<li><strong>High-confidence corpus</strong>: 90k sentences from WSJ from several treebanks, and 11M sentences for which two parsers produce the same tree (length resampled).</li>
<li></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Parsing with Transformers</strong> 

<a target=“_blank” href="https://arxiv.org/pdf/1805.01052.pdf" rel="noopener">Constituency Parsing with a Self-Attentive Encoder</a></p>
</li>
<li>
<p><strong>Architecture:</strong></p>
<ul>
<li></li>
<li><strong>Context-Aware Word Representation</strong>
<ul>
<li>sub-words are used for representation of token</li>
<li>simple concatenation of character embeddings or word prefixes/suffixes can outputperform POS information from external system</li>
<li></li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Content v/s Position Attention</strong></p>
<ul>
<li><strong>Attention on position</strong>:  Different locations in the sentence can attend to each other based on their positions,</li>
<li><strong>Attention on content/context</strong>:  also based on  their contents (i.e. based on the words at or around those positions)</li>
<li>We will later factor these two components in attention and we will find an improved performance in the parsing task.</li>
</ul>
</li>
</ul>
<h1 id="unsupervised-parsing">Unsupervised Parsing</h1>
<p>Readings</p>
<ul>
<li>

<a target=“_blank” href="https://arxiv.org/abs/2010.03146" rel="noopener">Unsupervised parsing via constituency tests</a>. Cao et al., EMNLP 2020.</li>
</ul>
<p><strong>What happens when we do not have a tree bank?</strong> Let&rsquo;s find out!</p>
<ul>
<li>
<p><strong>General approach of Cao. et. al:</strong></p>
<ul>
<li>
<p><strong>constituency test</strong>:</p>
<ul>
<li>take a sentence, modify it via a transformation (e.g., replace a span with a pronoun);</li>
<li>One type of constituency test involves modifying the sentence via some transformation and then judging the result.</li>
<li>An unsupervised parser is designed by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.</li>
<li>If the constituents pass the linguistic tests; we have evidence that a proposed constituent is an actual constituent</li>
</ul>
</li>
<li>
<p><strong>Neural grammaticality models</strong></p>
<ul>
<li>check if the result is grammatical using a neural grammaticality model;</li>
<li>produce a tree for the sentence by aggregating the results of constituency tests over spans;</li>
<li>Specifically, we  score the likelihood that a span is a constituent by<br>
applying the constituency tests and averaging their grammaticality judgments
<ul>
<li>$$s_{\theta}(\text{sent}, i, j) = \dfrac{1}{\lvert C \rvert}\sum_{c \in C} g_{\theta}(c(\text{sent}, i, j))$$
<ul>
<li>C denotes the set of constituency tests</li>
<li>$c(sent, i, j)$ is a proposed constintuent</li>
<li>a judgment function (grammaticality model) $g : sent  → {0, 1}$ that gives a score. If 1 -&gt; constituent is grammaticaly otherwise not.</li>
</ul>
</li>
</ul>
</li>
<li><strong>How we learn the grammaticality model?</strong>
*</li>
</ul>
</li>
<li>
<p><strong>Parsing Algoritm</strong></p>
<ul>
<li>select the tree with the highest score; $$t^*(sent) = arg\max_{t \in T (len(sent))} \sum_{(i,j) \in t} s_{\theta}(sent, i, j)$$
<ul>
<li>$T(len(sent))$ denotes the set of binary  trees with $len(sent)$ leaves.</li>
<li>If we interpret the score $s_θ(sent, i, j)$ as estimating the probability that the span $(sent, i, j)$ is a constituent, then this <strong>formulation corresponds to choosing the tree with the highest expected number of constituents</strong>, i.e. minimum risk decoding
<ul>
<li>This allows for a low probability span in the tree if the model is confident about the rest of the tree</li>
</ul>
</li>
</ul>
</li>
<li>we <strong>score each tree by summing the scores</strong>  of its spans and choose the <strong>highest scoring binary tree via CKY</strong></li>
</ul>
</li>
<li>
<p><strong>Refinement:</strong></p>
<ul>
<li>add to this refinement: alternate between improving the tree and improving the grammaticality model.</li>
<li>
<h2 id="how-does-this-work"><strong>How does this work?</strong></h2>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded></item><item><title/><link>https://lordzuko.github.io/UoE/Speech-Synthesis/9.-Sequence-to-Sequence/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lordzuko.github.io/UoE/Speech-Synthesis/9.-Sequence-to-Sequence/</guid><description/><content:encoded></content:encoded></item></channel></rss>