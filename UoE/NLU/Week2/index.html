<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Feedforward Language Model Reading:
A Course in Machine Learning, chapter 4, Daum√©. Chapter 7 of¬†Speech and Language Processing, 3rd edition, Jurafsky & Martin. Representing n-gram probabilities with neural networks For neural networks: $\mathbf{x}$ : input vector $\mathbf{h}$ : hidden layer $\mathbf{y}$ : output vector Vector Rrepresentations:"><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.71139b94f3f29dfda52032fbfdd80a2a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.fab86bbe49cf4e5f56dd16934c1866ee.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#feedforward-language-model><h1 id=feedforward-language-model><span class=hanchor arialabel=Anchor># </span>Feedforward Language Model</h1></a><p>Reading:</p><ul><li><a target=‚Äú_blank‚Äù href=http://ciml.info/ rel=noopener>A Course in Machine Learning</a>, chapter 4, Daum√©.</li><li>Chapter 7 of¬†
<a target=‚Äú_blank‚Äù href=https://web.stanford.edu/~jurafsky/slp3/ rel=noopener>Speech and Language Processing</a>, 3rd edition, Jurafsky & Martin.</li></ul><a href=#representing-n-gram-probabilities-with-neural-networks><h3 id=representing-n-gram-probabilities-with-neural-networks><span class=hanchor arialabel=Anchor># </span>Representing n-gram probabilities with neural networks</h3></a><ul><li><strong>For neural networks:</strong><ul><li>$\mathbf{x}$ : input vector</li><li>$\mathbf{h}$ : hidden layer</li><li>$\mathbf{y}$ : output vector</li></ul></li></ul><p><strong>Vector Rrepresentations:</strong></p><ul><li>one-hot encoding</li><li>probability distribution as vector<ul><li>Softmax: turns any vector into a probability distribution</li><li>Input: a vector $\mathbf{v}$ of length $|\mathbf{v}|$</li><li>Output: softmax($\mathbf{v}$) is a distribution $\mathbf{u}$ over $\mathbf{|v|}$ where: $$u_i = \frac{exp (v_i)}{\sum_{j=1}^{|v|} exp(v_j)}$$</li></ul></li><li><strong>Logistic Regression:</strong>$$ \mathbf{y} = softmax(\mathbf{Wx + b}) $$</li><li>$\mathbf{W}$ and $\mathbf{b}$ are parameters of the model</li><li>Input $\mathbf{x}$ and output $\mathbf{y}$ have different dimension, which determine the dimension of $\mathbf{W}$ and $\mathbf{b}$<ul><li>$\mathbf{W}$ = $|\mathbf{y}| \times |\mathbf{x}|$</li><li>$\mathbf{b}$ is bias = $|\mathbf{y}|$</li></ul></li></ul><a href=#learning-and-objective-functions><h3 id=learning-and-objective-functions><span class=hanchor arialabel=Anchor># </span>Learning and Objective Functions</h3></a><p><strong>Two views</strong>
* <strong>Maximize Likelihood</strong> : Attempt to put as much as probability on the target output
* <strong>Minimize Error</strong> : Minimizes the <strong>cross-entropy</strong> loss, which penalizes the model for the proportion of the output probability mass that it does not assign to the target output.
* Gradient descent: good framework, however, limited in power in practice, use the following to make it more efficient
* with momentum
* individual learning rates for each dimension
* adaptive learning rate
* decoupling step length from partial derivatives</p><a href=#recurrent-networks-for-language-modeling-rnn><h1 id=recurrent-networks-for-language-modeling-rnn><span class=hanchor arialabel=Anchor># </span>Recurrent Networks for Language Modeling (RNN)</h1></a><p>Readings</p><ul><li>Sections¬†4 and 5 of¬†
<a target=‚Äú_blank‚Äù href=https://arxiv.org/abs/1703.01619 rel=noopener>Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a>, Neubig</li></ul><p>So far we have seen n-gram and Feedforward networks:</p><ul><li>both have fixed context width, however, linguistic dependencies can be arbitrarily long. This is where RNN come in!</li></ul><p><strong>- Why not to use a standard network for sequence tasks? There are two problems:</strong></p><ul><li><p><strong>Inputs, outputs can be different lengths in different examples</strong>.</p><ul><li>This can be solved for normal NNs by paddings with the maximum lengths but it&rsquo;s not a good solution.</li></ul></li><li><p><strong>Doesn&rsquo;t share features learned across different positions of text/sequence</strong>.</p><ul><li>Using a feature sharing like in CNNs can significantly reduce the number of parameters in your model. That&rsquo;s what we will do in RNNs.</li></ul></li><li><p>Recurrent neural network doesn&rsquo;t have either of the two mentioned problems.</p></li><li><p><strong>Glossary</strong></p><ul><li>$x_i$ : the input word transformed into one hot encoding</li><li>$y_i$ : the output probability distribution</li><li>$\mathbf{U}$ : the weight matrix of the recurrent layer</li><li>$\mathbf{V}$ : the weight matrix between the input layer and the hidden layer</li><li>$\mathbf{W}$ : the weight matrix between the hidden layer and the output layer</li><li>$\sigma$ : the sigmoid activation function</li><li>$\mathbf{h}$ : the hidden layer</li></ul></li><li><p>A diagram showing a RNN (this in an unfolded representation or a complete computational graph)
*
* No markov assumption is made here: $$
\begin{align}
&\quad P(x_{i+1}|x_1, &mldr;,x_i) = y_i \\&\quad y_i = softmax(\mathbf{Wh_i + b_2}) \\&\quad h_i = \sigma(\mathbf{Vx_i + Uh_{i-1} + b_1}) \\&\quad x_i = onehot(\mathbf{x_i})
\end{align}
$$</p><ul><li><strong>Training:</strong><ul><li>SGD with cross entropy</li><li><strong>BPTT (backpropagation through time)</strong> can be done through arbitrary number of steps $\tau$. This makes RNN a deep neural network with $\tau$ depth, which can be arbitrarily large<ul><li>In practice we implement <strong>Truncated BPTT</strong>, i.e. we only update gradients only through a limited number of steps.</li><li>If we have large number of steps $\tau$, derivatives which are backpropagated becomes vanishingly small, resulting in no weight update. This problem is called <strong>Vanishing Gradient Problem</strong> which prevents the RNN from learning long term dependencies.</li></ul></li></ul></li></ul></li></ul><a href=#long-short-term-memory-networks-lstms><h1 id=long-short-term-memory-networks-lstms><span class=hanchor arialabel=Anchor># </span>Long Short-term Memory Networks (LSTMs)</h1></a><p>Readings:</p><ul><li>Section¬†6¬†of¬†
<a target=‚Äú_blank‚Äù href=https://arxiv.org/abs/1703.01619 rel=noopener>Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a>, Graham¬†Neubig.¬†</li><li><a target=‚Äú_blank‚Äù href=http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf rel=noopener>Backpropagation through Time</a>, Jiang Guo.</li></ul><p>Helpful Resources:
<a target=‚Äú_blank‚Äù href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/ rel=noopener>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p>LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) that is designed to handle the vanishing gradient problem in traditional RNNs. LSTMs can effectively capture long-term dependencies in sequential data by allowing information to flow through &ldquo;memory cells&rdquo; that can be selectively read from or written to at each time step.</p><p>Here are the main steps in LSTMs:</p><ul><li><p>First we have a <strong>Forget gate</strong>, which determines how much of the previous memory cell value should be retained, from cell state $C_{t-1}$</p><p>$f_t = \sigma(W_f[x_t, h_{t-1}] + b_f)$</p></li><li><p>Next step, is to decide what new information we&rsquo;re going to store in the cell steate. <strong>Input gate:</strong> determines how much of the new input should be incorporated into the memory cell. <strong>Candidate memory cell value:</strong> the new value to be potentially added to the memory cell</p><p>$i_t = \sigma(W_i[x_t, h_{t-1}] + b_i)$
$\tilde{C}<em>t = \tanh(W_c[x_t, h</em>{t-1}] + b_c)$</p></li></ul><p>It&rsquo;s now time to update the old cell state, $C_{t-1}$ into the new cell state $C_{t}$. <strong>Memory cell update:</strong> combines the input, forget, and candidate values to update the memory cell value</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
</span></span><span class=line><span class=cl>![[LSTM3-focus-C.png]]
</span></span></code></pre></td></tr></table></div></div><ul><li><p>Finally, we need to decide what we‚Äôre going to output. <strong>Output gate:</strong> determines how much of the current memory cell value should be output. <strong>Hidden state output:</strong> applies the output gate to the updated memory cell value to compute the hidden state output at the current time step</p><p>$o_t = \sigma(W_o[x_t, h_{t-1}] + b_o)$
$h_t = o_t * \tanh(C_t)$</p></li></ul><p>where $x_t$ is the input at time step $t, h_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, \sigma is the sigmoid activation function, and * denotes element-wise multiplication.</p><p>These equations allow the LSTM to selectively forget or remember information at each time step, based on the input and the current state of the memory cell. By doing so, LSTMs can effectively capture long-term dependencies in sequential data, making them well-suited for tasks such as natural language processing and speech recognition.</p><p><strong>How does LSTM solve the gradient problem?</strong></p><ul><li>the memory cell is linear, so its gradient doesn‚Äôt vanish;<ul><li><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>$C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
</span></span></code></pre></td></tr></table></div></div></li><li><p>This is linear.</p></li><li><p>If $f_t$ is 1 and $i_t$ is 0 then we can simply pass the activation from previous cell state $C_{t-1}$ ; this means LSTM block can retain information indefinitely.</p></li></ul></li><li>All the gates are trainable; the LSTM block learns when to accept input, produce output and forget information</li></ul><a href=#gated-recurrent-unit-gru><h1 id=gated-recurrent-unit-gru><span class=hanchor arialabel=Anchor># </span>Gated Recurrent Unit (GRU)</h1></a><p>GRUs are similar to LSTMs in that they are also designed to handle the vanishing gradient problem in traditional RNNs, but they have a simpler architecture with fewer parameters. Like LSTMs, GRUs use &ldquo;gates&rdquo; to selectively filter information flow through the network. However, they have a more streamlined approach to gating, with just two gates: an update gate and a reset gate.</p><p>Here are the main equations used in GRUs:</p><ul><li><p><strong>Update gate:</strong> determines how much of the previous hidden state should be retained</p><p>$z_t = \sigma(W_z[x_t, h_{t-1}] + b_z)$</p></li><li><p><strong>Reset gate:</strong> determines how much of the previous hidden state should be ignored</p><p>$r_t = \sigma(W_r[x_t, h_{t-1}] + b_r)$</p></li><li><p><strong>Candidate hidden state output:</strong> a new value to be potentially added to the hidden state output</p><p>$\tilde{h}<em>t = \tanh(W_h[x_t, r_t * h</em>{t-1}] + b_h)$</p></li><li><p><strong>Hidden state output:</strong> a linear combination of the current and candidate hidden state values, controlled by the update gate</p><p>$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$</p></li></ul><p>where $x_t$ is the input at time step $t, h_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, and \sigma is the sigmoid activation function. * denotes element-wise multiplication.</p><p>These equations allow the GRU to selectively update or ignore information at each time step, based on the input and the previous hidden state. The update gate $z_t$ controls how much of the new candidate hidden state value $\tilde{h}<em>t$ should be added to the previous hidden state value $h</em>{t-1}$, while the reset gate $r_t$ controls how much of the previous hidden state value should be ignored.</p><p>Overall, the GRU architecture is simpler than that of the LSTM, but it can still effectively capture long-term dependencies in sequential data. By selectively updating or ignoring information at each time step, the GRU can adapt to changes in the input and capture complex patterns in the data.</p><a href=#difference-bw-gru-vs-lstm><h3 id=difference-bw-gru-vs-lstm><span class=hanchor arialabel=Anchor># </span>Difference b/w GRU v/s LSTM</h3></a><p>One of the main differences is that GRUs have a simpler architecture with fewer parameters than LSTMs. <strong>GRUs use two gates, an update gate and a reset gate, to selectively update or ignore information in the hidden state</strong>, whereas LSTMs have <strong>three gates, an input gate, a forget gate, and an output gate, as well as a memory cell.</strong></p><p>Another difference is in the way that the hidden state is updated. In GRUs, <strong>the candidate hidden state value is computed using a single matrix multiplication of the input and the previous hidden state</strong>, while in LSTMs, the <strong>candidate memory cell value is computed separately from the input and the previous hidden state, using two matrix multiplications and an element-wise multiplication.</strong>
In GRUs, the candidate hidden state is computed as the element-wise product of the reset gate $r_t$ and the previous hidden state $h_{t-1}$, which is then concatenated with the input $x_t$ and transformed by a weight matrix W_h and an activation function tanh. In contrast, in LSTMs, the candidate memory cell $\tilde{C}<em>t$ is computed separately from the input $x_t$ and the previous hidden state $h</em>{t-1}$, using a weight matrix $W_C$ and an activation function $tanh$.
Furthermore, the memory cell $C_t$ in LSTMs is updated using both the forget gate $f_t$ (which controls how much of the previous memory cell state to retain) and the input gate $i_t$ (which controls how much of the new candidate memory cell value to use). The hidden state $h_t$ is then computed using the output gate $o_t$ (which controls how much of the memory cell state to output) and the updated memory cell $C_t$. In GRUs, the hidden state is computed directly using the candidate hidden state and the update gate $z_t$.</p><p>The differences in architecture between GRUs and LSTMs have implications for their performance in different tasks. For example, GRUs may be more suitable for tasks that require faster training and less complex models, while LSTMs may be more suitable for tasks that require modeling more complex dependencies and handling longer sequences of data.</p></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fNLU%2fWeek2%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ‚ô•Ô∏è hearing when the post was
helpful. It makes me smile üòÑ.</p></div><div id=contact_buttons><footer><p>Made with ‚ô• in Richmond, VA<br>¬© 2023 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>