<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Introduction Readings:
The Future of Computational Linguistics: Beyond the Alchemy Church and Lieberman (21)
Why has deep learning taken over NLP?
Universal Function Approximators Representation Learning Multi-task learning Works with wide class of data: strings, labels, trees, graphs, tables, images deep learning solves the difficulties of applying machine learning to NLP&mldr; But it does not solve NLP!"><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.87988a113f9da26a245c74dfd4fe9ffb.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.08aa5668b0574ddf4c558e24b5d47c32.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#introduction><h1 id=introduction><span class=hanchor arialabel=Anchor># </span>Introduction</h1></a><p>Readings:</p><ul><li><p><a target=“_blank” href=https://www.frontiersin.org/articles/10.3389/frai.2021.625341/full rel=noopener>The Future of Computational Linguistics: Beyond the Alchemy</a> Church and Lieberman (21)</p></li><li></li><li><p>Why has deep learning taken over NLP?</p><ul><li>Universal Function Approximators</li><li>Representation Learning</li><li>Multi-task learning</li><li>Works with wide class of data: <strong>strings, labels, trees, graphs, tables, images</strong></li><li>deep learning solves the difficulties of applying machine learning to NLP&mldr; <em>But it does not solve NLP!</em></li></ul></li><li><p>Problems with DL:</p><ul><li>Energy - carbon cost increasing exponentially</li><li>Ethical practice lags technical practice - <em>Privacy, Bias, Justice</em> etc.</li></ul></li></ul><p><strong>Fundamental Methods of the Course</strong></p><p>Our primary tool will be probabilistic models parameterized by</p><p>deep learning architectures such as:<br>• feed-forward neural networks<br>• recurrent neural networks<br>• transformers</p><p>&mldr; applied primarily to structured prediction tasks in NLP.</p><p>The second half of the course will focus on the application of deep models to a variety of core
<strong>NLP tasks:</strong><br>• machine translation<br>• word embeddings<br>• pretrained language models<br>• syntactic parsing<br>• semantic parsing<br><strong>And applications:</strong><br>• paraphrasing<br>• question answering<br>• summarization<br>• data-to-text generation</p><a href=#machine-translation><h1 id=machine-translation><span class=hanchor arialabel=Anchor># </span>Machine Translation</h1></a><p>Readings:</p><ul><li>Background Reading:
<a target=“_blank” href=https://kevincrawfordknight.github.io/papers/aimag97.pdf rel=noopener>Automating Knowledge Acquisition for Machine Translation</a>, Knight.</li></ul><p><strong>Challenges in MT:</strong></p><ul><li>Words are ambiguous (<strong>French: Banque</strong> / <strong>English: Bank</strong>, <strong>French: rive</strong> / <strong>English: Bank</strong>)</li><li>Words have complex morphology<ul><li>Tense</li><li>Singular/Plural</li><li>Morphology:<ul><li>Finnish: ostoskeskuksessa<br>ostos#keskus+N+Sg+Loc:in<br>shopping#center+N+Sg+Loc:in<br>English: ‘in the shopping center’</li></ul></li></ul></li><li>Word order matters (reordering can change the meaning of sentence)</li><li>Every word counts (can&rsquo;t omit word during translation)</li><li>Sentences are long sometimes and complex</li></ul><p><strong>Cues which might be helpful for translation:</strong></p><ul><li>Pairs of words that occur consecutively in the target language (bigrams)</li><li>Pairs of source and target words that occur frequentyl occur together in translations</li></ul><p><strong>How to model these cues? what are the ways they can fail?</strong>
Let&rsquo;s look at them!!</p><a href=#conditional-language-models-with-n-grams><h1 id=conditional-language-models-with-n-grams><span class=hanchor arialabel=Anchor># </span>Conditional Language Models (with n-grams)</h1></a><p>Readings:</p><ul><li><a target=“_blank” href=https://www.learn.ed.ac.uk/bbcswebdav/pid-7935785-dt-content-rid-30666816_1/xid-30666816_1 rel=noopener>Word Alignment and the Expectation Maximization Algorithm,</a> Lopez.</li><li> 
<a target=“_blank” href=https://arxiv.org/abs/1703.01619 rel=noopener>Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a></li></ul><p><strong>How to derive an n-gram language model</strong></p><p>To define the probability $P(w) = P(w1 . . . w_{|w|})$ for a given sequence of words w, we can use an n-gram language model. An n-gram model estimates the probability of a word sequence by approximating it as the product of probabilities of individual words given their previous n-1 words, where n is a positive integer (usually 1, 2, or 3).</p><p>Specifically, we can define P(w) as the product of conditional probabilities of each word given its previous n-1 words:</p><p>$$P(w) = P(w_1, w_2, \dots, w_{|w|}) \approx \prod_{i=1}^{|w|} P(w_i | w_{i-1}, w_{i-2}, \dots, w_{i-n+1})$$</p><p>where $P(wi | w(i-1), w(i-2), &mldr;, w(i-n+1))$ is the conditional probability of the ith word given its previous n-1 words, and the approximation holds by the Markov assumption that the probability of a word only depends on its previous n-1 words.</p><p><strong>Maximizing likelihood by counting n-grams</strong></p><p>counting n-grams is one way to estimate the probabilities in an n-gram language model, and it aims to maximize the likelihood of the observed data. The likelihood function L estimates the probability of the observed data given the model parameters (i.e., the n-gram probabilities):</p><p>$$L(P(w_1, w_2, &mldr;, w_{|w|})) = \prod_{i=1}^{|w|} P(w_i | w_{i-1}, w_{i-2}, &mldr;, w_{i-n+1})$$
$$
\hat{\theta} = \arg\max_\theta P(D | \theta)
$$</p><p>where $w_1, w_2, &mldr;, w_n$ is the observed data. The goal of estimating the n-gram probabilities is to maximize the likelihood function L, given the observed data.</p><p>Counting n-grams is a simple and effective method to estimate the n-gram probabilities, where we count the frequency of each n-gram in the training data and normalize it by the count of its preceding (n-1)-gram. However, this method has a <strong>limitation that it assigns zero probability to any n-gram that did not occur in the training data (i.e., zero-frequency problem), which can lead to poor performance in handling unseen data.</strong></p><p>To overcome this limitation, <strong>smoothing techniques are often used to estimate the probabilities of unseen n-grams.</strong> Smoothing methods aim to assign non-zero probabilities to unseen n-grams by borrowing probabilities from other n-grams or using prior knowledge.</p><p><strong>Predition using n-gram LM</strong></p><p>If we have a sequence of words $w_1 . . . w_k$, then we can use the language model to predict the next word $w_{k+1}$:
$$w^*<em>{k+1} = \operatorname{argmax}</em>{k+1} P(w_{k+1} | w_1, w_2, \dots, w_k)$$</p><a href=#can-we-model-machine-translation-with-n-gram><h3 id=can-we-model-machine-translation-with-n-gram><span class=hanchor arialabel=Anchor># </span>Can we model Machine Translation with n-gram?</h3></a><p>Yes, we can: but there is a lot of machinery involved such as:
Some issue:</p><ul><li>Length mismatch b/w source and target language.</li><li>Change in word order b/w source and target languages</li></ul><p><strong>Machine translations is Conditional Language Modeling:</strong></p><ul><li>We will have one-word or phrase from one language mapped to the other language</li><li>We will need to model this alignment. Now this alignment is actually unknown and unobserved, so HMM can be a good choice of model here.</li></ul><p>$$MT = P(|y|||x|)\prod_{i=1}^{|y|} P(z_i||x|)P(y_i|x_{z_i})$$
Here: $P(|y|||x|)$ means for all pairs of sentence lengths |y|, |x|. $P(y_i|x_i)$ is the pairs of all co-occuring words in both languages. $P(z_i||x|)$ probability of how $y_i$ aligns with $x_i$ according to some distribution.</p><p>Also: $P(z_i||x|)$ is the <em>transition probability</em> and $P(y_i|x_{z_i})$ is the <em>emission probability</em>. $z$ is the latent variable here which is unobserved. We can use <strong>Expectation-Maximization</strong> to estimate the MLE $\hat{\theta}$ .</p><p>Now for <strong>decoding</strong>:
$$
P(y|x) = \frac{P(x|y) P(y)}{P(x)} \propto P(y) P(x|y)
$$
where: $P(y)$ is <strong>language model</strong> and $P(x|y)$ is the <strong>translation model.</strong></p><p><strong>Why care about n-grams? Aren’t they obsolete?</strong></p><ol><li><p>Many of these ideas turn up again in neural models.</p><p>• All machine learning maximizes some <strong>objective function</strong>.<br>• Neural models still use <strong>beam search</strong>.<br>• <strong>Latent variables</strong> are common in <strong>unsupervised learning</strong>.<br>• <strong>Alignment</strong> directly inspired <strong>neural attention</strong>.<br>• Neural models exploit same signals, though more powerful.</p></li><li><p>Older models are still often useful in <strong>low-data settings</strong>.</p></li><li><p>An extension of the model in this lecture translates n-grams to n-grams: <strong>phrase-based translation</strong>. It is still used by Google for some languages, despite move to neural MT in 2017.</p></li><li><p>Understanding the tradeoffs of working with Markov assumptions will help you appreciate the fact that neural models usually make them go away!</p></li></ol></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fNLU%2fWeek1%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ♥️ hearing when the post was
helpful. It makes me smile 😄.</p></div><div id=contact_buttons><footer><p>Made with ♥ in Edinburgh, Scotland, UK<br>© 26263 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>