<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Neural Parsing Readings
Grammar as a Foreign Language, Vinyals et al., NeurIPS 2015. This is the encoder-decoder-based pasing model introduced in the lecture.
Constituency parsing with a self-attentive encoder. Kitaev and Klein, ACL 2018."><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.71139b94f3f29dfda52032fbfdd80a2a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.199b7e26532fa9b1aed82b2e5bf94695.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#neural-parsing><h1 id=neural-parsing><span class=hanchor arialabel=Anchor># </span>Neural Parsing</h1></a><p>Readings</p><ul><li><p><a target=“_blank” href=https://arxiv.org/abs/1412.7449 rel=noopener>Grammar as a Foreign Language</a>, Vinyals et al., NeurIPS 2015. This is the encoder-decoder-based pasing model introduced in the lecture.</p></li><li><p><a target=“_blank” href=https://arxiv.org/abs/1805.01052 rel=noopener>Constituency parsing with a self-attentive encoder.</a> Kitaev and Klein, ACL 2018. This is the transformer-based parsing model introduced in the lecture.</p></li><li><p><strong>The Big Idea</strong></p><ul><li>The big idea is that we can generate any structured representation that can be linearized.<ul><li>Eg: syntactic parsing, where the input is a string of word and the output is a tree. It&rsquo;s a fairly high it&rsquo;s a highly structured object.</li><li>Eg: Trees, Graphs</li></ul></li></ul></li><li><p>Parsing is the task of turning a sequence of words: (it helps disambiguate)</p><ul><li>(1) You saw a man with a telescope. into a syntax tree:</li></ul></li><li><p><strong>How can we use an encoder-decoder model for parsing?</strong></p><ul><li><strong>Input</strong>: is a string</li><li><strong>Outpu</strong>t: is not a string yet, we need to <strong>linearize the syntax tree</strong>:<ul><li><code>(S (NP (Pro You ) ) (VP (V saw ) (NP (Det a ) (N man ) (PP (P with ) (Det a ) (N telescope ) ) ) ) )</code></li><li>We can even get rid of the words (is this a valid assumption?)<ul><li><code>(S (NP Pro ) (VP V (NP Det N (PP P Det N ) ) ) )</code></li></ul></li><li>And we can make it <em>easier to process by annotating also the closing brackets</em>:<ul><li>$(S\ (NP \ Pro)<em>{NP} (VP\ V(NP\ Det\ N(PP\ Det\ N)</em>{PP} )<em>{NP} )</em>{VP} )_S$</li></ul></li></ul></li></ul></li></ul><p><strong>What could go wrong?</strong></p><ul><li><p>How do we know if the generated tree is valid?**</p><ul><li>turns out that RNN learns to close open brackets most of the time</li></ul></li><li><p><strong>How about cross branches in syntax tree?</strong></p><ul><li>model learns it as well</li></ul></li><li><p><strong>Encoder-Decoder for Parsing</strong>
<a target=“_blank” href=https://arxiv.org/abs/1412.7449 rel=noopener>Grammar as a Foreign Language</a>,</p><ul><li></li><li>To make it work properly, we need to do following:<ul><li><strong>Add an end-of-sequence symbol</strong>, as output sequences can vary in length.</li><li><strong>Reverse the input string</strong>: results in small performance gain.</li><li>Make the <strong>network deeper</strong>. Vinyals et al. (2015) use three LSTM layers for both encoder and decoder.</li><li>Add <strong>attention</strong>. This essentially works like the encoder-decoder with attention we saw for MT (lecture 7).</li><li>Use pre-trained word embeddings as input (here: word2vec).</li><li><strong>Get lots of training data</strong>. Vinyals et al. (2015) use an existing parser (the Berkeley parser) to parse a large amount of text, which they then use as training data.</li></ul></li></ul></li><li><p><strong>Potential Problems</strong></p><ul><li>How do we make sure that opening and closing brackets match? Else we won’t have a well-formed tree!<ul><li>This is really rare (0.8–1.5% of sentences). And if it occurs, just fix the brackets in post-processing (add brackets to beginning or end of the sequence).</li></ul></li><li>How do we associate the words in the input with the leaves of the tree in the output?<ul><li>You could just associate each input word with a PoS in the output, in sequence order. But in practice: only the tree is evaluated. Vinyals et al. (2015) replace all PoS tags with XX.</li></ul></li><li>The output sequence can be longer than the input sequence, isn’t this a problem?<ul><li>encoder-decoder can match sequence length mismatch; there is no restriction on sequence length both input and generated</li></ul></li><li>How can I make sure that the model outputs the best overall sequence, not just the best symbol at each time step?<ul><li>Use beam search to generate the output (as in MT). However, in practice, beam size has very little impact on performance. (this may not be the optimal one)</li><li>How about - viterbi decoding or probabilstic CYK (guranteed optimal)</li></ul></li></ul></li><li><p><strong>Evaluation</strong></p><ul><li>Training Corpora: (part of penn tree bank was used)<ul><li><strong>Wall Street Journal (WSJ)</strong>: treebank with 40k manually annotated sentences.</li><li><strong>BerkeleyParser corpus</strong>: 90k sentences from WSJ and several other treebanks, and 11M sentences parsed with <strong>Berkeley Parser</strong>.</li><li><strong>High-confidence corpus</strong>: 90k sentences from WSJ from several treebanks, and 11M sentences for which two parsers produce the same tree (length resampled).</li><li></li></ul></li></ul></li><li><p><strong>Parsing with Transformers</strong>
<a target=“_blank” href=https://arxiv.org/pdf/1805.01052.pdf rel=noopener>Constituency Parsing with a Self-Attentive Encoder</a></p></li><li><p><strong>Architecture:</strong></p><ul><li></li><li><strong>Context-Aware Word Representation</strong><ul><li>sub-words are used for representation of token</li><li>simple concatenation of character embeddings or word prefixes/suffixes can outputperform POS information from external system</li><li></li></ul></li></ul></li><li><p><strong>Content v/s Position Attention</strong></p><ul><li><strong>Attention on position</strong>: Different locations in the sentence can attend to each other based on their positions,</li><li><strong>Attention on content/context</strong>: also based on their contents (i.e. based on the words at or around those positions)</li><li>We will later factor these two components in attention and we will find an improved performance in the parsing task.</li></ul></li></ul><a href=#unsupervised-parsing><h1 id=unsupervised-parsing><span class=hanchor arialabel=Anchor># </span>Unsupervised Parsing</h1></a><p>Readings</p><ul><li><a target=“_blank” href=https://arxiv.org/abs/2010.03146 rel=noopener>Unsupervised parsing via constituency tests</a>. Cao et al., EMNLP 2020.</li></ul><p><strong>What happens when we do not have a tree bank?</strong> Let&rsquo;s find out!</p><ul><li><p><strong>General approach of Cao. et. al:</strong></p><ul><li><p><strong>constituency test</strong>:</p><ul><li>take a sentence, modify it via a transformation (e.g., replace a span with a pronoun);</li><li>One type of constituency test involves modifying the sentence via some transformation and then judging the result.</li><li>An unsupervised parser is designed by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.</li><li>If the constituents pass the linguistic tests; we have evidence that a proposed constituent is an actual constituent</li></ul></li><li><p><strong>Neural grammaticality models</strong></p><ul><li>check if the result is grammatical using a neural grammaticality model;</li><li>produce a tree for the sentence by aggregating the results of constituency tests over spans;</li><li>Specifically, we score the likelihood that a span is a constituent by<br>applying the constituency tests and averaging their grammaticality judgments<ul><li>$$s_{\theta}(\text{sent}, i, j) = \dfrac{1}{\lvert C \rvert}\sum_{c \in C} g_{\theta}(c(\text{sent}, i, j))$$<ul><li>C denotes the set of constituency tests</li><li>$c(sent, i, j)$ is a proposed constintuent</li><li>a judgment function (grammaticality model) $g : sent → {0, 1}$ that gives a score. If 1 -> constituent is grammaticaly otherwise not.</li></ul></li></ul></li><li><strong>How we learn the grammaticality model?</strong>
*</li></ul></li><li><p><strong>Parsing Algoritm</strong></p><ul><li>select the tree with the highest score; $$t^*(sent) = arg\max_{t \in T (len(sent))} \sum_{(i,j) \in t} s_{\theta}(sent, i, j)$$<ul><li>$T(len(sent))$ denotes the set of binary trees with $len(sent)$ leaves.</li><li>If we interpret the score $s_θ(sent, i, j)$ as estimating the probability that the span $(sent, i, j)$ is a constituent, then this <strong>formulation corresponds to choosing the tree with the highest expected number of constituents</strong>, i.e. minimum risk decoding<ul><li>This allows for a low probability span in the tree if the model is confident about the rest of the tree</li></ul></li></ul></li><li>we <strong>score each tree by summing the scores</strong> of its spans and choose the <strong>highest scoring binary tree via CKY</strong></li></ul></li><li><p><strong>Refinement:</strong></p><ul><li>add to this refinement: alternate between improving the tree and improving the grammaticality model.</li><li><a href=#how-does-this-work><h2 id=how-does-this-work><span class=hanchor arialabel=Anchor># </span><strong>How does this work?</strong></h2></a></li></ul></li></ul></li></ul></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fNLU%2fWeek9%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ♥️ hearing when the post was
helpful. It makes me smile 😄.</p></div><div id=contact_buttons><footer><p>Made with ♥ in Edinburgh, Scotland, UK<br>© 25257 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>