<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Open Vocabulary Models Readings
Neural Machine Translation of Rare Words with Subword Units, Sennrich et al. (2016)
BPE-Dropout: Simple and Effective Subword Regularization, Provilkov et al. (2020)
We are encoding rare and unknown words as sequences of subword units."><title>Himanshu Maurya</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://lordzuko.github.io//icon.png><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&family=Source+Sans+Pro:wght@400;600;700&family=Fira+Code:wght@400;700&family=Oswald:wght@700&display=block" rel=stylesheet><link href=https://lordzuko.github.io/styles.56d47f5cc6b40c4c97475ab7dab36b03.min.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://lordzuko.github.io/js/popover.688c5dcb89a57776d7f1cbeaf6f7c44b.min.js></script>
<script>const BASE_URL="https://lordzuko.github.io/",fetchData=Promise.all([fetch("https://lordzuko.github.io/indices/linkIndex.71139b94f3f29dfda52032fbfdd80a2a.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://lordzuko.github.io/indices/contentIndex.fab86bbe49cf4e5f56dd16934c1866ee.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),draw=()=>{const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(draw);e.textContent="",drawGraph("https://lordzuko.github.io",[{"/moc":"#4388cc"}],1,!0,!1,!0),initPopover("https://lordzuko.github.io",!0,!0),renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script>window.Million={navigate:e=>window.location.href=e,prefetch:()=>{}},draw()</script></head><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://lordzuko.github.io/js/search.1d58f2d3eaac68cc50beeb118d91edc9.min.js></script><div class=bodyContent><div class="singlePage logo-background"><header><a class=header_logo href=https://lordzuko.github.io/><img src=../../../header.svg alt="Himanshu Maurya"></a><div class=spacer></div><svg tabindex="0" id="search-icon" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></header><article><ul class=tags></ul><a href=#open-vocabulary-models><h1 id=open-vocabulary-models><span class=hanchor arialabel=Anchor># </span>Open Vocabulary Models</h1></a><p>Readings</p><ul><li><p><a target=“_blank” href=https://www.aclweb.org/anthology/P16-1162/ rel=noopener>Neural Machine Translation of Rare Words with Subword Units</a>, Sennrich et al. (2016)</p></li><li><p><a target=“_blank” href=https://aclanthology.org/2020.acl-main.170.pdf rel=noopener>BPE-Dropout: Simple and Effective Subword Regularization</a>, Provilkov et al. (2020)</p></li></ul><p>We are encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).</p><p><strong>Problem:</strong></p><p><strong>how do we represent text?</strong></p><ul><li><p>1-hot encoding</p><ul><li>lookup of word embedding for input</li><li>probability distribution over vocabulary for output</li></ul></li><li><p>large vocabularies</p><ul><li>increase network size</li><li>decrease training and decoding speed</li></ul></li><li><p>typical network vocabulary size: 10,000–100,000 symbols</p></li></ul><p><strong>NLU and NLG are open-vocabulary problems</strong></p><ul><li>many training corpora contain millions of word types</li><li>productive word formation processes (compounding; derivation) allow formation and understanding of unseen words<ul><li>names, numbers are morphologically simple, but open word classes</li></ul></li><li>Rest of this class we are going to focus on translation</li></ul><p><strong>Some common approaches which do not work:</strong></p><ul><li><strong>Ignoring rare words:</strong><ul><li>replace out-of-vocabulary words with UNK</li><li>a vocabulary of 50,000 words covers 95% of text which gets you 95% of the way; if you only care about automatic metrics</li><li>However:<ul><li>rare words are generally high information words:<ul><li>Eg: We we miss the name below; we won&rsquo;t know the <code>subject</code> of the sentence<ul><li><strong>source</strong>: Mr <code>Gallagher</code> has offered a ray of hope.</li><li><strong>reference</strong>: Herr <code>Gallagher</code> hat einen hoffnungsstrahl ausgesandt .</li></ul></li></ul></li></ul></li></ul></li></ul><p><strong>Some initial approaches, which are not good enough:</strong></p><ul><li><strong>Approximative Softmax</strong><ul><li>compute softmax over &ldquo;active&rdquo; subset of vocabulary (20 subsets)<ul><li>→ smaller weight matrix, faster softmax</li></ul></li><li>at training time: vocabulary based on words occurring in training set partition</li><li>at test time: determine likely target words based on source text (using cheap method like translation dictionary)</li><li>limitations<ul><li>allows larger vocabulary, but still not open (completely new words don&rsquo;t have any representation)</li><li>network may not learn good representation of rare words (eg. a word seen only once in 20 sets, will not have a good representation)</li></ul></li></ul></li><li><strong>Using back-off models</strong><ul><li>replace rare words with <strong>UNK</strong> at training time</li><li>when system produces <strong>UNK</strong>, align <strong>UNK</strong> to source word, and translate this with back-off method</li><li>limitations:<ul><li><strong>compounds</strong>: hard to model 1-to-many relationships (assumes that there is a 1-1 mapping of words in source and target languages)</li><li><strong>morphology</strong>: hard to predict inflection with back-off dictionary (eg. in languages like turkish with complex morphology)</li><li><strong>names</strong>: if alphabets differ, we need transliteration</li><li><strong>alignment</strong>: attention model unreliable (if you use alignment, to map words, the alignment from attention could be unreliable)</li></ul></li></ul></li><li><strong>character-based translations with phrase based models</strong> - good results for closely related languages</li><li><strong>segmentation algorithm for phrase-based SMT are too conservative</strong> - we need aggressive segmentation for open-word vocabulary (compact, without need for back-off dictionary)</li></ul><p><strong>Important approaches, which work and are generally good:</strong></p><ul><li><strong>Subword NMT</strong><ul><li><p><strong>Subword Translation:</strong></p><ul><li>Idea: Rare words are potentially translateable through smaller units</li><li>Subword segmentation can also avoid the information bottleneck of a fixed-length representation - i.e. in character models when we have too long words, joining embeddings could loose information.</li><li>Potential category or words:<ul><li><strong>named entities</strong>: Between languages that share an alphabet, names can often be copied from source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example:<ul><li>Barack Obama (English; German)</li><li>バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)</li></ul></li><li><strong>cognates and loanwords</strong>: Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient . Example:<ul><li>claustrophobia (English)</li><li>Klaustrophobie (German)</li></ul></li><li><strong>morphologically complex words</strong>: Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example:<ul><li>solar system (English)</li><li>Sonnensystem (Sonne + System) (German)</li><li>Naprendszer (Nap + Rendszer) (Hungarian)</li></ul></li></ul></li></ul></li><li><p><strong>Bye Pair Encoding (BPE)</strong> (A method that actually works)</p><ul><li><p>merge frequent pairs of characters or character sequences.</p></li><li><p><strong>why BPE</strong>?</p><ul><li><strong>open-vocabulary</strong>: operations learned on training set can be applied to unknown words</li><li><strong>compression of frequent character sequences improves efficiency</strong> → trade-off between text length and vocabulary size</li></ul></li><li><p><strong>Algorithm</strong>:</p></li><li><p>two strategies:</p><ul><li>apply BPE separately for source and target language</li><li><strong>apply BPE jointly for source and target language</strong> (<strong>Shared BPE</strong>):<ul><li>translitration is used such that both language have same characters</li><li><strong>Big Idea:</strong> <em>If we apply BPE independently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units.</em></li></ul></li></ul></li><li><p>Example:</p></li></ul></li><li><p><strong>BPE-Dropout</strong></p><ul><li><p>Adding stochastic noise to increase model robustness</p></li><li><p><strong>Idea:</strong></p><ul><li>In BPE most frequent words are intact in vocabulary, learns how to compose with infrequent words</li><li>If we sometimes forget to merge, we will learn how words compose, and better transliteration</li><li>forget 1 in 10 times for most scripts, 6/10 in CKJ scripts</li></ul></li><li><p><strong>Algorithm:</strong></p></li><li><p>Consistently give 1+ BLEU scores across language pairs - widely used</p></li><li><p><code>-</code> (merge performed) ; <code>_</code> (red) (merge dropped) ; <code>_</code> (green) (merge performed)
*</p></li></ul></li></ul></li><li><strong>Character level NMT</strong><ul><li>Character-level Models:<ul><li><strong>advantages</strong>:<ul><li>(mostly) open-vocabulary</li><li>no heuristic or language-specific segmentation</li><li>neural network can conceivably learn from raw character sequences •</li></ul></li><li><strong>drawbacks</strong>:<ul><li>increasing sequence length slows training/decoding (reported x2–x8 increase in training time)</li></ul></li><li>open questions<ul><li>on which level should we represent meaning?</li><li>on which level should attention operate?</li><li><strong>The disappointing answer is:</strong> whichever gives the better downstream performance</li></ul></li></ul></li></ul></li></ul><a href=#low-resource-mt><h1 id=low-resource-mt><span class=hanchor arialabel=Anchor># </span>Low Resource MT</h1></a><p>Readings</p><ul><li><a target=“_blank” href=https://arxiv.org/abs/2109.00486 rel=noopener>Survey of Low-Resource Machine Translation</a>, Haddow et al. (2021)</li><li><a target=“_blank” href=https://aclanthology.org/P16-1009.pdf rel=noopener>Improving Neural Machine Translation Models with Monolinguagl Data</a></li><li><a target=“_blank” href=https://arxiv.org/pdf/2103.12028.pdf rel=noopener>Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets</a></li><li><a target=“_blank” href=https://aclanthology.org/D16-1163.pdf rel=noopener>Transfer Learning for Low-Resource Neural Machine Translation</a></li><li><a target=“_blank” href=https://aclanthology.org/W18-6325.pdf rel=noopener>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></li><li><a target=“_blank” href=https://arxiv.org/pdf/1905.12688.pdf rel=noopener>Choosing Transfer Languages for Cross-Lingual Learning</a><ul><li><a target=“_blank” href=https://github.com/neulab/langrank rel=noopener>https://github.com/neulab/langrank</a></li><li>LangRank is a program to solve this task of automatically selecting optimal transfer languages, treating it as a ranking problem and building models that consider the aforementioned features to perform this prediction.</li></ul></li><li><a target=“_blank” href=https://aclanthology.org/2020.tacl-1.47.pdf rel=noopener>Multilingual Denoising Pre-training for Neural Machine Translation</a> - mBART<ul><li><a target=“_blank” href=https://arxiv.org/pdf/1910.13461.pdf rel=noopener>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li></ul></li><li><a target=“_blank” href=https://arxiv.org/pdf/1907.05019.pdf rel=noopener>Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</a></li><li><a target=“_blank” href=https://arxiv.org/pdf/2108.03265.pdf rel=noopener>Facebook AI’s WMT21 News Translation Task Submission</a></li><li><a target=“_blank” href=http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.458.pdf rel=noopener>An Analysis of Massively Multilingual Neural Machine Translation for Low-Resource Languages</a></li></ul><p><em><strong>“Low-resourced”-ness is a complex problem going beyond data availability and refects systemic problems in society.</strong></em> - Masakhane</p><ul><li><p><strong>Corpus Creation</strong></p><ul><li>Web crawling:<ul><li>Extract text from websites identified as multilingual <code>-></code> Align documents then sentences <code>-></code> Collate, deduplicate and filter</li><li>For new languages: ask native speakers for websites where we can collect such parallel data</li></ul></li><li>Process of large scale extraction:
*</li><li>How can we extract parallel data?<ul><li><p>Extraction from monolingual data:</p><ul><li>Large collections of monolingual data contain parallel sentences: <em><strong>Common Crawl, Internet Archive</strong></em></li><li>How do we detect these parallel data:<ul><li>Map sentences into a common embedding space using eg. LASER</li><li>Nearest neighbours to find parallel sentences</li></ul></li><li>Eg: Datasets: <strong>Paracrawl, WikiMatrix, CCMatrix, Samanantar</strong></li></ul></li><li><p>Problems with large scale extraction:</p><ul><li><strong>Tools for low-resource languages are poor</strong> - such techniques may work for resource rich languages, but for resource scarce languages the tools might not be evolved and may not succeed to give the required overall outcome of sufficient quality</li><li><strong>False positives can dominate</strong> - suppose we have very less parallel data - as we are working at petabyte of data, 1% of the false data will be too much false data.</li><li>The crawled resource itself may not have enough parallel data</li></ul></li><li><p>Quality of crawled data</p><ul><li></li></ul></li></ul></li></ul></li></ul><pre><code>  * **ParaCrawl v7.1** seems to be best
</code></pre><ul><li><p><strong>Using Monolingual data for MT</strong></p><ul><li>By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task <code>English↔German (+2.8–3.7 BLEU)</code>, and for the low-resourced IWSLT 14 task <code>Turkish→English (+2.1–3.4 BLEU)</code>, obtaining new state-of-the-art results</li><li></li><li>Iterated back translation for 2-3 iteration is sufficient, however this can fail if initial system is too weak.</li></ul></li><li><p><strong>Using Multilingual data for MT</strong></p><ul><li><strong>Big Idea:</strong><ul><li>First train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs</li></ul></li><li><strong>How transfer learning was done exactly:</strong><ul><li><p>In the French–English to Uzbek–English example, as a result of the initialization, <strong>the English word embeddings from the parent model are copied</strong>, <em>but the Uzbek words are initially mapped to random French embeddings</em>. <strong>The parameters of the English embeddings are then frozen</strong>, while <em>the Uzbek embeddings’ parameters are allowed to be modified, i.e. fine-tuned, during training of the child model</em>.</p></li><li><p>Parent and Child do not need to be related <code>-></code>
<a target=“_blank” href=https://aclanthology.org/W18-6325.pdf rel=noopener>Trivial Transfer Learning for Low-Resource Neural Machine Translation</a></p></li><li><p>Extensive investigation of choice of parents <code>-></code>
<a target=“_blank” href=https://arxiv.org/pdf/1905.12688.pdf rel=noopener>Choosing Transfer Languages for Cross-Lingual Learning</a></p><ul><li>Data set size and lexical overlap important</li></ul></li></ul></li></ul></li><li><p><strong>Transfer learning from Many Monolingual Corpora</strong></p><ul><li><strong>mBART</strong>:<ul><li><strong>Data</strong>: 25 languages from Common Crawl <code>-></code> Then finetune parallel data separately for each task</li><li><strong>Architecture:</strong> Encoder-Decoder</li><li><strong>Learning Method:</strong> Our training data covers $K$ languages: $D = {D_1, . . . , D_K }$ where each $D_i$ is a collection of monolingual documents in language $i$. We assume access to a noising function $g$, defined below, that corrupts text, and (2) train the model to predict the original text $X$ given $g(X)$. More formally, we aim to maximize $L_θ$: $$ L_{\theta} = \sum_{D_i \epsilon D} \sum_{X \epsilon D_i} log P(X|g(X);\theta)$$</li><li><strong>Objective:</strong> loss over full text reconstruction (not just over masked spans)<ul><li><strong>Noise</strong>:<ul><li><strong>mask spans</strong> of text: 35% of words</li><li><strong>permute the order of sentences</strong></li></ul></li></ul></li><li><strong>Token Type:</strong> Language token for both source and target language</li><li><em>mBART</em>, <em>mBART50</em> <code>-></code> Basis of much practical work on low-resource MT</li></ul></li></ul></li><li><p><strong>Multilingual Models</strong>:</p><ul><li><strong>Idea:</strong> Handle all $N$ by $N$ translation directions with a single model (instead of $O(N^2)$</li><li>Usually 1-n or n-1</li><li>Use a small number of related langauges (As not all language pair gives good result - different linguistic properties have an effect, other possibilities as well)</li><li>Or go big: 103 languages
<a target=“_blank” href=https://arxiv.org/pdf/1907.05019.pdf rel=noopener>Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges</a></li><li>There is a trade-off:<ul><li>Transfer: benefit from addition of other languages</li><li>Interferance: performance is degraded due to having to also learn to translate other languages</li></ul></li><li><strong>Pros</strong>: Benefits are more noticeable for the many-to-English and low-resource pairs</li><li><strong>Cons:</strong> High-resource pairs tend to be harmed</li><li><strong>Cons:</strong> Massive systems require capacity</li></ul></li><li><p><strong>Evaluation of Low-resrouce MT</strong></p><ul><li>Is automatic evaluation of low-resource languages harder?<ul><li>Metrics are designed with high-resource langauges in mind</li><li>Metrics are less reliable on poor systems</li><li>Lack of good test sets and human evaluations for training metrics</li></ul></li><li>Human evaluation is preferable for low resource language</li><li>Researchers need to connect to language communities</li></ul></li></ul><a href=#nlp-ethics><h1 id=nlp-ethics><span class=hanchor arialabel=Anchor># </span>NLP Ethics</h1></a><p>Readings</p><ul><li><p><a target=“_blank” href=http://aclweb.org/anthology/P16-2096 rel=noopener>The Social Impact of Natural Language Processing</a>, Hovy and Spruit (2016)</p></li><li><p><a target=“_blank” href=https://arxiv.org/abs/2112.04359 rel=noopener>Ethical and social risks of harm from Language Models</a>   Weidinger et al. (2021)</p></li><li><p><a target=“_blank” href=https://www.scu.edu/media/ethics-center/technology-ethics/IntroToDataEthics.pdf rel=noopener>An Introduction to Data Ethics</a>, Vallor and Rewak. </p></li><li><p><a target=“_blank” href=https://arxiv.org/abs/1901.10002 rel=noopener>A Framework for Understanding Unintended Consequences of Machine Learning</a>, Suresh and Guttag (2019)</p></li><li><p><strong>NLP affects people&rsquo;s lives</strong></p><ul><li>We need to ask who is affected by an NLP experiment?</li><li>Have they consented to the experiment or participating in it? - <strong>Facebook&rsquo;s contagion experiment</strong></li><li>What derived information about you is collected which you have not consented to? they can be traced from their data</li></ul></li><li><p><strong>Who do these systems harm?</strong></p><ul><li>Who gets admitted.</li><li>Who gets hired.</li><li>Who gets promoted.</li><li>Who receives a loan.</li><li>Who receives treatment for medical problems.</li><li>Who receives the death penalty.</li></ul></li><li><p><strong>Type of Risks</strong></p><ul><li><p>The NLP model accurately refects natural speech, including unjust, toxic, and oppressive tendencies present in the training data.</p></li><li><p><strong>Discrimination, Exclusion and Toxicity</strong></p><ul><li><strong>Allocational (material) harm</strong>: discrimination<ul><li>eg. Models that analyse CVs for recruitment can be less likely to recommend historically discriminated groups</li><li>eg: Accent challenge
*</li></ul></li><li><strong>Representative harm</strong>: exclusionary norms eg. Q: what is a family? A: a man and a woman who get married and have children, and social stereotypes (Dr. - Man | Nurse - Woman)<ul><li>Audacious is to boldness as [religion] is to &mldr;<ul><li>Muslim - Terrorism</li><li>Jewish - Money</li></ul></li></ul></li><li><strong>Ofensive Behaviour</strong>: generate toxic language</li></ul></li><li><p><strong>Information Hazards</strong> (Leads to Privacy and safety harms)</p><ul><li>The language models could be prompted to give the private information / safety critical information which is present in it</li></ul></li><li><p><strong>Misinformation Harms</strong></p><ul><li>The LM assigning high probabilities to false, misleading, nonsensical or poor quality information.</li><li><strong>Harms involve</strong>: people believing and acting on false information</li></ul></li><li><p><strong>Malicious Uses</strong></p><ul><li>From humans intentionally using the LM to cause harms</li><li>Types of harms:<ul><li>Illegitimate surveillance and censorship<ul><li>Saudi Arabia monitoring social media <code>-></code> persecutes dissidents without trial, often violently</li></ul></li><li>Facilitating fraud and impersonation scams</li><li>Cyber attacks</li><li>Misinformation campaigns</li></ul></li></ul></li></ul></li><li><p><strong>While solving a problem we can ask these questions:</strong></p><ul><li>Who are the stakeholders?</li><li>What could go wrong?</li><li>Who could benefit, and how?</li><li>Who could be harmed, and how?</li><li>What can you do to mitigate possible harms?</li></ul></li></ul></article></div><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://lordzuko.github.io/js/graph.5388a070919094961d3e5151252e9065.js></script></div></div><div class=footer><div class=outreach><h3>Have a question?</h3><p>Reach out via <strong><a href=mailto:himanshumaurya2214225@gmail.com target=_blank>Email</a></strong> or
<strong><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></strong>.</p><p>If you liked the post, <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2flordzuko.github.io%2fUoE%2fNLU%2fWeek6%2f&text=&via=lordzuko2" target=_blank>click here to share it with your friends on Twitter</a>.<br>I ♥️ hearing when the post was
helpful. It makes me smile 😄.</p></div><div id=contact_buttons><footer><p>Made with ♥ in Richmond, VA<br>© 2023 Himanshu Maurya</p><ul><li><a href=https://twitter.com/lordzuko2 target=_blank>Twitter</a></li><li><a href=https://www.youtube.com/@HimanshuMauryalordzuko target=_blank>YouTube</a></li><li><a href=https://github.com/lordzuko target=_blank>Github</a></li></ul></footer></div></div></div></body></html>