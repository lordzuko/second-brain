{"/AI/Deep-Generative-Modeling":{"title":"Deep Generative Modeling","content":"\nhttps://deepgenerativemodels.github.io/\nhttps://www.deeplearningbook.org/\n\n* https://www.youtube.com/playlist?list=PLsXu9MHQGs8df5A4PzQGw-kfviylC-R9b\n","lastmodified":"2023-06-25T14:09:32.947885868+01:00","tags":[]},"/AI/DeepLearning/Residual-Connections":{"title":"Residual Connections","content":"\n\n","lastmodified":"2023-06-25T14:09:32.948074743+01:00","tags":[]},"/AI/DeepLearning/Resources":{"title":"Resources","content":"\n* [Welcome to the UvA Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/index.html)\n  \n  * Awesome code complete E2E Deep Learning Work\n  * Lectures: [UvA Deep Learning Lectures 2020](https://www.youtube.com/playlist?list=PLdlPlO1QhMiDlES3Vck6oQwO3TMYbdZDk)\n  * Tutorials: [UvA Deep Learning Tutorial Walkthrough - Notebooks](https://www.youtube.com/watch?v=oluO8JiC7EA\u0026list=PLdlPlO1QhMiAkedeu0aJixfkknLRxk1nA\u0026ab_channel=UvADeepLearningcourse)\n* These slides are really good and comprehensive:\n  \n  * http://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/\n","lastmodified":"2023-06-25T14:09:32.95047816+01:00","tags":[]},"/AI/Machine-Learning/Resources":{"title":"Resources","content":"\n* **EE511 - Advanced Introduction to Machine Learning - Spring Quarter, 2020 - Spring Quarter, 2020**\n  * https://people.ece.uw.edu/bilmes/classes/ee511/ee511_spring_2020/\n* **Marc Deisenroth**\n  * https://www.youtube.com/playlist?list=PL93aLKqThq4h7UpgeNhkOtEeCnX3DMseS\n","lastmodified":"2023-06-25T14:09:32.947974702+01:00","tags":[]},"/AI/Maths":{"title":"Maths","content":"\n* **Geometric Meaning of Positive Definite Matrix:**\n  Eg; Hessian in optimization, covariance in gaussian etc.\n\nThe geometric meaning of a positive definite matrix is closely related to its eigenvalues and eigenvectors. A matrix A is positive definite if it satisfies the following conditions:\n\n1. A is symmetric: A = A^T, meaning it is equal to its transpose.\n1. All eigenvalues of A are positive: λ \u003e 0 for all eigenvalues λ of A.\n\nThe positive definite matrix has significant geometric implications:\n\n1. Positive-Definiteness and Quadratic Forms: A positive definite matrix defines a positive quadratic form. This means that when a vector is multiplied by the matrix, the result is always positive, except when the vector is the zero vector. Geometrically, this corresponds to an ellipsoid centered at the origin that is entirely contained within the positive orthant of the space.\n\n1. Defining an Inner Product: A positive definite matrix can be used to define an inner product, also known as a dot product, in a vector space. The inner product measures the angle between vectors and provides a notion of distance or similarity. The positive definiteness ensures that the inner product satisfies the necessary properties, such as being positive, symmetric, and satisfying the triangle inequality.\n\n1. Convexity: Positive definiteness plays a crucial role in convex optimization and convex sets. In particular, if the Hessian matrix of a function is positive definite, the function is convex. Convexity has important implications in optimization, as it guarantees the existence and uniqueness of global minima and efficient optimization algorithms.\n\n1. Positive-Definite Matrices as Covariance Matrices: Positive definite matrices are commonly used as covariance matrices in multivariate statistics and probability theory. Covariance matrices represent the variance and covariance relationships between different variables. The positive definiteness of the covariance matrix ensures that the variables are positively correlated but not perfectly linearly dependent, enabling reliable statistical analysis.\n\nThe importance of positive definite matrices lies in their properties and applications across various fields, including optimization, geometry, statistics, and machine learning. They provide essential mathematical tools for solving optimization problems, defining distances and angles, characterizing convex sets, and modeling relationships between variables. The positive definiteness ensures the reliability and meaningfulness of these mathematical operations and concepts.\n\n* **change in coordinates, using an eigenvalue decomposition**\n\nEigenvector and eigenvalue\n*Given a square matrix A, an eigenvector is a non-zero vector v such that when A is applied to v, the resulting vector is parallel to v. In other words, v only changes by a scalar factor when multiplied by A. This scalar factor is called the eigenvalue corresponding to that eigenvector.*\n\nGiven a square matrix (A), the eigenvalue decomposition expresses (A) as the product of three matrices:\n\n$A = VDV^{-1}$\n\nwhere (V) is a matrix whose columns are the eigenvectors of (A), (D) is a diagonal matrix containing the eigenvalues of (A), and $V^{-1}$ is the inverse of $V$.\n\nTo perform a change in coordinates using the eigenvalue decomposition, the steps are as follows:\n\n1. Eigendecomposition: Find the eigenvectors and eigenvalues of the matrix A.\n\n1. Coordinate Transformation: Define a new coordinate system by using the eigenvectors as the basis vectors. These eigenvectors form an orthonormal basis for the space.\n\n1. Transformation Matrix: Construct a matrix (T) whose columns are the eigenvectors of (A). This matrix (T) represents the transformation from the original coordinate system to the new coordinate system.\n\n1. Diagonal Matrix: Form a diagonal matrix (\\Lambda) using the eigenvalues of (A). Each eigenvalue appears on the diagonal.\n\n1. Change of Coordinate Formula: The change of coordinate formula relates the original coordinates (x) in the original coordinate system to the new coordinates (y) in the transformed coordinate system:\n\n$y = T^{-1}x$\n\nor\n\n$x = Ty$\n\nHere, $T^{-1}$ is the inverse of the transformation matrix (T).\n\nThe eigenvalue decomposition allows for a change of coordinates that simplifies the representation of mathematical problems, revealing underlying structure and facilitating analysis in the transformed coordinate system.\n\n* **Basis vectors**\n\nBasis vectors are fundamental vectors that form a basis for a vector space. They serve as building blocks for representing any vector within that space. A set of basis vectors is chosen to span the entire space and provide a unique representation for each vector.\n\nIn more detail:\n\n1. Basis for a Vector Space:\n   \n   * A vector space is a collection of vectors that satisfies certain properties, such as closure under addition and scalar multiplication.\n   * A basis for a vector space is a set of vectors that spans the entire space (i.e., any vector in the space can be expressed as a linear combination of the basis vectors) and is linearly independent (i.e., no vector in the set can be expressed as a linear combination of the other vectors in the set).\n1. Representation of Vectors:\n   \n   * Any vector within a vector space can be represented as a linear combination of the basis vectors.\n   * For example, in a two-dimensional space, a vector v can be represented as v = a * v1 + b * v2, where v1 and v2 are the basis vectors, and a and b are scalar coefficients.\n1. Standard Basis:\n   \n   * The standard basis refers to a set of basis vectors in which each vector has a single element equal to 1 and all other elements equal to 0.\n   * In a two-dimensional space, the standard basis consists of two vectors: \\[1, 0\\] and \\[0, 1\\].\n1. Coordinate Representation:\n   \n   * The coefficients of the linear combination used to represent a vector with respect to the basis vectors are called the coordinates or components of the vector.\n   * For example, in a two-dimensional space with the standard basis, the vector \\[3, 2\\] can be represented as 3 * \\[1, 0\\] + 2 * \\[0, 1\\], and its coordinates are (3, 2).\n\nBasis vectors play a crucial role in linear algebra and various applications, including:\n\n* Coordinate systems: Basis vectors define the coordinate axes in a given vector space, enabling the representation and measurement of quantities along those axes.\n* Vector operations: Basis vectors are used to decompose vectors into their components, perform vector addition and scalar multiplication, and compute dot products and cross products.\n* Linear transformations: Basis vectors can be transformed by linear transformations, allowing the understanding and analysis of how these transformations affect vectors in the space.\n\nChoosing an appropriate set of basis vectors is essential for effectively representing vectors and solving problems in a given vector space. The choice of basis vectors can vary depending on the context and the problem at hand, and different bases can offer different advantages or insights into the underlying structure of the vectors and operations within the space.\n\n* **orthonormal basis**\n\nAn orthonormal basis is a set of vectors in a vector space that is both orthogonal and normalized. Each vector in the basis is perpendicular (orthogonal) to every other vector in the set, and each vector has a length of 1 (normalized). Orthonormal bases have several important properties and applications in linear algebra and signal processing. \n\nMore specifically:\n\n1. Orthogonality:\n   \n   * Orthogonal vectors are perpendicular to each other, meaning their dot product is zero.\n   * In an orthonormal basis, all vectors are pairwise orthogonal. This property simplifies calculations and makes it easier to express vectors as linear combinations of the basis vectors.\n1. Normalization:\n   \n   * Each vector in an orthonormal basis has a length (norm) of 1.\n   * Normalized vectors provide a consistent scale, making it easier to reason about their relative magnitudes and simplify computations.\n1. Matrix Representation:\n   \n   * Orthonormal bases play a significant role in linear algebra, especially when representing matrices as collections of column vectors.\n   * A matrix with columns representing an orthonormal basis is called an orthogonal matrix. Its inverse is equal to its transpose, and its rows are also orthonormal.\n1. Coordinate Systems:\n   \n   * Orthonormal bases define coordinate systems in vector spaces, where each coordinate axis corresponds to a basis vector.\n   * By expressing vectors in terms of their coordinates with respect to an orthonormal basis, computations and transformations become more intuitive and efficient.\n1. Signal Processing:\n   \n   * Orthonormal bases are crucial in areas such as Fourier analysis and wavelet analysis.\n   * Fourier basis functions, such as sine and cosine waves, form an orthonormal basis for representing signals in the frequency domain.\n   * Wavelet functions also provide an orthonormal basis for analyzing signals at different scales and resolutions.\n\nOrthonormal bases have numerous advantages, including simplified calculations, easier interpretation of vectors, efficient signal representation, and robustness in numerical computations. They serve as fundamental tools in various areas of mathematics, including linear algebra, signal processing, and data analysis.\n\nExamples of orthonormal bases include the standard basis in Euclidean space (where each basis vector corresponds to a unit vector along the coordinate axes) and the Fourier basis functions (where each basis vector represents a specific frequency component).\n\n* **what are conjugate directions and explain its relationship to gradient descent with line search**\n\nConjugate directions are a concept used in optimization algorithms, particularly in the context of solving unconstrained optimization problems. In the context of gradient descent with line search, conjugate directions play a crucial role in improving the efficiency of the optimization process.\n\nHere's an explanation of conjugate directions and their relationship to gradient descent with line search:\n\nSure! Here's the explanation of conjugate directions and their relationship to gradient descent with line search in LaTeX-supported markdown:\n\n1. Conjugate Directions:\n   \n   * Given a problem of minimizing a function $f(x)$, a set of vectors ${d_1, d_2, \\ldots, d_n}$ is said to be a set of conjugate directions if they satisfy the property that for any $(i \\neq j)$, the direction vectors are orthogonal with respect to a positive-definite matrix $A$. In other words, $d_i^T A d_j = 0$ for $i \\neq j$.\n   * The conjugate directions allow us to search the solution space in mutually orthogonal directions, ensuring that we don't revisit the same directions during the optimization process.\n1. Gradient Descent with Line Search:\n   \n   * Gradient descent is an optimization algorithm used to find the minimum of a function. It iteratively updates the current solution by taking steps in the direction of the negative gradient of the function.\n   * In gradient descent with line search, the step size (learning rate) is determined by performing a line search along the descent direction to find the optimal step size that minimizes the function along that direction.\n   * The descent direction in each iteration is typically chosen as the negative gradient of the function at the current solution.\n1. Relationship to Conjugate Directions:\n   \n   * Conjugate directions are particularly useful when the function being minimized is not quadratic. In such cases, using the standard gradient descent direction may result in slow convergence or oscillations.\n   * By incorporating conjugate directions into the optimization process, we can effectively search the solution space in more efficient and independent directions, potentially leading to faster convergence.\n   * Conjugate directions help avoid unnecessary backtracking or oscillations by ensuring that the search proceeds along orthogonal directions that are not redundant with previous search directions.\n   * Conjugate gradient descent is an optimization algorithm that combines the benefits of conjugate directions with gradient descent, allowing for efficient optimization in non-quadratic problems.\n\nIn summary, conjugate directions provide a way to optimize functions more efficiently by ensuring independent and orthogonal search directions during the optimization process. Incorporating conjugate directions into gradient descent with line search can help accelerate convergence and avoid unnecessary oscillations, particularly in non-quadratic optimization problems.\n","lastmodified":"2023-06-25T14:09:32.94852216+01:00","tags":[]},"/AI/NLP/Large-Language-Models":{"title":"Large Language Models","content":"\nReference: [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)\n","lastmodified":"2023-06-25T14:09:32.948237702+01:00","tags":[]},"/AI/NLP/Papers":{"title":"Papers","content":"\n[Yannic Kilcher](https://www.youtube.com/@YannicKilcher) - https://www.youtube.com/playlist?list=PL1v8zpldgH3pQwRz1FORZdChMaNZaR3pu\n","lastmodified":"2023-06-25T14:09:32.948450285+01:00","tags":[]},"/AI/NLP/Transformer-Family":{"title":"Transformer Family","content":"\nReferences: Blog: [The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/)\n","lastmodified":"2023-06-25T14:09:32.94873191+01:00","tags":[]},"/AI/Probabilistic-Modeling/UVA/Deep-Learning-Course-II/Deep-Probabilistic-Modelsm":{"title":"Deep Probabilistic Modelsm","content":"\nhttps://uvadl2c.github.io/\nhttps://uvadl2c.github.io/lectures-2022.html\n\nProbabilistic Models\n\n* A probabilistic model *prescribes* the probability measure of a random experiment!\n\nThere are many ways to prescribe a probabilistic measure:\n\n* We may specify the measure of events in the event space, one at a time. This is very tedious and sometimes impossible (eg. all the subsets of natural numbers) ![Screenshot 2023-06-13 at 2.05.20 PM.png](../../figures/Screenshot%202023-06-13%20at%202.05.20%20PM.png)\n* We may instead specify a probability mass or density function (PMF or PDF) for outcomes of a random variable 1Using probability distribution - changing parameters (PDF) ![Screenshot 2023-06-13 at 2.06.22 PM.png](../../../../Screenshot%202023-06-13%20at%202.06.22%20PM.png)\n* We may instead specify the cumulative distribution functions (CDF) of an RV, the CDF in turn identifies a PDF, the RV and its PDF identify a probabilistic measure\n  ![Screenshot 2023-06-13 at 2.10.13 PM.png](../../../../Screenshot%202023-06-13%20at%202.10.13%20PM.png)\n","lastmodified":"2023-06-25T14:09:32.949635993+01:00","tags":[]},"/AI/Probabilistic-Modeling/UVA/Deep-Learning-Course-II/UVA-Deep-Learning-II-Course":{"title":"UVA Deep Learning II Course","content":"\nhttps://uvadl2c.github.io/\n\nScore-Based Generative Modeling\nhttps://yang-song.net/blog/2021/score/\n\nNormalizing Flows:\n\n* Pytorch\n  https://github.com/francois-rozet/zuko - implementation of normalizing flows in pytorch (multiple examples implemented)\n  https://www.youtube.com/watch?v=3KUvxIOJD0k - This gives a good overview of the framework\n  https://blog.evjang.com/2018/01/nf1.html\n  https://blog.evjang.com/2018/01/nf2.html\n  https://lilianweng.github.io/posts/2018-10-13-flow-models/\n","lastmodified":"2023-06-25T14:09:32.94896916+01:00","tags":[]},"/AI/Probabilistic-Modeling/UVA/Deep-Learning-Course/UVA-Deep-Learning-Course":{"title":"UVA Deep Learning Course","content":"\nhttps://uvadlc.github.io/\n","lastmodified":"2023-06-25T14:09:32.948547076+01:00","tags":[]},"/AI/Probabilistic-Modeling/VAE":{"title":"VAE","content":"\n* https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n* https://github.com/jaanli/variational-autoencoder/tree/master\n* https://github.com/ikostrikov/pytorch-flows/tree/master\n  * https://github.com/gpapamak/maf#how-to-get-the-datasets\n  * Masked Autoregressive Flow for Density Estimation\n* https://github.com/bayesiains/nflows\n  * `nflows` is a comprehensive collection of [normalizing flows](https://arxiv.org/abs/1912.02762) using [PyTorch](https://pytorch.org).l\n","lastmodified":"2023-06-25T14:09:32.948670577+01:00","tags":[]},"/AI/Prompt-Based-Learning":{"title":"Prompt-Based Learning","content":"\n\n","lastmodified":"2023-06-25T14:09:32.948037951+01:00","tags":[]},"/AI/Reinforcement-Learning/RLHF":{"title":"RLHF","content":"\n# Reinforcement Learning from Human Feedback (RLHF)\n\nReferences: \n\n* Notes: [Reinforcement Learning from Human Feedback](https://vinija.ai/toolkit/RLHF/#refresher-basics-of-reinforcement-learning)\n* Video: [Reinforcement Learning from Human Feedback: From Zero to chatGPT](https://www.youtube.com/watch?v=2MBJOuVq380\u0026ab_channel=HuggingFace)\n* Course: [Deep Reinforcement Learning Course](https://huggingface.co/deep-rl-course/unit0/introduction)\n* Blog:  [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)\n","lastmodified":"2023-06-25T14:09:32.949668993+01:00","tags":[]},"/AI/Speech-Processing/Speech-Processing":{"title":"Speech Processing","content":"\nExcellent Notes: https://speechprocessingbook.aalto.fi/index.html\n\nLecture and Videos on Prosody\nhttps://nigelward.com/prosody/\nhttps://www.youtube.com/watch?v=QrwaRUjcOM4\u0026list=PLCFybA0SDVTjbQQRxJ1p2NnirCw_tk_z7\u0026index=1\n\nDeep Learning for Audio Processing\nhttps://github.com/lordzuko/deep-learning-for-audio-processing\n\nSpeech has two aspects:\n\n* Lexical \n* Prosody\n\nThe musical aspect of speech:\n\n* pitch\n* loudness\n* duration\n* timing\n\nThe throat-generated aspects of speech\n\n* creakiness\n* breathiness\n\nThe non-textual aspects of speech\n\n* prominence\n* pragmatics\n* phrasing\n* tone \n* stress\n\n# Prosody\n\n### Pitch and F0\n\n### Loudness, Timing and More\n","lastmodified":"2023-06-25T14:09:32.949174076+01:00","tags":[]},"/AI/Speech-Processing/Speech-Synthesis":{"title":"Speech Synthesis","content":"\n#### Generative Model-Based Text-to-Speech Synthesis\n\nhttps://www.youtube.com/watch?v=nsrSrYtKkT8\u0026list=RDLV7mjh0PSUv0M\u0026index=4\n","lastmodified":"2023-06-25T14:09:32.949310951+01:00","tags":[]},"/Interview-Preparations/DeepLearning/Notes":{"title":"Notes","content":"\n\n","lastmodified":"2023-06-25T14:09:32.947673951+01:00","tags":[]},"/Mathematics-for-ML/Maths-for-ML":{"title":"Maths for ML","content":"\nMathematics for Machine Learning — Ulrike von Luxburg, 2020/21\nhttps://www.youtube.com/playlist?list=PL05umP7R6ij1a6KdEy8PVE9zoCv6SlHRS\n\nhttp://www.tml.cs.uni-tuebingen.de/teaching/2020_maths_for_ml/index.php\n\nStatistics Foundation for Machine Learning - https://www.youtube.com/playlist?list=PLuqhl4iqeAZZMwUUdc-3pCy8LtFTGrQZL\n\nWritten Notes: Look in `written notes` directory\n\n# Linear Algebra\n\n* [ ] Vector Spaces\n* [ ] Basis and Dimension\n* [ ] Direct Sum\n* [ ] Linear Maps, Kernel, Range\n* [ ] Matrices\n* [ ] Invertible maps and matrices\n* [ ] Transpose\n* [ ] Change of basis\n* [ ] Rank of a matrix\n* [ ] Quotient Space; Equivalence Relation\n* [ ] Determinant\n* [ ] Eigenvalues\n* [ ] Characteristic Polynomial\n* [ ] Trace of a matrix\n* [ ] Diagonalization; triangular matrices\n* [ ] Metrics spaces\n* [ ] Normed Spaces; p-norms\n* [ ] Norms on $R^d$ are equivalent\n* [ ] Convex set induces a norm\n* [ ] Spaces of continuous and differentiable functions\n* [ ] Lp-spaces of integrable functions\n* [ ] Scalar product\n* [ ] Orthogonal vectors and basis\n* [ ] Orthogonal matrices\n* [ ] Symmetric Matrices\n* [ ] Spectral theorem for symmetric matrices\n* [ ] Positive definite matrices\n* [ ] Variational characterization of eigenvalues\n* [ ] Singular value decomposition\n* [ ] Rank-k-approximation, matrix norms\n* [ ] Pseudo-inverse of a matrix\n* [ ] Continuous linear functionals and operator norm\n* [ ] Dual space, Riesz representative theorem\n\n# Probability\n\n* [ ] Definition of a probability measure\n* [ ] Different types of measures: discrete, with density; Radon-Nikodym\n* [ ] Different types of measures: singular measures, Lebesgue decomposition\n* [ ] Cumulative Distribution Function (CDF)\n* [ ] Random Variables (RVs)\n* [ ] Conditional Probabilities\n* [ ] Bayes Theorem\n* [ ] Independence\n* [ ] Expectation (discrete case)\n* [ ] Variance, covariance, correlation (discrete case)\n* [ ] Expectation and covariance (general case)\n* [ ] Markov and Chebyshev inequality\n* [ ] Example distributions: binomial, poisson, multivariate normal\n* [ ] Convergence of random variables\n* [ ] Borel-Cantelli\n* [ ] Law of large numbers, Central limit theorem\n* [ ] Concentration inequalities\n  * [ ] Glivenko-Cantelli theorem\n* [ ] Product space and joint distribution\n* [ ] Marginal distribution\n* [ ] Conditional distribution\n* [ ] Conditional Expectation\n\n# Calculus\n\n* [ ] Sequences and convergence\n* [ ] Continuity\n* [ ] Sequences of functions; pointwise and uniform convergence\n* [ ] Differentiation on R\n* [ ] Riemann integram on R\n* [ ] Fundamental theorem of calculus on R\n* [ ] Power Series\n* [ ] Taylor Series\n* [ ] Sigma-Algebra\n* [ ] Measure\n* [ ] Lebesgue measure on $R^n$\n* [ ] Differentiation on $R^n$: total derivative\n* [ ] Differentiation on $R^n$: Higher order derivatives\n* [ ] Minima, maxima, saddlepoints\n* [ ] Matrix calculus\n* [ ] \n\n# Statistics\n\n* [ ] Estimation, bias, variance\n* [ ] Confidence sets\n* [ ] Maximum likelihood estimator\n* [ ] Sufficiency, identifiability\n* [ ] Hypothesis testing, level and power of a test\n* [ ] Likelihood-ration tests and Neyman-Pearson lemma\n* [ ] p-values\n* [ ] Multiple testing\n* [ ] Non-parametric tests (rank and permutation tests, Kolmogorov-Smirnov)\n* [ ] The bootstrap\n* [ ] Bayesian statistics\n\n# Others\n\n* [ ] High-dimensional spaces\n* [ ] What is a convex optimisation problem?\n* [ ] Convex optimisation, Lagrangian, dual problem\n","lastmodified":"2023-06-25T14:09:32.948289743+01:00","tags":[]},"/Mathematics-for-ML/Numerics-of-Machine-Learning-2023":{"title":"Numerics of Machine Learning - 2023","content":"\nThis class discusses both practical and theoretically interesting aspects of the algorithms that drive machine learning. It endorses the view of Probabilistic Numerics, the idea that computation itself is an inference process, to describe the numerical algorithms *inside* the learning machine, *as* learning machines.\n\n* Slides: https://github.com/philipphennig/NumericsOfML/\n\n* Lectures: https://www.youtube.com/playlist?list=PL05umP7R6ij2lwDdj7IkuHoP9vHlEcH0s\n\n* [ ] Lecture 1: Introduction\n\n* [ ] Lecture 2: Numerical Linear Algebra\n\n* [ ] Lecture 3: Scaling Gaussian Processes\n\n* [ ] Lecture 4: Computational-Aware Gaussian Processes\n\n* [ ] Lecture 5: State-space Models\n\n* [ ] Lecture 6: Solving Ordinary Differential Equations (ODE)\n\n* [ ] Lecture 7: Probabilistic Numerical ODE Solvers\n\n* [ ] Lecture 8: Partial Differential Equations\n\n* [ ] Lecture 9: Monte Carlo\n\n* [ ] Lecture 10: Bayesian Quadrature\n\n* [ ] Lecture 11: Optimization for Deep Learning\n\n* [ ] Lecture 12: Second-Order Optimization for Deep Learning\n\n* [ ] Lecture 13: Uncertainty in Deep Learning\n\n* [ ] Lecture 14: Conclusion\n","lastmodified":"2023-06-25T14:09:32.947135827+01:00","tags":[]},"/Mathematics-for-ML/Probabilistic-Machine-Learning":{"title":"Probabilistic Machine Learning","content":"\n* Philipp Hennig, 2023\n* https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx\n* CSC412 Winter 2020: Probabilsitic Machine Learning : https://probmlcourse.github.io/csc412/\n\n* [ ] Lecture 1 - Introduction\n* [ ] Lecture 2 - Reasoning under uncertainty\n* [ ] Lecture 3 - Continuous Variables\n* [ ] Lecture 4 - Exponential Families\n* [ ] Lecture 5 - Exponential Families II \n* [ ] Lecture 6 - Gaussian Probability Distributions\n* [ ] Lecture 7 - Parametric Regression\n* [ ] Lecture 8 - Gaussian Processes\n* [ ] Lecture 9 - Understanding Gaussian Processes\n* [ ] Lecture 10 - GP Regression: An extensive example\n","lastmodified":"2023-06-25T14:09:32.94739841+01:00","tags":[]},"/Mathematics-for-ML/Quick-ML-for-DL":{"title":"Quick ML for DL","content":"\nhttps://www.youtube.com/playlist?list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm\n\n* [ ] MaDL Introduction\n* [ ] Sets, Scalars, Vectors, Matrices and Tensors\n* [ ] Adding and Multiplying Matrices and Vectors\n* [ ] Identity and Inverse Matrices\n* [ ] Linear Dependence and Span\n* [ ] Vector and Matrix Norms\n* [ ] Special Matrices and Vectors\n* [ ] Eigenvalue and Singular Value Decomposition\n* [ ] The trace operator and determinant\n* [ ] Differential Calculus\n* [ ] Vector Calculus\n* [ ] Random Variables and Probability Distributions\n* [ ] Common Probability Distributions\n* [ ] Bayesian Decision Theory\n* [ ] Expectation, Variance and Covariance\n* [ ] Information and Entropy\n* [ ] Kullback-Leibler Divergence\n* [ ] The Argmin and Argmax Operators\n","lastmodified":"2023-06-25T14:09:32.947208035+01:00","tags":[]},"/Obsedian-Videos":{"title":"Obsedian Videos","content":"\n* https://www.youtube.com/watch?v=bBk2kg8Gm_U\u0026ab_channel=LinkingYourThinking\n\n## Obsedian Tasks\n\nLearn this after exam\nhttps://publish.obsidian.md/tasks/Introduction\n","lastmodified":"2023-06-25T14:09:32.948194243+01:00","tags":[]},"/Papers/Speech-Synthesis/Categoris-of-Models":{"title":"Categoris of Models","content":"\n* *Autoregressive*\n  * *WaveNet*\n  * *WaveRNN*\n* *GAN*\n  * *MelGAN*\n  * *Parallel WaveGAN*\n* *Diffusion*\n  * *WaveGrad*\n  * *DiffWave*\n","lastmodified":"2023-06-25T14:09:32.952260576+01:00","tags":[]},"/Papers/Speech-Synthesis/Datasets":{"title":"Datasets","content":"\n* *LJ Speech*\n* *LibriTTS*\n* *VCTK*\n","lastmodified":"2023-06-25T14:09:32.947817368+01:00","tags":[]},"/Papers/Speech-Synthesis/Diffusion/Grad-TTS":{"title":"Grad-TTS","content":"\n* Atli's Notes: https://docs.google.com/presentation/d/1-SVmRGUJw7dA3zs7duvpFq_JthljMtF0LbUMyRMSZCQ/edit#slide=id.p\n","lastmodified":"2023-06-25T14:09:32.948119285+01:00","tags":[]},"/Papers/Speech-Synthesis/Metrics":{"title":"Metrics","content":"\n* *Mean Opinion Score* (MOS)\n  \n  * Objective evaluation metric for quality measurement\n* *Frechet Audio Distance* (FAD)\n  \n  * Objective evaluation metric for quality measurement\n  * FAD score is the distance between two multivariate Gaussian distributins (i.e. the background and evaluation embeddings)\n* *Structural Similarity Index* (SSIM)\n\n* *Log-mel Spectrogram Mean Squared Error* (LS-MSE)\n\n* *Peak Signal-to-Noise Ration* (PSNR)\n\n* SSIM, LS-MSE, PSNR are quantitative evaluation metrics for similarity and noise measurement\n* All the computations in each metric are done in the frequency domain to compare the synthesized Mel-Spectrogram with ground truth\n\nReference: \n\n* [Vocbench: A Neural Vocoder Benchmark for Speech Synthesis](https://arxiv.org/pdf/2112.03099.pdf)\n","lastmodified":"2023-06-25T14:09:32.952801367+01:00","tags":[]},"/README":{"title":"README","content":"\n# lordzuko_sb\n\nBuilding my second brain\n\nThis is my space for writing everything that I read and synthesize. I expect this to serve as a second brain for me.\n\n## Tools\n\nI use Obsedian to write notes and Notability to annotate the PDFs.\n","lastmodified":"2023-06-25T14:09:32.947103077+01:00","tags":[]},"/Thesis/Abstract":{"title":"Abstract","content":"\n**Topic:** **[Aligning latent space of speaking style with human perception using a re-embedding strategy \\[sigurgeirsson:speaking-style\\]](https://www.wiki.ed.ac.uk/pages/viewpage.action?spaceKey=CSTR\u0026title=SLP+project+list+2022-23)**\n\nTo generate appropriate speech in context, we must not only consider what is said but also how it is said. Prosody is used to influence a particular meaning, to indicate attitude and emotion, and more. Modelling prosody in Text-To-Speech (TTS) is a highly complex task, as prosody manifests itself in multiple hierarchies in speech, from phone-level acoustic features to suprasegmental effects spanning multiple sentences. As perception of certain prosodic features is highly subjective and prosodically labelled speech corpora are scarce, researchers have looked to unsupervised methods of modelling prosody in TTS.\n\nMany propose to jointly train a reference encoder to model a latent representation of prosody or speaking style \\[e.g., 1, 2, 3\\]. The reference encoder is conditioned on the target speech during training and learns to model features that are perceptually important for the acoustic model in reconstructing the target speech \\[1, 2\\]. Many recent unsupervised style-modelling methods have focused on disentangling non-prosodic features from the latent prosodic representations \\[e.g., 3, 4, 8\\], but there is little known about how well this latent space aligns with human perception of style. Understanding this space could help solve the issue of feature-entanglement without any modifications to the model.\n\nHere, we propose to manipulate the latent prosody space using light-supervision from human annotators. Such strategies have already been proposed for speaker-modelling \\[5, 6\\] and speaker-adaptation \\[9\\] but not for speaking-style modelling. To achieve this we suggest to re-embed synthesised utterances that human annotators have tuned acoustically to match a target speaking style. After re-embedding the annotated utterances we aim to tune a fixed bank of style tokens \\[2\\]. This could not only further understanding the latent prosody space but also align it with human perception improving the quality of style generation.\n\n\\[1\\] [Towards end-to-end prosody transfer for expressive speech synthesis with tacotron](http://proceedings.mlr.press/v80/skerry-ryan18a.html)\n\n\\[2\\] [Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis](http://proceedings.mlr.press/v80/wang18h.html?ref=https://githubhelp.com)\n\n\\[3\\] [Daft-exprt: Robust prosody transfer across speakers for expressive speech synthesis](https://arxiv.org/abs/2108.02271)\n\n\\[4\\] [Copycat: Many-to-many fine-grained prosody transfer for neural text-to-speech](https://arxiv.org/abs/2004.14617)\n\n\\[5\\] [Perceptual-similarity-aware deep speaker representation learning for multi-speaker generative modeling](https://ieeexplore.ieee.org/iel7/6570655/9289074/09354556.pdf)\n\n\\[6\\] [DNN-based Speaker Embedding Using Subjective Inter-speaker Similarity for Multi-speaker Modeling in Speech Synthesis](https://arxiv.org/pdf/1907.08294)\n\n\\[7\\] [Fastspeech: Fast, robust and controllable text to speech](https://proceedings.neurips.cc/paper/2019/hash/f63f65b503e22cb970527f23c9ad7db1-Abstract.html)\n\n\\[8\\] [Effective use of variational embedding capacity in expressive end-to-end speech synthesis](https://arxiv.org/abs/1906.03402)\n\n\\[9\\] [Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS](https://arxiv.org/abs/2206.10256)\n","lastmodified":"2023-06-25T14:09:32.949961659+01:00","tags":[]},"/Thesis/Background":{"title":"Background","content":"\n## Architectures\n\n* Tacotron\n* Tacotron2\n* FastPitch\n* FastSpeech2\n\n## Prosody\n\n* Prosody Control: Ctrl+P\n* \\[8\\] [Effective use of variational embedding capacity in expressive end-to-end speech synthesis](https://arxiv.org/abs/1906.03402)\n\n## Style Tokens\n\n* Globle Style Tokens:\n  * Works on \n\n## Speaking Style\n\nWhat is speaking style? How is this different from Prosody? What is done for Speaking Style so far? What doesn't seem to fit the equation? (Challenges) What we think is a good/expected outcome? (expectation) What can be done to reach there?\n\nWe are modeling speaking style to improve perception of a limited set of speaking styles\n","lastmodified":"2023-06-25T14:09:32.949453451+01:00","tags":[]},"/Thesis/Dataset":{"title":"Dataset","content":"\nWell maintained lihttps://github.com/coqui-ai/open-speech-corpora\n\nPotential Source\nhttps://speechresearch.github.io/prompttts/#dataset\n\n* 5 different style factors (gender, pitch, speaking speed, volume, and emotion) from a commercial TTS [API](https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/#overview)\n* The emotion factor has 5 categories\n* Thee gender factor has 2 categories\n* rest of style factors including **pitch**, **speaking** **speed**, and **volume**, we extract the value of style factors from speech with signal processing tools\n  * divided into 3 categories (high/normal/low)\n* we ask experts to write style prompts for each category\n  * further augment the style prompts, we utilize SimBERT to generate more style prompts with similar semantics.\n\nOur definition of Speaking Style\n**speaking style as any perceptually distinct manner of speaking that is context-appropriate.**\n\nnarration (book reading), novel/anime characters, neutral discourse, news reader, customer-service style, poetry style\n\n**News Reading**\n\n1. The \"**1996 English Broadcast News Speech (HUB4)**\" dataset contains a total of 104 hours of broadcasts from ABC, CNN, and CSPAN television networks and NPR and PRI radio networks with corresponding transcripts. The dataset is divided into a training set, development data, and evaluation data. Transcripts have been made of all recordings in this publication, manually time aligned to the phrasal level, annotated to identify boundaries between news stories, speaker turn boundaries, and gender information about the speakers​[1](https://catalog.ldc.upenn.edu/LDC97S44)\n1. The \"**TDT4 Multilingual Broadcast News Speech Corpus**\" was developed by the Linguistic Data Consortium (LDC) with support from the DARPA TIDES Program. This release contains the complete set of American English, Modern Standard Arabic, and Mandarin Chinese broadcast news audio used in the 2002 and 2003 Topic Detection and Tracking (TDT) technology evaluations, totaling approximately 607 hours. The TDT4 corpus contains news data collected daily from 20 news sources in three languages (American English, Mandarin Chinese, and Modern Standard Arabic), over a period of four months (October 2000 through January 2001)​[2](https://catalog.ldc.upenn.edu/LDC2005S11)​.​\n\n* **BBC Programs** (Not News)\n  **Lip Reading Sentences 2 (LRS2)**: This is a dataset for lip reading sentences. It's one of the largest publicly available datasets for lip reading sentences in-the-wild. The database consists mainly of news and talk shows from BBC programs. Each sentence is up to 100 characters in length. It contains thousands of speakers without speaker labels and large variation in head pose. The pre-training set contains 96,318 utterances, the training set contains 45,839 utterances, the validation set contains 1,082 utterances, and the test set contains 1,242 utterances​[1](https://paperswithcode.com/datasets?task=speech-recognition)​.\n  \n  * https://www.robots.ox.ac.uk/~vgg/data/lip_reading/\n* Blizzard challenge: \n  \n  * The books are read by  the 2013 Blizzard Challenge speaker, Catherine Byers, in an  \n    animated and emotive storytelling style. Some books contain  very expressive character voices with high dynamic range, which are challenging to model \n  * https://arxiv.org/pdf/1808.01410.pdf - PREDICTING EXPRESSIVE SPEAKING STYLE FROM TEXT IN END-TO-END SPEECH SYNTHESIS\n\n1. **LibriSpeech**: This is a standard large-scale dataset for evaluating ASR (Automatic Speech Recognition) systems. It consists of approximately 1,000 hours of narrated audiobooks collected from the LibriVox project. The speaking style is described as \"narrated\"​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **Common Voice**: This is a crowd-sourced open-licensed speech dataset where speakers record text from Wikipedia in various languages. The speaking style is \"narrated\" and the dataset contains significant variation in both audio quality and speakers. The English subset of version 11.0 contains approximately 2,300 hours of validated data​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **VoxPopuli**: This is a large-scale multilingual speech corpus consisting of data sourced from 2009-2020 European Parliament event recordings. The speaking style is described as \"oratory, political speech\" and the dataset largely consists of non-native speakers. The English subset contains approximately 550 hours of labeled speech​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **TED-LIUM**: This dataset is based on English-language TED Talk conference videos. The speaking style is \"oratory educational talks\" covering a range of different cultural, political, and academic topics. The Release 3 (latest) edition of the dataset contains approximately 450 hours of training data​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **GigaSpeech**: This is a multi-domain English speech recognition corpus curated from audiobooks, podcasts, and YouTube. It covers both \"narrated and spontaneous speech\" over a variety of topics. The dataset contains training splits varying from 10 hours - 10,000 hours​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **SPGISpeech**: This is an English speech recognition corpus composed of company earnings calls that have been manually transcribed. The speaking style is described as \"oratory and spontaneous speech\" and it contains training splits ranging from 200 hours - 5,000 hours​[1](https://huggingface.co/blog/audio-datasets)​.\n\n1. **Earnings-22**: This is a 119-hour corpus of English-language earnings calls collected from global companies. The dataset does not explicitly mention the speaking style, but it is described as covering a range of real-world financial topics​[1](https://huggingface.co/blog/audio-datasets)​.\n","lastmodified":"2023-06-25T14:09:32.949692118+01:00","tags":[]},"/Untitled":{"title":"Untitled","content":"\nCertainly! Let's delve into more details behind step 5, which is the maximum likelihood estimation (MLE) in MLLR:\n\nIn MLLR, the goal is to estimate the transformation parameters A and B that maximize the likelihood of the adaptation data given the adapted model. The likelihood is typically defined as the product of the probabilities of each individual feature vector in the adaptation data.\n\nLet's denote the adapted model as $θ^\\tilde = (A, B)$. The likelihood function can be expressed as:\n\n$L(D | \\theta) = P(D | A, B)$\n\nTo estimate the parameters A and B, we maximize the likelihood function. However, maximizing the likelihood directly can be challenging due to the presence of the transformation in the model.\n\nTo simplify the optimization, a common approach is to take the logarithm of the likelihood function, which converts the product into a sum:\n\n$l(D | \\theta) = \\log L(D | \\theta) = \\sum\\_{i=1}^{N} \\log P(x_i | A, B)$\n\nNow, maximizing the logarithm of the likelihood is equivalent to maximizing the likelihood itself, as the logarithm is a monotonically increasing function.\n\nTo estimate the parameters A and B, we typically use an iterative optimization algorithm, such as the expectation-maximization (EM) algorithm or gradient-based optimization methods. The specific optimization algorithm employed may vary depending on the implementation and requirements of the system.\n\nDuring the optimization process, we adjust the values of A and B to find the maximum of the logarithm of the likelihood function. This involves computing the gradient of the objective function with respect to A and B and updating their values iteratively.\n\nThe optimization process continues until convergence, where the estimated transformation parameters A and B reach a point where the likelihood is maximized.\n\nIt's worth noting that there can be additional constraints or regularization terms incorporated into the optimization process to prevent overfitting or to control the magnitude of the transformation parameters.\n\nOnce the parameters A and B are estimated, they are applied to the mean vectors of the acoustic features in both the adaptation data and the initial acoustic model to transform them accordingly, as described in step 6.\n\nI hope this provides more insight into the maximum likelihood estimation step in MLLR. If you have any further questions, please let me know.\n","lastmodified":"2023-06-25T14:09:32.947370452+01:00","tags":[]},"/UoE/CSAI/Summary":{"title":"Summary","content":"\n\n","lastmodified":"2023-06-25T14:09:32.952758034+01:00","tags":[]},"/UoE/MLPR/Probabilistic-Models-in-TTS":{"title":"Probabilistic Models in TTS","content":"\nIn this note I am trying to understand what does `Variational` means in models like `Variational Autoencoders` , `Variational Inference` etc. \n\nThis is an interesting setting and after going through everything which is discussed below, it doesn't sounds that complicated. Often the papers, makes things too convoluted, but there are only a few things to keep in mind. There are a few steps, which are taken to make the model probabilistic, which will be made clear as you go through the explanations one by one and do look at the code and things won't seem that scary anymore.\n\nAlso, I was curious about how probabilistic modeling works for Neural networks, they talked some of that in MLPR, but they made it sound all too complicated. However, looking at the code it doesn't seem that complicated. All we do is draw weights and biases in various layers in the neural networks from a gausssian or some other distribution. Now, instead of giving a point wise prediction, we get a number of predictions for every point, which can be used to know uncertainty in predictions. Generally, non-bayesian neural network gives the mean-prediction, but now we have a distribution over prediction, where the variance term tells about the certainty or confidence of prediction. Frameworks like !(Pyro)\\[https://num.pyro.ai/en/stable/getting_started.html\\] makes it easier to implement various probabilistic models, including Bayesian Neural Netwoks. We will see an example towards the end of the note.\n\nAlright; start digging.\n\n### what is the difference between autoencoder and variational autoencoder\n\nAutoencoders and Variational Autoencoders (VAEs) are both types of neural network architectures used for unsupervised learning tasks, such as ***dimensionality reduction, representation learning, and generative modeling**.* While they share similarities in their structure, they have different goals and properties.\n\n1. **Structure:**\n   \n   * Autoencoder: Consists of an encoder and a decoder. The encoder maps the input data to a lower-dimensional latent space, and the decoder reconstructs the input data from the latent representation. The goal is to *minimize the reconstruction error*.\n   * Variational Autoencoder: Also consists of an encoder and a decoder but introduces a probabilistic layer in the latent space. *The encoder learns to map the input data to a probability distribution (usually Gaussian) in the latent space, and the decoder reconstructs the input data from samples drawn from that distribution.*\n1. **Objective:**\n   \n   * Autoencoder: Minimizes the reconstruction error, which measures the difference between the input data and the data reconstructed by the autoencoder. This objective encourages the autoencoder to *learn a compact representation of the data in the latent space*.\n   * Variational Autoencoder: Maximizes the Evidence Lower Bound (ELBO), which is a lower bound on the log likelihood of the observed data. The ELBO consists of the reconstruction term (similar to the autoencoder objective) and the Kullback-Leibler (KL) divergence term, which regularizes the latent space by measuring the difference between the learned distribution and a prior distribution (usually a standard normal distribution).\n1. **Properties:**\n   \n   * Autoencoder: Learns deterministic mappings between the input data and the latent space. The *latent space may not be well-structured, which can make it challenging to generate new data or interpolate between data points*.\n   * Variational Autoencoder: Learns a probabilistic mapping between the input data and the latent space. *The VAE encourages a more structured latent space by regularizing it with the KL divergence term, making it better suited for generative tasks, such as sampling new data points or interpolating between existing data points*.\n\nIn summary, *autoencoders focus on learning a compact and deterministic representation* of the data, while *Variational Autoencoders learn a probabilistic and structured representation that is more suitable for generative tasks*.\n\n### explain the various probabilistic terminologies used in VAEs\n\nIn Variational Autoencoders (VAEs), several probabilistic terminologies are used to describe different aspects of the model and its training process. Here is an explanation of some key probabilistic terms:\n\n1. **Latent variable:** A latent variable, often denoted as z, is an unobserved or hidden variable that we assume influences the observed data. In VAEs, the latent variable captures the underlying structure of the data in a lower-dimensional space.\n\n1. **Prior distribution:** The prior distribution, typically denoted as p(z), represents our initial belief about the latent variables before observing any data. In VAEs, we usually assume a standard normal distribution (mean 0 and standard deviation 1) as the prior, which helps in regularizing the latent space and making it more structured.\n\n1. **Likelihood:** The likelihood, denoted as p(x|z), describes the probability of observing the data x given the latent variable z. In VAEs, the likelihood is modeled by the decoder network, which generates the data from the latent space.\n\n1. **Posterior distribution:** The posterior distribution, denoted as p(z|x), represents the updated belief about the latent variables after observing the data x. **It describes the probability distribution of the latent variables conditioned on the observed data.** In VAEs, the true posterior is often intractable, so we approximate it using the variational distribution.\n\n1. **Variational distribution:** The variational distribution, denoted as q(z|x), is an approximation of the true posterior distribution. In VAEs, it is modeled by the encoder network, which maps the input data to the latent space. The variational distribution is usually chosen to be a Gaussian distribution with **learnable mean** (μ) and **standard deviation** (σ) parameters.\n   \n   1. This is often called **guide function** in framework like Pyro\n1. **Evidence Lower Bound (ELBO):** The ELBO is an objective function used in VAEs to maximize a lower bound on the log likelihood of the observed data. It consists of two terms: **the reconstruction term**, which encourages the VAE to reconstruct the input data accurately, and **the Kullback-Leibler (KL) divergence term**, which measures the difference between the variational distribution q(z|x) and the prior distribution p(z). The ELBO is given by:\n   \n   ELBO = E_q(z|x)\\[log p(x|z)\\] - KL(q(z|x) || p(z))\n\nMaximizing the ELBO is equivalent to minimizing the difference between the true posterior and the variational distribution, leading to a better approximation of the true posterior.\n\nThese probabilistic terminologies form the foundation of the VAE framework, helping to describe the relationships between the observed data, latent variables, and the model's learning objectives.\n\n#### explain variational methods in TTS in simple terms\n\nVariational methods in Text-to-Speech (TTS) refer to a group of techniques that help convert written text into spoken words. In simple terms, these methods help computers generate speech that sounds natural and realistic.\n\nThe process involves breaking down the task into smaller steps, and the variational methods help the computer learn how to handle each step. One crucial part of TTS is learning how to pronounce words correctly and varying the speech to make it sound more human-like.\n\nVariational methods help by introducing a bit of randomness or flexibility in the way the computer generates speech. This is done by creating a set of possible ways to pronounce words and then selecting the most appropriate option based on the context.\n\nThese methods help improve the quality of the generated speech by making it sound more natural and less robotic. They are particularly useful for creating voice assistants, audiobooks, and other applications where it is essential to have high-quality, human-like speech.\n\nVariational Autoencoders are a type of neural network used to learn the underlying structure of data. In TTS, they help to model the variability in speech, such as prosody, which includes rhythm, stress, and intonation patterns.\n\nMathematically, VAEs consist of two main components: an encoder and a decoder. The encoder takes the input data (e.g., phoneme sequences) and maps it to a lower-dimensional latent space, usually represented by a Gaussian distribution with a mean (μ) and a standard deviation (σ). The decoder then takes a sample from this distribution and generates an output (e.g., speech waveform).\n\nThe math comes in when we define the loss function, which measures how well the VAE is performing. The loss function consists of two parts:\n\n1. Reconstruction loss: This measures the difference between the original input data and the data reconstructed by the VAE. It encourages the model to generate speech that closely matches the original. Mathematically, it can be defined as the mean squared error (MSE) or the negative log-likelihood (NLL) between the input and output.\n\n1. KL divergence: This measures the difference between the distribution of the latent space and a standard Gaussian distribution (with a mean of 0 and a standard deviation of 1). It encourages the model to keep the latent space structured and prevent overfitting. Mathematically, the KL divergence is calculated as:\n   \n   KL(P||Q) = ∫ P(x) * log(P(x) / Q(x)) dx\n\nIn the context of VAEs, P represents the encoder's output distribution (with mean μ and standard deviation σ), and Q is the standard Gaussian distribution. The KL divergence term in the loss function helps the latent space to have a more regular structure, which makes it easier to sample and generate new data.\n\nTo train the VAE, we minimize the combined loss (reconstruction loss + KL divergence) using optimization algorithms like stochastic gradient descent (SGD) or Adam. This process adjusts the weights of the neural network to better generate natural-sounding speech.\n\nSure! Here's a simplified example of how to implement a Variational Autoencoder (VAE) for TTS using Python and the deep learning library PyTorch. Remember that a real-world implementation would involve more complexity and training data.\n\nFirst, let's import the necessary libraries:\n\n````\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n# Next, let's define the VAE architecture, including the encoder, decoder, and  the reparameterization trick:\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(VAE, self).__init__()\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n\t\t# The mean and variance are both learnable parameters\n        self.mu = nn.Linear(hidden_dim, latent_dim)\n        self.log_var = nn.Linear(hidden_dim, latent_dim)\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n        )\n    \n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n    \n    def forward(self, x):\n        # Encode\n        h = self.encoder(x)\n        mu, log_var = self.mu(h), self.log_var(h)\n        \n        # Reparameterize\n        z = self.reparameterize(mu, log_var)\n        \n        # Decode\n        x_recon = self.decoder(z)\n        return x_recon, mu, log_var\n\n# Now, let's create a dataset loader for our data. In a real-world TTS scenario, you would preprocess and load the input features (e.g., mel-spectrograms) and target features (e.g., speech waveforms). Here, we'll use random data for demonstration purposes:\n\nclass DummyDataset(Dataset):\n    def __init__(self, size, input_dim):\n        self.data = torch.randn(size, input_dim)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\ninput_dim = 128\ndata_size = 1000\ndataset = DummyDataset(data_size, input_dim)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n\n# We'll also need a function to calculate the VAE loss, which includes the reconstruction loss and the KL divergence:\n\ndef vae_loss(x, x_recon, mu, log_var):\n    recon_loss = nn.MSELoss()(x_recon, x)\n    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    return recon_loss + kl_div\n\n\nvae = VAE(input_dim, hidden_dim=256, latent_dim=64) optimizer = optim.Adam(vae.parameters(), lr=0.001)\n\nepochs = 50\nfor epoch in range(epochs):\n    for batch in dataloader:\n        x = batch\n        optimizer.zero_grad()\n        \n        x_recon, mu, log_var = vae(x)\n        loss = vae_loss(x, x_recon, mu, log_var)\n        \n        loss.backward()\n        optimizer.step()\n        \n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}\")\n\n````\n\nThis code defines a training loop that runs for 50 epochs. For each epoch, we process the batches of data, perform forward propagation through the VAE, and calculate the loss. We then perform backpropagation to update the model parameters using the optimizer.\n\nAfter the model is trained, you can use the decoder to generate speech features (e.g., mel-spectrograms) from the latent space. In a real-world TTS system, you would then convert these features into a speech waveform using a vocoder.\n\nKeep in mind that this example is greatly simplified and uses dummy data. A real-world TTS implementation would involve more complex architectures, preprocessing, and larger datasets to achieve high-quality speech synthesis.\n\n### what is the reparameterization trick metioned earlier?\n\nThe reparameterization trick is a technique used in Variational Autoencoders (VAEs) to enable backpropagation through the stochastic sampling process in the latent space. It allows the VAE to learn a more structured latent representation of the data while maintaining the ability to generate diverse outputs.\n\nIn VAEs, the encoder learns to represent the input data as a probability distribution in the latent space. During training, we sample a point from this distribution and use it as input for the decoder to generate the output. However, the sampling operation is non-differentiable, meaning that we cannot directly apply backpropagation through it.\n\nThe reparameterization trick addresses this issue by separating the random component from the sampling operation. Instead of directly sampling from the distribution with mean (μ) and standard deviation (σ), we sample from a standard normal distribution (with mean 0 and standard deviation 1) and scale the sampled value by σ and shift it by μ.\n\nMathematically, the reparameterization trick can be expressed as:\n\nz = μ + ε * σ\n\nwhere z is the sampled point in the latent space, μ and σ are the mean and standard deviation learned by the encoder, and ε is a random sample from the standard normal distribution.\n\nBy using this trick, we transform the non-differentiable sampling operation into a differentiable one. The randomness is now isolated in the ε term, while the rest of the equation is differentiable with respect to μ and σ. This allows us to apply backpropagation through the entire VAE architecture, making it possible to train the model using gradient-based optimization methods.\n\n### what is sampling operation non-differenitable?\n\nIn the context of Variational Autoencoders (VAEs) and other deep learning models that involve random sampling, the sampling operation is considered non-differentiable because it introduces a discrete or stochastic step in the computation, which breaks the chain of continuous and smooth operations required for gradient-based optimization methods like backpropagation.\n\nTo train deep learning models, we typically use gradient-based optimization algorithms that rely on computing gradients (partial derivatives) of the loss function with respect to the model's parameters. These gradients indicate how much each parameter should be adjusted to minimize the loss.\n\nDifferentiability is a crucial property required for calculating these gradients. A function is differentiable at a point if it has a derivative at that point, which means that it is smooth and has a well-defined tangent. Non-differentiable functions, on the other hand, have discontinuities, sharp turns, or points with undefined tangents, making it impossible to compute the gradients.\n\nThe sampling operation in VAEs is non-differentiable because it involves drawing random samples from a probability distribution. This random step introduces a discontinuity in the computation graph, and as a result, the gradients cannot be computed directly through the sampling operation.\n\nThe reparameterization trick, as explained earlier, helps overcome this issue by isolating the random component and making the rest of the computation differentiable. This allows us to apply gradient-based optimization methods, such as backpropagation, to train the VAE.\n\n### what is the difference between variational methods and diffusion methods?\n\nVariational methods and diffusion methods are two different approaches used in machine learning for different purposes. Here is a brief comparison of the two:\n\nVariational Methods:\n\n1. Purpose: Variational methods are primarily used for approximate inference in probabilistic models, such as Bayesian networks or graphical models. They can also be employed in unsupervised learning tasks, such as representation learning and generative modeling, as seen in Variational Autoencoders (VAEs).\n\n1. Approach: Variational methods work by approximating an intractable probability distribution (e.g., the true posterior) with a simpler distribution that has tractable properties. They optimize the parameters of the simpler distribution to minimize the difference (usually measured by the Kullback-Leibler divergence) between the approximated and target distributions.\n\n1. Applications: Variational methods are widely used in Bayesian inference, unsupervised learning, and generative modeling. Examples include Variational Autoencoders (VAEs), Variational Bayesian methods, and mean-field approximations.\n\nDiffusion Methods:\n\n1. Purpose: Diffusion methods are a class of stochastic processes that model the random movement of particles or information through a medium. In machine learning, diffusion methods can be employed for optimization, sampling, and data generation, as seen in the recently proposed diffusion probabilistic models.\n\n1. Approach: Diffusion methods rely on stochastic processes, such as random walks, Brownian motion, or diffusion processes. In the context of machine learning, diffusion processes can be utilized to design algorithms that involve iterative noisy updates or transitions between different states. One such example is the denoising score matching framework, where a diffusion process is used to model the generation of data from a simple noise distribution.\n\n1. Applications: Diffusion methods have applications in optimization, sampling, data generation, and deep learning. Examples include stochastic gradient descent (SGD) optimization, Markov Chain Monte Carlo (MCMC) sampling, and diffusion probabilistic models for data generation.\n\nIn summary, variational methods are primarily concerned with approximate inference and learning in probabilistic models, while diffusion methods involve stochastic processes for optimization, sampling, and data generation tasks. Both approaches have unique strengths and applications in machine learning.\n\n### what is the difference in the variational models and diffusion based models in TTS?\n\nVariational models and diffusion-based models are two different approaches used in the context of Text-to-Speech (TTS) systems. While both methods have been used to develop TTS systems, they have different principles and mechanisms.\n\nVariational Models in TTS:\n\n1. Principle: Variational models in TTS often employ Variational Autoencoders (VAEs) as the core architecture. VAEs are generative models that learn a probabilistic mapping between input text and speech features (e.g., mel-spectrograms) by optimizing an objective that balances reconstruction accuracy and latent space regularization.\n\n1. Mechanism: VAEs in TTS consist of an encoder that maps input text to a probability distribution in the latent space and a decoder that generates speech features from samples drawn from the latent distribution. The reparameterization trick enables backpropagation through the stochastic sampling process, allowing the VAE to learn a more structured latent representation of the data.\n\n1. Applications: Variational models have been used in TTS systems to generate more diverse and expressive speech, control prosody, and enable better disentanglement of speaker identity and linguistic content. Examples include Tacotron-VAE and Global Style Tokens (GST).\n\nDiffusion-Based Models in TTS:\n\n1. Principle: Diffusion-based models in TTS utilize diffusion processes or stochastic transitions to model the generation of speech features from simpler distributions. These models often employ denoising score matching frameworks or diffusion probabilistic models to learn the generation process.\n\n1. Mechanism: Diffusion-based TTS models consist of a series of noisy transitions that progressively transform a simple noise distribution into the target speech features. The model learns a score function that captures the local structure of the data distribution and guides the diffusion process. The generation process involves reversing the diffusion process, producing speech features from noise samples.\n\n1. Applications: Diffusion-based models have been used in TTS systems to generate high-quality speech and model complex dependencies between text and speech features. Examples include WaveGrad and Text-to-Diffusion.\n\nIn summary, variational models in TTS rely on Variational Autoencoders to learn a probabilistic mapping between input text and speech features, while diffusion-based models utilize diffusion processes or stochastic transitions to model the generation of speech features from simpler distributions. Both approaches have been successful in generating high-quality and expressive speech in TTS systems.\n\n## Probabilistic graphical modeling view of neural networks?\n\nProbabilistic graphical models (PGMs) are a powerful framework for modeling complex systems by representing joint probability distributions over multiple random variables. They use graphs to describe the conditional dependencies between variables, allowing for efficient inference and learning. Neural networks, on the other hand, are a class of machine learning models that consist of interconnected nodes or neurons that transform input data into desired outputs.\n\nAlthough neural networks are not inherently probabilistic models, they can be viewed from a probabilistic graphical modeling perspective. In this view, the neural network's nodes represent random variables, and the connections between nodes represent conditional dependencies between these variables. The weights and biases in the network can be seen as parameters governing these dependencies.\n\nTo build a probabilistic graphical model of a neural network, we can follow these steps:\n\n1. Define random variables: Each node in the neural network corresponds to a random variable. For example, in a feedforward neural network, each neuron in the input, hidden, and output layers can be considered a random variable.\n\n1. Define conditional dependencies: The connections between nodes in the neural network represent the conditional dependencies between the corresponding random variables. For example, the output of a neuron depends on the weighted sum of its inputs, passed through an activation function.\n\n1. Define probability distributions: Assign probability distributions to the random variables in the graphical model. In a Bayesian neural network, the weights and biases can be assigned prior distributions (e.g., Gaussian distributions), and the output random variables can be assigned likelihood distributions (e.g., Gaussian for regression or categorical for classification).\n\n1. Perform inference and learning: Once the probabilistic graphical model is constructed, we can perform inference and learning tasks using PGM techniques, such as maximum likelihood estimation (MLE), maximum a posteriori (MAP) estimation, or variational inference.\n\nThis probabilistic view of neural networks allows us to treat them as generative models, where the goal is to model the joint probability distribution of the input data and the outputs (or targets). It enables us to incorporate uncertainty into the model's predictions and learn more robust representations. Bayesian neural networks, for instance, use this perspective to model uncertainty in the weights and biases, leading to more robust and interpretable predictions.\n\nIn summary, although neural networks are not explicitly designed as probabilistic models, they can be viewed and analyzed from a probabilistic graphical modeling perspective, offering insights into their behavior and enabling the incorporation of uncertainty and robustness into their predictions.\n\n### is probabilistic graphical models a pre-requisitite to learn generative models?\n\nProbabilistic graphical models (PGMs) are not a strict prerequisite for learning generative models, but having a good understanding of PGMs can be beneficial. Generative models aim to learn the underlying data distribution and can generate new samples that resemble the training data. While some generative models, such as Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs), are based on PGMs, there are other popular generative models that do not explicitly rely on PGMs, such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs).\n\nUnderstanding PGMs can provide you with a strong foundation in probability theory, Bayesian inference, and statistical modeling, which can be helpful when studying generative models. It will allow you to grasp the underlying principles and assumptions behind many generative models, as well as the inference and learning techniques used to train them.\n\nHowever, it is possible to learn about generative models without a deep understanding of PGMs, especially if you focus on models like VAEs and GANs, which are primarily based on neural networks. In this case, having a good understanding of deep learning concepts, optimization techniques, and neural network architectures is more crucial.\n\nIn summary, while understanding PGMs is beneficial for learning generative models, it is not a strict prerequisite. You can still learn about generative models, especially those based on neural networks, by focusing on deep learning concepts and techniques. Nevertheless, having a strong foundation in probability theory and PGMs can enhance your understanding of generative models and help you develop a more comprehensive perspective on their principles and applications.\n\n### explain bayesian neural network using simple pytorch code\n\nA Bayesian Neural Network (BNN) is a neural network that incorporates uncertainty by treating the weights and biases as random variables with prior distributions. In this example, we'll demonstrate how to create a simple BNN for regression using PyTorch. We'll use a toy dataset, the `sin(x)` function with added noise, for illustration.\n\nFirst, let's import the necessary libraries and generate the toy dataset:\n\n````\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate toy dataset\ndef generate_toy_data(num_points=100):\n    np.random.seed(0)\n    x = np.random.uniform(-5, 5, size=(num_points, 1))\n    y = np.sin(x) + 0.5 * np.random.normal(size=(num_points, 1))\n    return x, y\n\nx, y = generate_toy_data()\nx_tensor = torch.tensor(x, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\n\n# Next, we define a simple neural network with one hidden layer and a `tanh` activation function:\n\nclass SimpleBNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleBNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ninput_dim = 1\nhidden_dim = 10\noutput_dim = 1\nmodel = SimpleBNN(input_dim, hidden_dim, output_dim)\n\n# Now, we'll define the prior distribution for the weights and biases. In this example, we'll use Gaussian distributions with mean 0 and standard deviation 1:\n\n\ndef gaussian_prior(w, mu=0, sigma=1):\n    log_prob = -0.5 * (w - mu)**2 / sigma**2 - 0.5 * np.log(2 * np.pi * sigma**2)\n    return log_prob.sum()\n\ndef compute_log_prior(model, mu=0, sigma=1):\n    log_prior = 0\n    for param in model.parameters():\n        log_prior += gaussian_prior(param, mu, sigma)\n    return log_prior\n\n# For the likelihood, we'll use a Gaussian distribution with the mean given by the model's output and a fixed standard deviation of 0.5:\n\ndef compute_log_likelihood(model, x, y, sigma=0.5):\n    y_pred = model(x)\n    log_likelihood = -0.5 * (y_pred - y)**2 / sigma**2 - 0.5 * np.log(2 * np.pi * sigma**2)\n    return log_likelihood.sum()\n\n# Now, we'll define the loss function as the negative log-posterior, which is the sum of the negative log-prior and the negative log-likelihood:\n\ndef loss_function(model, x, y):\n    log_prior = compute_log_prior(model)\n    log_likelihood = compute_log_likelihood(model, x, y)\n    loss = -(log_prior + log_likelihood)\n    return loss\n\n# Finally, we'll train the model using stochastic gradient descent:\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nnum_epochs = 1000\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    loss = loss_function(model, x_tensor, y_tensor)\n    loss.backward()\n    optimizer.step()\n\n\tif (epoch + 1) % 100 == 0:         \n\t\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n\n# Now that the model is trained, let's use it for prediction and plot the results:\n\n# Test data for prediction\nx_test = torch.tensor(np.linspace(-6, 6, num=100).reshape(-1, 1), dtype=torch.float32)\ny_test = model(x_test).detach().numpy()\n\nplt.scatter(x, y, label=\"Training Data\")\nplt.plot(x_test, y_test, color=\"red\", label=\"Predictions\")\nplt.legend()\nplt.show()\n\n````\n\nThis code demonstrates a simple example of a Bayesian Neural Network using PyTorch. Note that this is not a fully Bayesian approach, as we are using point estimates of the weights and biases instead of sampling from their posterior distributions. To implement a more accurate Bayesian approach, you could use techniques such as Markov Chain Monte Carlo (MCMC) or Variational Inference.\n\nAdditionally, more sophisticated BNN libraries like Pyro (built on top of PyTorch) can be used to implement BNNs with more advanced features and better handling of the Bayesian aspect.\n\n### show a more accurate bayesian approach you mentioned above for the same example\n\nIn this example, we'll use the Pyro library, which is built on top of PyTorch, to implement a more accurate Bayesian approach for our toy dataset. We'll use the same dataset and neural network architecture as before.\n\nFirst, let's import the necessary libraries and generate the toy dataset:\n\n````\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\n\n# Generate toy dataset\ndef generate_toy_data(num_points=100):\n    np.random.seed(0)\n    x = np.random.uniform(-5, 5, size=(num_points, 1))\n    y = np.sin(x) + 0.5 * np.random.normal(size=(num_points, 1))\n    return x, y\n\nx, y = generate_toy_data()\nx_tensor = torch.tensor(x, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\n\n\n# Next, let's define the same neural network architecture as before:\n\nclass SimpleBNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(SimpleBNN, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = torch.tanh(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# Now we'll define the model and guide functions for Pyro:\n\ndef model(x, y):\n    # Set prior distributions for the weights and biases\n    fc1_weight_prior = dist.Normal(torch.zeros_like(model.fc1.weight), torch.ones_like(model.fc1.weight))\n    fc1_bias_prior = dist.Normal(torch.zeros_like(model.fc1.bias), torch.ones_like(model.fc1.bias))\n    fc2_weight_prior = dist.Normal(torch.zeros_like(model.fc2.weight), torch.ones_like(model.fc2.weight))\n    fc2_bias_prior = dist.Normal(torch.zeros_like(model.fc2.bias), torch.ones_like(model.fc2.bias))\n\n    priors = {'fc1.weight': fc1_weight_prior, 'fc1.bias': fc1_bias_prior, 'fc2.weight': fc2_weight_prior, 'fc2.bias': fc2_bias_prior}\n    lifted_module = pyro.random_module(\"module\", model, priors)\n    lifted_model = lifted_module()\n\n    with pyro.plate(\"map\", len(x)):\n        # Model's output\n        prediction_mean = lifted_model(x).squeeze(-1)\n        # Likelihood\n        pyro.sample(\"obs\", dist.Normal(prediction_mean, 0.5), obs=y)\n\ndef guide(x, y):\n    # Variational distributions for the weights and biases\n    fc1_weight_loc = torch.randn_like(model.fc1.weight)\n    fc1_weight_scale = torch.rand_like(model.fc1.weight)\n    fc1_bias_loc = torch.randn_like(model.fc1.bias)\n    fc1_bias_scale = torch.rand_like(model.fc1.bias)\n    \n    fc2_weight_loc = torch.randn_like(model.fc2.weight)\n    fc2_weight_scale = torch.rand_like(model.fc2.weight)\n    fc2_bias_loc = torch.randn_like(model.fc2.bias)\n    fc2_bias_scale = torch.rand_like(model.fc2.bias)\n\n    fc1_weight_q = pyro.param(\"fc1_weight_q\", fc1_weight_loc)\n    fc1_weight_scale_q = pyro.param(\"fc1_weight_scale_q\", fc1_weight_scale, constraint=dist.constraints.positive)\n    fc1_bias_q = pyro.param(\"fc1_bias_q\", fc1_bias_loc)\n    fc1_bias_scale_q = pyro.param(\"fc1_bias_scale_q\", fc1_bias_scale, constraint=dist.constraints.positive)\n    \n    fc2_weight_q = pyro.param(\"fc2_weight_q\", fc2_weight_loc)\n    fc2_weight_scale_q = pyro.param(\"fc2_weight_scale_q\", fc2_weight_scale, constraint=dist.constraints.positive)\n    fc2_bias_q = pyro.param(\"fc2_bias_q\", fc2_bias_loc)\n    fc2_bias_scale_q = pyro.param(\"fc2_bias_scale_q\", fc2_bias_scale, constraint=dist.constraints.positive)\n\n    fc1_weight = dist.Normal(fc1_weight_q, fc1_weight_scale_q)\n    fc1_bias = dist.Normal(fc1_bias_q, fc1_bias_scale_q)\n    fc2_weight = dist.Normal(fc2_weight_q, fc2_weight_scale_q)\n    fc2_bias = dist.Normal(fc2_bias_q, fc2_bias_scale_q)\n\n    posterior = {'fc1.weight': fc1_weight, 'fc1.bias': fc1_bias, 'fc2.weight': fc2_weight, 'fc2.bias': fc2_bias}\n    lifted_module = pyro.random_module(\"module\", model, posterior)\n    return lifted_module()\n\n# Instantiate the model\ninput_dim = 1\nhidden_dim = 10\noutput_dim = 1\nmodel = SimpleBNN(input_dim, hidden_dim, output_dim)\n\n# Set up the optimizer and the inference algorithm (SVI)\noptimizer = Adam({\"lr\": 0.01})\nsvi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n\n# Training loop\nnum_epochs = 1000\nfor epoch in range(num_epochs):\n    loss = svi.step(x_tensor, y_tensor)\n    if (epoch + 1) % 100 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss/len(x_tensor)}\")\n\n\n# Finally, let's use the trained model for prediction and plot the results:\n\ndef predict(x):\n    sampled_models = [guide(None, None) for _ in range(100)]\n    y_preds = [model(x).detach().numpy() for model in sampled_models]\n    return np.array(y_preds)\n\nx_test = torch.tensor(np.linspace(-6, 6, num=100).reshape(-1, 1), dtype=torch.float32)\ny_test = predict(x_test)\ny_mean = np.mean(y_test, axis=0)\ny_std = np.std(y_test, axis=0)\n\nplt.scatter(x, y, label=\"Training Data\")\nplt.plot(x_test, y_mean, color=\"red\", label=\"Predictions\")\nplt.fill_between(x_test.flatten(), y_mean.flatten()-y_std, y_mean.flatten()+y_std, alpha=0.2, color=\"red\", label=\"Uncertainty\")\nplt.legend()\nplt.show()\n\n\n````\n\nIn this example, we've used Pyro to implement a more accurate Bayesian approach to our neural network. We've defined the model and guide functions, and used Stochastic Variational Inference (SVI) for training. The predictions now include uncertainty, as we sample multiple models from the variational posterior and average their predictions.\n\n* https://medium.com/paper-club/understanding-pyros-model-and-guide-a-love-story-7f3aa0603886\n* Bayesian Neural Network: https://num.pyro.ai/en/stable/examples/bnn.html\n","lastmodified":"2023-06-25T14:09:32.952590951+01:00","tags":[]},"/UoE/MLSP/QS/Week10/PCA-Notes":{"title":"PCA Notes","content":"\n## PCA Algo\n\n![Screenshot 2023-04-30 at 10.25.38 PM.png](Screenshot%202023-04-30%20at%2010.25.38%20PM.png)\n\nThe algorithm for PCA can be broken down into the following steps:\n\n1. Standardize the data: Subtract the mean from each feature and divide by the standard deviation to ensure that all features are on the same scale.\n\n\\*\\*2.  Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n\n**3.  Compute the eigenvectors and eigenvalues of the covariance matrix: Use eigenvalue decomposition to find the eigenvectors and eigenvalues of the covariance matrix.**\n\n**4.  Sort the eigenvectors by decreasing eigenvalues: Arrange the eigenvectors in order of decreasing eigenvalues to identify the principal components.**\n\n**5.  Choose the number of principal components: Determine how many principal components to retain based on the amount of variance explained and/or the desired dimensionality reduction.**\n\n6. Project the data onto the principal components: Multiply the standardized data by the matrix of eigenvectors corresponding to the top k principal components to obtain the k-dimensional representation of the data.\n   1. In part 4. of the screenshot -\u003e do this instead cuz it does not highlight the fact that we are using M principal component i.e. the M principal eigen values for representing the data.\n\nThe key step in this algorithm is step 3, which involves computing the eigenvectors and eigenvalues of the covariance matrix. This can be done using eigenvalue decomposition, which decomposes the covariance matrix into a product of eigenvectors and eigenvalues.\n\nMathematically, this can be expressed as:\n\n$\\Sigma=Q\\Lambda Q^{-1}$\n\nwhere $\\Sigma$ is the covariance matrix, $Q$ is the matrix of eigenvectors, and $\\Lambda$ is the diagonal matrix of eigenvalues.\n\nTo compute the eigenvectors and eigenvalues, we can use an algorithm such as the power iteration method, which iteratively estimates the largest eigenvector and eigenvalue. Once the largest eigenvector has been found, it can be removed from the data and the process can be repeated to find the next largest eigenvector and eigenvalue.\n\nOverall, eigenvalue decomposition is a critical part of the PCA algorithm as it enables us to find the eigenvectors and eigenvalues of the covariance matrix, which in turn allows us to identify the principal components of the data.\n\n### Here is a step-by-step process for performing PCA and spatial whitening on a given dataset:\n\n1. Given a dataset $X$ of $n$ samples each having $d$ dimensions, compute the mean of the data and subtract it from each sample to center the data:\n\n$$\n\\\\bar{x} = \\frac{1}{n} \\sum\\_{i=1}^{n} x_i \\\\\nx_i \\leftarrow x_i - \\bar{x}\n$$\n\n2. Compute the covariance matrix $\\Sigma$ of the centered data:\n\n$$\n\\\\Sigma = \\frac{1}{n} \\sum\\_{i=1}^{n} x_i x_i^T\n$$\n\n3. Compute the eigendecomposition of the covariance matrix $\\Sigma$:\n\n$$\n\\\\Sigma = U \\Lambda U^T\n$$\n\nwhere $\\Lambda$ is a diagonal matrix of the eigenvalues and $U$ is a matrix of the corresponding eigenvectors. Note that the eigenvectors in $U$ are already orthonormal.\n\n4. Sort the eigenvectors in $U$ in descending order of their corresponding eigenvalues in $\\Lambda$ to obtain a matrix $W$:\n\n$$\nW = \\[u_1, u_2, ..., u_d\\]\n$$\n\nwhere $u_i$ is the eigenvector corresponding to the $i$th largest eigenvalue.\n\n5. Project the centered data onto the new basis defined by $W$ to obtain the transformed data $Z$:\n\n$$\nZ = XW\n$$\n**Now -\u003e The step 5 in the previous answer only guarantees zero covariance between dimensions. To ensure unit variance along all dimensions, the covariance matrix of the whitened data needs to be normalized to the identity matrix**\n\n6. Compute the whitening matrix $W\\_{white}$ as:\n\n$$\nW\\_{white} = \\Lambda^{-1/2} U^T\n$$\n\n7. Compute the whitened data $Z\\_{white}$ as:\n\n$$\nZ\\_{white} = ZW\\_{white}\n$$\n\nNow, the whitened data $Z\\_{white}$ has a covariance matrix equal to the identity matrix, indicating that the dimensions are uncorrelated and have unit variance.\n\nNote that the spacial whitening is applied after performing PCA, as it is necessary to first transform the data into a new basis before computing the whitening matrix.\n\nIn Summary:\n\n**Spatial whitening is typically applied after performing PCA**. This is because PCA decorrelates the data, but it does not necessarily produce data with unit variance along all dimensions. By applying spatial whitening after PCA, we can ensure that the resulting transformed data has unit variance along all dimensions, which can be useful for certain applications such as gradient descent optimization.\n\nAfter performing PCA, the eigenvalues of the covariance matrix are used to normalize the data, resulting in a new set of transformed data points. These transformed data points are then further modified through the process of spatial whitening, which results in data that has unit variance along all dimensions.\n\n# Independent Component Analysis (ICA)\n\nPCA aims to retain the max variance in the components. However, these components may not be meaningful for the purpose of for example classification; as the component which has maximum variance, may not be the direction in which we have maximum separation of the data.\n\n* Non-Gaussianity: Non-Gaussianity refers to the property of a distribution that does not follow a Gaussian or normal distribution.\n\n* **Independent Component Analysis (ICA)** is a technique that is particularly suited for dealing with non-Gaussian data. The *basic idea behind ICA is to identify a set of independent sources that underlie the observed data, by exploiting the non-Gaussianity of the sources*. \n  \n  * The assumption is that the data can be expressed as a linear combination of these sources, and that the sources are mutually independent.\n\nIndependent Component Analysis (ICA) is a technique used to extract independent signals from a set of mixed signals. It is similar to PCA but it assumes that the sources are not only linearly correlated but also statistically independent. ICA is often used in signal processing and machine learning applications such as image and speech recognition.\n\n**ICA can be performed on a set of mixed signals by following these steps:**\n\nICA algorithm does not require PCA, but it can be useful to apply PCA or spacial whitening as preprocessing steps before performing ICA. The purpose of these preprocessing steps is to decorrelate the data and reduce the dimensionality of the problem, which can make the ICA algorithm more computationally efficient and more effective.\n\nHere is the complete process for performing ICA with PCA and spacial whitening:\n\n1. Center the data: Subtract the mean of each feature from each data point to center the data around zero.\n\n$$x\\_{centered} = x - \\mu_x$$\n\n2. Perform PCA: Compute the principal components of the centered data using eigenvalue decomposition of the covariance matrix.\n\n$$\\Sigma = U \\Lambda U^T$$\n\nwhere $\\Sigma$ is the covariance matrix, $U$ is the matrix of eigenvectors, and $\\Lambda$ is the diagonal matrix of eigenvalues.\n\n3. Whiten the data: Transform the centered data into a new space where the covariance matrix is the identity matrix by multiplying it by the inverse square root of the diagonal matrix of eigenvalues and the transpose of the matrix of eigenvectors.\n\n$$x\\_{whitened} = \\Lambda^{-1/2} U^T x\\_{centered}$$\n\n4. Apply ICA to the pre-processed data. This involves finding a set of linear transformations, represented by a matrix $\\mathbf{W}$, that decorrelates the columns of $x\\_{whitened}$ and maximizes the non-Gaussianity of the transformed data.\n\n4. Retrieve the original sources: Compute the original sources by multiplying the mixing matrix by the whitened data. \n\nOriginally we have $$ x = \\mathbf{A}s $$, where A is an unknow invertible transform, we aim to learn $\\textbf{W}$ which is $\\mathbf{A^{-1}}$, such that$$ s = \\mathbf{W} x\\_{whitened}$$\nNote that step 2 involves the transpose of the matrix of eigenvectors, $U^T$, which is used in step 3 to transform the data into the whitened space. This transpose operation is necessary to ensure that the columns of $U$ form an orthonormal basis for the whitened space.\n\n### Ambiguities in ICA\n\n1. **Ambiguity in determining the variances of the independent components**\n\nIn ICA, the variances of the estimated independent components are not uniquely determined, and this is known as the variance indeterminacy problem. This ambiguity arises because the independent components can be scaled by arbitrary factors without affecting their independence. \n\nMathematically, if we have a set of independent components $s_1, s_2, ..., s_k$ and their corresponding mixing coefficients $A\\_{i,j}$, then we can scale each independent component $s_i$ by an arbitrary factor $\\alpha_i$ to obtain a new set of independent components $s_1', s_2', ..., s_k'$:\n\n$$s_i' = \\alpha_i s_i$$\n\nThe corresponding mixing coefficients for the new set of independent components will be:\n\n$$A'*{i,j} = \\frac{A*{i,j}}{\\alpha_i}$$\n\nIt can be shown that the new set of independent components $s_1', s_2', ..., s_k'$ is also independent and satisfies the same linear mixing model as the original set of independent components $s_1, s_2, ..., s_k$.\n\nTherefore, the variance of the independent components cannot be uniquely determined because any scaling factor applied to the independent components will affect their variances. However, in practice, the scaling factor can be normalized to make the variance of the independent components equal to one or any other desired value, making the indeterminacy of the variances less of a concern. **(The step we take during whitenening step, helps resolve this as it ensures unit variances across all the independent components)**\n\n2. **Ambiguity of order of independent components**\n\nThe ambiguity of order of independent components refers to the fact that the order in which the independent components are estimated by the ICA algorithm is arbitrary. In other words, the independent components can be returned in any order, and this order may not be meaningful or interpretable.\n\nThis ambiguity can be resolved by using additional information or criteria to order the independent components. For example, if the independent components represent audio signals, they can be ordered based on their frequency content or their energy levels. Alternatively, if the independent components represent images, they can be ordered based on their spatial frequency content or their edge information.\n\nAnother approach is to use clustering algorithms to group independent components that are similar or belong to the same source. This can help to identify the underlying sources of the independent components and to assign them to specific sources in a meaningful way.\n\nOverall, the ambiguity of order of independent components is a common challenge in ICA, but it can be mitigated by using additional information and criteria to order and group the independent components.\n","lastmodified":"2023-06-25T14:09:32.951977451+01:00","tags":[]},"/UoE/MLSP/QS/Week6/QS_W6":{"title":"QS_W6","content":"\n**1. b: By differentiating, show that a necessary condition to minimize P (error) is that θ satisfies  p(θ|c1)P (c1) = p(θ|c2)P (c2)**\n\nLeibniz's rule for differentiating an integral is a method that allows us to differentiate an integral that depends on a parameter. It is named after the German mathematician Gottfried Wilhelm Leibniz.\n\nThe rule states that if we have an integral of the form:\n\n$F(t) = \\int\\_{a(t)}^{b(t)} f(x,t) dx$\n\nwhere $a(t)$ and $b(t)$ are functions of the parameter $t$, and if the integrand $f(x,t)$ and the functions $a(t)$ and $b(t)$ are continuous, then we can differentiate $F(t)$ with respect to $t$ by first differentiating the limits of integration with respect to $t$, and then integrating the result with respect to $x$. That is:\n\n$\\frac{d}{dt} F(t) = \\int\\_{a(t)}^{b(t)} \\frac{\\partial}{\\partial t} f(x,t) dx + f(b(t),t)\\frac{db}{dt} - f(a(t),t)\\frac{da}{dt}$\n\nHere, $\\frac{\\partial}{\\partial t} f(x,t)$ represents the partial derivative of the integrand with respect to $t$.\n\nTo apply this rule to differentiate the expression for P(error) with respect to $\\theta$, we can first write the expression as:\n\n$P(error) = P(c1) \\int\\_{-\\infty}^{\\theta} p(x|c1) dx + P(c2) \\int\\_{\\theta}^{\\infty} p(x|c2) dx$\n\nHere, $a(\\theta) = -\\infty$, $b(\\theta) = \\theta$, and $f(x,\\theta) = p(x|c1)$ for the first integral, and $a(\\theta) = \\theta$, $b(\\theta) = \\infty$, and $f(x,\\theta) = p(x|c2)$ for the second integral.\n\nUsing Leibniz's rule, we can differentiate P(error) with respect to $\\theta$ as follows:\n\n$\\frac{d}{d\\theta} P(error) = P(c1) \\frac{\\partial}{\\partial \\theta} \\int\\_{-\\infty}^{\\theta} p(x|c1) dx + P(c2) \\frac{\\partial}{\\partial \\theta} \\int\\_{\\theta}^{\\infty} p(x|c2) dx + P(c2) p(\\theta|c2) - P(c1) p(\\theta|c1)$\n\nEvaluating the partial derivatives using the fundamental theorem of calculus, we get:\n\n$\\frac{d}{d\\theta} P(error) = P(c1) p(\\theta|c1) - P(c2) p(\\theta|c2)$\n\nThis is the desired result, which shows that the derivative of P(error) with respect to $\\theta$ is equal to the difference between the probability density functions of the two classes evaluated at $\\theta$, weighted by the prior probabilities of each class.\n\nMy simple answer:\n\n![Screenshot 2023-04-29 at 8.13.56 PM.png](Screenshot%202023-04-29%20at%208.13.56%20PM.png)\n\n**1.c: Explain why this equation does not necessarily define a unique θ or even one that minimizes  P (error). Illustrate your answer with examples.**\n\nThe equation $p(\\theta|c1)P(c1) = p(\\theta|c2)P(c2)$ is a necessary condition for finding the value of $\\theta$ that minimizes the probability of error $P(error)$ in a two-category one-dimensional problem with the decision rule \"choose $c1$ if $x \u003e \\theta$, otherwise choose $c2$.\" However, this equation does not necessarily define a unique value of $\\theta$ or even one that minimizes $P(error)$.\n\n**We can also have multiple minimum (local)**\n\nThere are a few reasons for this:\n\n1. **The equation is only a necessary condition, not a sufficient condition, for finding the value of $\\theta$ that minimizes $P(error)$. Even if the equation is satisfied, there may be other values of $\\theta$ that give lower values of $P(error)$.**\n\n1. The equation assumes that the probability density functions $p(x|c1)$ and $p(x|c2)$ are known and can be differentiated with respect to $\\theta$. In practice, these functions may be unknown or difficult to estimate accurately, which can make it challenging to find the value of $\\theta$ that minimizes $P(error)$.\n\n1. The equation assumes that the prior probabilities $P(c1)$ and $P(c2)$ are known and accurately reflect the true class probabilities. In practice, these probabilities may be unknown or incorrectly specified, which can affect the value of $\\theta$ that minimizes $P(error)$.\n\nT\\*\\*o illustrate these points, let's consider an example. Suppose we have a two-category one-dimensional problem with the following probability density functions:\n\n$p(x|c1) = N(x; 0, 1)$\n\n$p(x|c2) = N(x; 2, 1)$\n\nwhere $N(x; \\mu, \\sigma^2)$ is the normal (Gaussian) probability density function with mean $\\mu$ and variance $\\sigma^2$.\n\nIf we assume equal prior probabilities $P(c1) = P(c2) = 0.5$, then the equation $p(\\theta|c1)P(c1) = p(\\theta|c2)P(c2)$ simplifies to:\n\n$N(\\theta; 0, 1) = N(\\theta; 2, 1)$\n\nThis equation has two solutions: $\\theta = 0$ and $\\theta = 2$. However, neither of these values minimizes $P(error)$, which can be shown to be minimized at $\\theta = 1$.\\*\\*\n\nThis example illustrates how the necessary condition $p(\\theta|c1)P(c1) = p(\\theta|c2)P(c2)$ does not necessarily define a unique value of $\\theta or one that minimizes $P(error)$. In this case, the equation is satisfied at two different values of $\\theta$, but neither of these values gives the optimal decision rule. Other factors, such as the shape of the probability density functions and the prior probabilities, can also influence the value of $\\theta$ that minimizes $P(error)$.\n\n**2. Suppose we have two normal distributions with the same covariance but different means.**\n(a) Explain what is meant by a Bayes Classifier.\n\nA Bayes classifier is a classification system that minimises the probability of an  \nincorrect classification.\n\n(b) Derive an expression for the Bayesian Decision Boundary (BDB) in terms of the prior proba-  \nbilities P (c1) and P (c2), the means μ1 and μ2 and the common covariance Σ.\n\n3.a Show that the true error cannot decrease if we first project the distributions to a lower dimensional space and then classify them. \\[Hint: you can think of this geometrically first. If you want to show it mathematically it helps to think of the data lying on a 2D space.\\]\n\nGeometrically, projecting the data to a lower-dimensional space corresponds to mapping the data points onto a lower-dimensional subspace of the original feature space. This can be thought of as collapsing the data points onto a lower-dimensional plane or line.\n\nNow, suppose we have two arbitrary distributions $p(\\mathbf{x}|c_1)$ and $p(\\mathbf{x}|c_2)$ with known priors $P(c_1)$ and $P(c_2)$ in a $d$-dimensional feature space, and we want to classify them using a linear decision boundary. If we project the distributions onto a lower-dimensional subspace (i.e., a lower-dimensional plane or line), then the decision boundary will also be a linear boundary in the lower-dimensional space, and we can classify the data points as belonging to one of two classes based on which side of the boundary they lie.\n\nHowever, projecting the data to a lower-dimensional space can never increase the distance between the means of the two distributions, since the projection essentially collapses the data onto a lower-dimensional plane or line. As a result, the linear decision boundary in the lower-dimensional space can never be further away from the means than the linear decision boundary in the original space. This means that the classification error in the lower-dimensional space can never be lower than the classification error in the original space.\n\nMathematically, we can show this as follows. Let $\\mathbf{x}\\_1$ and $\\mathbf{x}\\_2$ be two data points sampled from the two distributions, with means $\\boldsymbol{\\mu}\\_1$ and $\\boldsymbol{\\mu}\\_2$, respectively. Let $\\mathbf{w}$ be a $d$-dimensional vector that defines the projection onto the lower-dimensional space, and let $\\mathbf{y}\\_1 = \\mathbf{w}^T\\mathbf{x}\\_1$ and $\\mathbf{y}\\_2 = \\mathbf{w}^T\\mathbf{x}\\_2$ be the corresponding projected data points. Then, the distance between the means of the projected distributions is given by:\n\n$|\\boldsymbol{\\mu}\\_{\\mathbf{y}*1} - \\boldsymbol{\\mu}*{\\mathbf{y}\\_2}| = |\\mathbf{w}^T\\boldsymbol{\\mu}\\_1 - \\mathbf{w}^T\\boldsymbol{\\mu}\\_2| = |\\mathbf{w}^T(\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2)|$\n\nSince $\\mathbf{w}$ is a $d$-dimensional vector, $|\\mathbf{w}^T(\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2)|$ is the projection of the vector $(\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2)$ onto the subspace defined by $\\mathbf{w}$. By the Cauchy-Schwarz inequality, this projection is bounded by the length of $(\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2)$, i.e.,\n\n$|\\mathbf{w}^T(\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2)| \\leq ||\\boldsymbol{\\mu}\\_1 - \\boldsymbol{\\mu}\\_2||$\n\nwhere $||\\cdot||$ denotes the Euclidean norm. Therefore, the distance between the means of the projected distributions is always less than or equal to the distance between the means of the original distributions.\n\nSince the classification error is proportional to the distance between the means of the distributions, it follows that the true error cannot decrease if we first project the distributions to a lower-dimensional space and then classify them. In other words , the classification error in the lower-dimensional space can never be lower than the classification error in the original space. This is true for any choice of the projection vector $\\mathbf{w}$ and any choice of the lower-dimensional subspace.\n\nTherefore, if we want to minimize the classification error, we should always use the original high-dimensional feature space for classification, unless we have a good reason to believe that the relevant information in the data lies in a lower-dimensional subspace. In practice, we can use dimensionality reduction techniques, such as principal component analysis (PCA) or linear discriminant analysis (LDA), to identify the most important dimensions or subspace that capture the relevant information in the data. However, we should always keep in mind that projecting the data to a lower-dimensional space can never improve the classification accuracy beyond what is achievable in the original high-dimensional space.\n","lastmodified":"2023-06-25T14:09:32.950591701+01:00","tags":[]},"/UoE/MLSP/QS/Week7/Bayesian-Learning":{"title":"Bayesian Learning","content":"\n## Expectations as Integrals\n\nIn probability theory and statistics, expectations are often expressed as integrals. The expectation of a function $g(x)$ with respect to a probability distribution $p(x)$ is defined as:\n\n$\\mathbb{E}\\[g(x)\\] = \\int g(x)p(x)dx$\n\nThis integral represents the average value of $g(x)$ over the probability distribution $p(x)$. The integral is taken over the entire domain of $x$ and the function $g(x)$ is weighted by the probability density function $p(x)$.\n\nFor example, the mean of a random variable $X$ with probability density function $f(x)$ can be expressed as:\n\n$\\mathbb{E}\\[X\\] = \\int x f(x)dx$\n\nSimilarly, the variance of $X$ can be expressed as:\n\n$\\text{Var}(X) = \\mathbb{E}\\[(X-\\mu)^2\\] = \\int (x-\\mu)^2 f(x)dx$\n\nwhere $\\mu$ is the mean of $X$.\n\nExpectations as integrals allow us to compute various properties of probability distributions, such as mean, variance, and higher moments, and to perform statistical inference, such as hypothesis testing and parameter estimation, based on probability distributions. They also provide a way to generalize the concept of average or expected value beyond simple arithmetic means.\n\n## Making Predictions with Bayesian Learning: Posterior Mean\n\nIn Bayesian learning, the goal is to compute the posterior distribution over the model parameters given the observed data. Once we have obtained the posterior distribution, we can use it to make predictions for new, unseen data points.\n\nOne way to make predictions using Bayesian learning is to use the posterior mean of the model parameters as the predicted value. The posterior mean is a weighted average of the possible parameter values, where the weights are given by the posterior distribution.\n\nSuppose we have a model with parameters $\\theta$ and observed data $\\mathcal{D}$. We can compute the posterior distribution over $\\theta$ using Bayes' rule:\n\n$p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p(\\mathcal{D})}$\n\nwhere $p(\\mathcal{D}|\\theta)$ is the likelihood function, $p(\\theta)$ is the prior distribution over the parameters, and $p(\\mathcal{D})$ is the marginal likelihood, which acts as a normalizing constant.\n\n\\*\\*Once we have obtained the posterior distribution $p(\\theta|\\mathcal{D})$, we can use the posterior mean of $\\theta$ as the predicted value for a new data point $\\mathbf{x}$. That is,\n\n$\\hat{y}(\\mathbf{x}) = \\mathbb{E}\\_{p(\\theta|\\mathcal{D})}\\[y(\\mathbf{x},\\theta)\\]$\n\n**where $y(\\mathbf{x},\\theta)$ is the predicted output for $\\mathbf{x}$ given the parameter value $\\theta$, and $\\mathbb{E}\\_{p(\\theta|\\mathcal{D})}$ denotes the expectation with respect to the posterior distribution over $\\theta$.**\n\nIn practice, computing the posterior mean may involve numerical integration or Monte Carlo methods, depending on the complexity of the model and the posterior distribution. However, the posterior mean provides a principled way to make predictions that takes into account the uncertainty in the model parameters and the observed data.\n\n## Making Predictions with Bayesian Learning: Maximum A Priori\n\nIn Bayesian learning, we often seek to estimate the posterior distribution over the model parameters given the observed data. One way to make predictions using Bayesian learning is to use the maximum a priori (MAP) estimate of the parameters.\n\nThe MAP estimate of the parameters $\\theta$ is the value that maximizes the posterior distribution $p(\\theta|\\mathcal{D})$. Mathematically, we can write:\n\n$$\n\\\\theta\\_{\\text{MAP}} = \\operatorname\\*{argmax}*{\\theta} p(\\theta\\mid D) = \\operatorname\\*{argmax}*{\\theta} p(D\\mid\\theta)p(\\theta)\n$$\n\nwhere $\\mathcal{D}$ represents the observed data, $p(\\mathcal{D}|\\theta)$ is the likelihood function, and $p(\\theta)$ is the prior distribution over the parameters.\n\nOnce we have obtained the MAP estimate of the parameters, we can use it to make predictions for new, unseen data points. For example, in a regression problem, we can predict the output value $\\hat{y}$ for a new input $\\mathbf{x}$ as follows:\n\n$$\n\\\\hat{y} = f(\\mathbf{x};\\theta\\_{\\text{MAP}})\n$$\n\nwhere $f(\\mathbf{x};\\theta)$ is the regression function that maps inputs to outputs using the parameters $\\theta$.\n\nThe MAP estimate can be viewed as a point estimate of the parameters, since it provides a single value for the parameters rather than a distribution over possible values. It can be more computationally efficient than computing the full posterior distribution, especially for large-scale problems.\n\nHowever, the MAP estimate may not fully capture the uncertainty in the model parameters and can be sensitive to the choice of prior distribution. In addition, it can be affected by overfitting if the prior distribution is not well-suited to the data. Therefore, it is important to consider the limitations and assumptions of the MAP estimate and to compare it with other methods of Bayesian learning, such as computing the full posterior distribution or using Bayesian model averaging.\n","lastmodified":"2023-06-25T14:09:32.950873118+01:00","tags":[]},"/UoE/MLSP/QS/Week7/QS_W7":{"title":"QS_W7","content":"\n1.b **Show that the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.**\n\n##### MAP vs. ML Estimates under Nonlinear Transformations\n\nThe maximum likelihood (ML) estimate of the model parameters $\\theta$ is the value that maximizes the likelihood function $p(D\\mid\\theta)$, where $D$ represents the observed data. Mathematically, we can write:\n\n$$\n\\\\theta\\_{\\text{ML}} = \\operatorname\\*{argmax}\\_{\\theta} p(D\\mid\\theta)\n$$\n\nThe maximum a posteriori (MAP) estimate of the parameters $\\theta$ is the value that maximizes the posterior distribution $p(\\theta\\mid D)$. Mathematically, we can write:\n\n$$\n\\\\theta\\_{\\text{MAP}} = \\operatorname\\*{argmax}*{\\theta} p(\\theta\\mid D) = \\operatorname\\*{argmax}*{\\theta} p(D\\mid\\theta)p(\\theta)\n$$\n\nSuppose we have a nonlinear transform $g$ of the parameter space such that $\\theta = g(\\phi)$, where $\\phi$ represents the transformed parameters. If $g$ is invertible, we can write $\\phi = g^{-1}(\\theta)$. \n\nThe likelihood function under the transformed parameter space is given by $p(D\\mid\\phi)$. The ML estimate of the transformed parameters $\\phi$ is the value that maximizes the likelihood function, which we can write as:\n\n$$\n\\\\phi\\_{\\text{ML}} = \\operatorname\\*{argmax}\\_{\\phi} p(D\\mid\\phi)\n$$\n\nHowever, the MAP estimate of the transformed parameters $\\phi$ is given by:\n\n$$\n\\\\phi\\_{\\text{MAP}} = \\operatorname\\*{argmax}*{\\phi} p(\\phi\\mid D) = \\operatorname\\*{argmax}*{\\phi} p(D\\mid\\phi)p(\\phi)\n$$\n\nNow, we can use the **change of variables formula** for probability densities to write:\n\n(look for the change of variables concept below)\n\n$$\np(D\\mid\\phi) = p(D\\mid\\theta) \\left|\\det\\left(\\frac{\\partial g(\\phi)}{\\partial\\phi}\\right)\\right|^{-1}\n$$\n\nwhere $\\left|\\det\\left(\\frac{\\partial g(\\phi)}{\\partial\\phi}\\right)\\right|$ is the absolute value of the determinant of the Jacobian matrix of $g$ with respect to $\\phi$. \n\nSubstituting this into the expressions for the ML and MAP estimates, we get:\n\n$$\n\\\\phi\\_{\\text{ML}} = g^{-1}\\left(\\operatorname\\*{argmax}\\_{\\theta} p(D\\mid\\theta)\\right)\n$$\n\nand\n\n$$\n\\\\phi\\_{\\text{MAP}} = g^{-1}\\left(\\operatorname\\*{argmax}\\_{\\theta} p(D\\mid\\theta)p(g(\\phi))\\left|\\det\\left(\\frac{\\partial g(\\phi)}{\\partial\\phi}\\right)\\right|^{-1}\\right)\n$$\n\nWe can see that the ML estimate of the transformed parameters $\\phi$ is simply the inverse transform of the ML estimate of the original parameters $\\theta$. However, the MAP estimate of the transformed parameters $\\phi$ depends on both the prior distribution over the transformed parameters $p(\\phi)$ and the Jacobian of the transformation. Therefore, the MAP estimate can change under an invertible nonlinear transform of the parameter space while the ML estimate does not.\n\n**Change of Variables Formula for Probability Densities**\n\nThe change of variables formula for probability densities is a formula that allows us to transform a probability density function from one variable to another. It is used when we want to work with a probability distribution in a new set of variables.\n\nSuppose we have a probability density function $p_Y(y)$ over the random variable $Y$, and we want to transform it to a probability density function $p_X(x)$ over the random variable $X$ using a one-to-one function $g: X \\rightarrow Y$. Then, the change of variables formula for probability densities states that:\n\n$$\np_X(x) = p_Y(g(x)) \\left|\\frac{dg(x)}{dx}\\right|\n$$\n\nwhere $\\left|\\frac{dg(x)}{dx}\\right|$ is the absolute value of the derivative of the function $g$ with respect to $x$.\n\nIntuitively, this formula tells us that the probability density of $X$ at $x$ is equal to the probability density of $Y$ at $g(x)$ multiplied by the factor that accounts for the stretching or compression of the probability density as we change variables.\n\nTo use the formula, we first find the inverse function of $g$, which we can use to express $y$ in terms of $x$. Then, we substitute this expression for $y$ into the original probability density function $p_Y(y)$ to get $p_Y(g(x))$. Finally, we multiply this by the factor $\\left|\\frac{dg(x)}{dx}\\right|$ to get the probability density function $p_X(x)$ over the variable $X$.\n\nThe change of variables formula is commonly used in Bayesian inference and machine learning to transform probability distributions between different parameterizations or to transform data from one feature space to another.\n\n3. **Let x be drawn from a uniform density; calculate the maximum likelihood estimate for θ given the data: D = {0.1, 0.4, 0.2, 0.8, 0.45}.**\n\nWe know that $x$ is drawn from a uniform density with parameter $\\theta$ over the interval $\\[0, 1\\]$. The probability density function of $x$ is given by:\n\n$$\np(x | \\theta) =\n\\\\begin{cases}\n\\\\frac{1}{\\theta} \u0026 0 \\leq x \\leq \\theta \\\\\n0 \u0026 \\text{otherwise}\n\\\\end{cases}\n$$\n\nGiven the data $D = {0.1, 0.4, 0.2, 0.8, 0.45}$, the likelihood function is:\n\n$$\nL(\\theta | D) = \\prod\\_{i=1}^n p(x_i | \\theta) =\n\\\\begin{cases}\n\\\\frac{1}{\\theta^n} \u0026 x\\_{(1)} \\leq \\theta \\leq x\\_{(n)} \\\\\n0 \u0026 \\text{otherwise}\n\\\\end{cases}\n$$\n\nwhere $x\\_{(1)}$ and $x\\_{(n)}$ are the minimum and maximum values in the dataset, respectively.\n\nTo find the maximum likelihood estimate of $\\theta$, we need to find the value of $\\theta$ that maximizes the likelihood function. Since the likelihood function is a decreasing function of $\\theta$ for $\\theta \\\u003c x\\_{(n)}$, and an increasing function of $\\theta$ for $\\theta \u003e x\\_{(1)}$, the maximum likelihood estimate of $\\theta$ is given by:\n\n$$\n\\\\hat{\\theta}*{ML} = x*{(n)}\n$$\n\nIn this case, the maximum value in the dataset is 0.8, so the maximum likelihood estimate of $\\theta$ is $\\hat{\\theta}\\_{ML} = 0.8$.\n\n**NOTE: Why are we not taking derivatives in this case? Can we also not solve for it by taking derivative of likelihood function?**\n\n*In this particular case, the likelihood function is not differentiable everywhere, so we cannot take its derivative to find the maximum likelihood estimate of $\\theta$.*\n\nHowever, we can observe that the likelihood function is a decreasing function of $\\theta$ for $\\theta \\\u003c x\\_{(n)}$ and an increasing function of $\\theta$ for $\\theta \u003e x\\_{(1)}$. This means that the maximum likelihood estimate of $\\theta$ must lie in the interval $\\[x\\_{(1)}, x\\_{(n)}\\]$. We can then compare the likelihood of the two endpoints of this interval to find the maximum likelihood estimate. Since the likelihood is 0 outside of this interval, we can be sure that the maximum likelihood estimate lies within it.\n\nIn this case, the endpoint $x\\_{(n)}$ has the highest likelihood among all the possible values of $\\theta$ within the interval, so the maximum likelihood estimate is $\\hat{\\theta}*{ML} = x*{(n)} = 0.8$.\n","lastmodified":"2023-06-25T14:09:32.951086909+01:00","tags":[]},"/UoE/MLSP/QS/Week9/QS_W9":{"title":"QS_W9","content":"\nQ2. \n\nWe can use the law of total probability to express the joint probability of the observation and the current state in terms of the previous state as:\n\n$P(V_t, \\omega(t) = j | \\omega_0) = \\sum_i P(V_t, \\omega(t) = j, \\omega(t-1) = i | \\omega_0)$\n\nUsing the Markov assumption, we can simplify this to:\n\n$P(V_t, \\omega(t) = j | \\omega_0) = \\sum_i P(V_t | \\omega(t) = j, \\omega(t-1) = i, \\omega_0) P(\\omega(t) = j | \\omega(t-1) = i, \\omega_0) P(\\omega(t-1) = i | \\omega_0)$\n\nWe can rewrite the first term using the emission probability $b_j(v_t)$, and the second term using the transition probability $a\\_{i,j}$:\n\n$P(V_t, \\omega(t) = j | \\omega_0) = \\sum_i b_j(v_t) a\\_{i,j} P(\\omega(t-1) = i | \\omega_0)$\n\nSubstituting this into the expression for $\\alpha_j(t)$, we have:\n\n$$\n\\\\begin{align}\n\u0026\\quad \\alpha_j(t) = P(V_1, \\dots, V_t, \\omega(t) = j | \\omega_0) \\\\\n\u0026\\quad = \\sum_i P(V_t, \\omega(t) = j, \\omega(t-1) = i | V_1, \\dots, V\\_{t-1}, \\omega_0) P(V_1, \\dots, V\\_{t-1}, \\omega(t-1) = i | \\omega_0) \\\\\n\u0026\\quad = \\sum_i P(V_t | \\omega(t) = j, \\omega(t-1) = i, V_1, \\dots, V\\_{t-1}, \\omega_0) P(\\omega(t) = j | \\omega(t-1) = i, V_1, \\dots, V\\_{t-1}, \\omega_0) P(V_1, \\dots, V\\_{t-1}, \\omega(t-1) = i | \\omega_0) \\\\\n\u0026\\quad = \\sum_i P(V_t | \\omega(t) = j) P(\\omega(t) = j | \\omega(t-1) = i) P(V_1, \\dots, V\\_{t-1}, \\omega(t-1) = i | \\omega_0) \\\\\n\u0026\\quad = \\sum_i b_j(v_t) a\\_{i,j} \\alpha_i(t-1) \n\\\\end{align} $$\n\nSo we have recursively defined $\\alpha_j(t)$ as a function of the previous time step $\\alpha_i(t-1)$, the transition probability $a\\_{i,j}$, and the emission probability $b_j(v_t)$.\n","lastmodified":"2023-06-25T14:09:32.951307618+01:00","tags":[]},"/UoE/MLSP/Track-Weeks":{"title":"Track Weeks","content":"\n* [x] Week 1\n  * [x] Classification Part 1\n    * Introduction to the biological neuron that served as inspiration for the Perceptron\n    * Linear classification and simple linear discriminant functions, and the concept of hyperplanes\n    * Multicategory classification, linear machines and generalized linear discriminants\n    * Normalisation, solution regions, the margin\n* [x] Week 2\n  * [x] Classification Part 2\n    * Convex function, set and hull\n    * The Perceptron Criterion and Algorithm\n    * The MSE Classification Procedure\n    * Support Vector Machines\n* [x] Week 3\n  * [x] Neural Network\n    * Multilayer Neural Networks: forward operation and expressive power\n    * Multilayer Neural Networks: Back-propagation setup\n    * Multilayer Neural Networks: Back-propagation algorithm\n    * Multilayer Neural Networks: Training protocols\n* [ ] Week 4\n  * [ ] CNN\n    * Convolutional Neural Networks and Deep Learning\n  * [ ] Optimization\n    * Reviewing Background for Optimisation\n    * Bracketing Methods for 1-d Optimisation\n    * Gradient Descent\n    * Newton’s Method\n* [ ] Week 5\n  * [ ] Optimization\n    * Levenberg Marquardt\n    * Gradient descent with line minima\n    * Conjugate Gradient Descent\n* [ ] Week 6\n  * [ ] Bayes Decision Theory\n    * Supervised vs unsupervised learning; probabilistic view to learning\n    * Bayes formula, Bayes decision rule and probability of error\n    * Examples of statistical classification from a Bayesian view\n* [ ] Week 7\n  * [ ] Model Learning\n    * Maximum likelihood estimation; non-linear regression; Maximum likelihood fitting of Gaussians\n    * Overfitting; bias-variance dilemma\n    * Learning models with Bayesian; fitting 1D Gaussians; the Bayes – Maximum likelihood link\n* [ ] Week 8\n  * [ ] Clustering\n    * K-means clustering; k-means for vector quantization; optimality of k-means\n    * Relation to matrix factorisation; k-medoids algorithm\n    * Mixture of Gaussians and EM algorithm\n* [ ] Week 9\n  * [ ] HMM\n    * First-order Markov models; first-order hidden Markov models; different models\n    * Three fundamental problems; evaluation problem; isolated word speech recognition\n    * Evaluation problem; decoding problem; learning problem\n* [ ] Week 10\n  * [ ] Principal Component Analysis (PCA)\n    * Principal component analysis; importance and implications; subspace projections\n    * Relation to matrix factorisation; PCA example\n  * [ ] Independent Component Analysis (ICA)\n    * Independent component analysis; differences with PCA\n","lastmodified":"2023-06-25T14:09:32.952173992+01:00","tags":[]},"/UoE/MLSP/Week1/Notes":{"title":"Notes","content":"\n# Linear Discriminant Functions\n\n**Linear discriminant functions**, also known as **linear classifiers**, are mathematical functions used in classification tasks to separate data points from different classes based on their feature values. These functions **model a linear decision boundary or hyperplane that separates the classes in the feature space**.\n\nHere's an overview of linear discriminant functions:\n\n1. **Representation**: A linear discriminant function is typically represented as a linear combination of the input features. It takes the form: $f(x) = w^T x + b$ where $f(x$) represents the predicted class label for a given input vector $x, w$ is the weight vector, and $b$ is the bias or intercept term.\n\n1. **Decision Boundary**: The linear discriminant function defines a decision boundary or hyperplane in the feature space. The decision boundary is the locus of points where the function output changes, determining the classification of the input points. In a binary classification problem, the decision boundary separates the data points of one class from the other class.\n\n**Linear discriminant functions assume that the decision boundaries between classes are linear.** While they may not capture complex relationships present in the data, linear classifiers can be effective when classes are separable by linear decision boundaries or in situations where interpretability and simplicity are important.\n\nOverall, linear discriminant functions play a crucial role in classification tasks by separating data points based on linear decision boundaries or hyperplanes, enabling the prediction of class labels for unseen data.\n","lastmodified":"2023-06-25T14:09:32.953060409+01:00","tags":[]},"/UoE/NLU/Topics-Tracker":{"title":"Topics Tracker","content":"\n## [Week 1 (Machine Translation)](Week1)\n\n* [x] Introduction\n* [x] Machine Translation\n* [x] Conditional Language Models (with n-grams)\n\n## [Week 2 (Language Models)](Week2)\n\n* [x] Feedforward Language Models\n* [x] RNN\n* [x] LSTM\n* [x] GRU\n\n## [Week 3](Week3)\n\n* [x] Seq2Seq Models with attention\n* [ ] Transformers\n\n## [Week 4](Week4)\n\n* [x] Word Embeddings\n* [x] Pretrained Language Models\n\n## [Week 5](Week5)\n\n* [x] Prompting\n* [x] Evaluation of MT\n\n## [Week 6](Week6)\n\n* [x] Open Vocabulary Models\n* [x] Low Resource MT\n* [x] NLP Ethics\n\n## [Week 7](Week7)\n\n* [ ] Bias in Embeddings and Language Models\n* [x] Summarization 1\n* [x] Summarization 2\n\n## [Week 8](Week7)\n\n* [x] Summarization 3\n\n## Week 9\n\n* [ ] Neural Parsing\n* [ ] Unsupervised Parsing\n\n## Week 10\n\n* [ ] Movie Summarization\n\n## Tutorials\n\n* [ ] Tutorial 1 - Language Models\n* [ ] Tutorial 2 - Neural Network Language Models\n* [ ] Tutorial 3 - Transformers\n* [ ] Tutorial 4 - Ethics in NLP\n* [ ] Tutorial 5 - Summarization\n","lastmodified":"2023-06-25T14:09:32.949176118+01:00","tags":[]},"/UoE/NLU/Week1":{"title":"Week1","content":"\n# Introduction\n\nReadings:\n\n* [The Future of Computational Linguistics: Beyond the Alchemy](https://www.frontiersin.org/articles/10.3389/frai.2021.625341/full) Church and Lieberman (21)\n\n* \n  \n  * Why has deep learning taken over NLP?\n    * Universal Function Approximators\n    * Representation Learning\n    * Multi-task learning\n    * Works with wide class of data: **strings, labels, trees, graphs, tables, images**\n    * deep learning solves the difficulties of applying machine learning to NLP... *But it does not solve NLP!*\n  * Problems with DL:\n    * Energy - carbon cost increasing exponentially\n    * Ethical practice lags technical practice - *Privacy, Bias, Justice* etc.\n\n**Fundamental Methods of the Course**\n\nOur primary tool will be probabilistic models parameterized by\n\ndeep learning architectures such as:  \n• feed-forward neural networks  \n• recurrent neural networks  \n• transformers\n\n... applied primarily to structured prediction tasks in NLP.\n\nThe second half of the course will focus on the application of deep models to a variety of core \n**NLP tasks:**  \n• machine translation  \n• word embeddings  \n• pretrained language models  \n• syntactic parsing  \n• semantic parsing  \n**And applications:**  \n• paraphrasing  \n• question answering  \n• summarization  \n• data-to-text generation\n\n# Machine Translation\n\nReadings:\n\n* Background Reading: [Automating Knowledge Acquisition for Machine Translation](https://kevincrawfordknight.github.io/papers/aimag97.pdf), Knight.\n\n**Challenges in MT:**\n\n* Words are ambiguous (**French: Banque** / **English: Bank**, **French: rive** / **English: Bank**)\n* Words have complex morphology\n  * Tense\n  * Singular/Plural\n  * Morphology:\n    * Finnish: ostoskeskuksessa  \n      ostos#keskus+N+Sg+Loc:in  \n      shopping#center+N+Sg+Loc:in  \n      English: ‘in the shopping center’\n* Word order matters (reordering can change the meaning of sentence)\n* Every word counts (can't omit word during translation)\n* Sentences are long sometimes and complex\n\n**Cues which might be helpful for translation:**\n\n* Pairs of words that occur consecutively in the target language (bigrams)\n* Pairs of source and target words that occur frequentyl occur together in translations\n\n**How to model these cues? what are the ways they can fail?**\nLet's look at them!!\n\n# Conditional Language Models (with n-grams)\n\nReadings:\n\n* [Word Alignment and the Expectation Maximization Algorithm,](https://www.learn.ed.ac.uk/bbcswebdav/pid-7935785-dt-content-rid-30666816_1/xid-30666816_1) Lopez.\n*  [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619)\n\n**How to derive an n-gram language model**\n\nTo define the probability $P(w) = P(w1 . . . w\\_{|w|})$ for a given sequence of words w, we can use an n-gram language model. An n-gram model estimates the probability of a word sequence by approximating it as the product of probabilities of individual words given their previous n-1 words, where n is a positive integer (usually 1, 2, or 3).\n\nSpecifically, we can define P(w) as the product of conditional probabilities of each word given its previous n-1 words:\n\n$$P(w) = P(w_1, w_2, \\dots, w\\_{|w|}) \\approx \\prod\\_{i=1}^{|w|} P(w_i | w\\_{i-1}, w\\_{i-2}, \\dots, w\\_{i-n+1})$$\n\nwhere $P(wi | w(i-1), w(i-2), ..., w(i-n+1))$ is the conditional probability of the ith word given its previous n-1 words, and the approximation holds by the Markov assumption that the probability of a word only depends on its previous n-1 words.\n\n**Maximizing likelihood by counting n-grams**\n\ncounting n-grams is one way to estimate the probabilities in an n-gram language model, and it aims to maximize the likelihood of the observed data. The likelihood function L estimates the probability of the observed data given the model parameters (i.e., the n-gram probabilities):\n\n$$L(P(w_1, w_2, ..., w\\_{|w|})) = \\prod\\_{i=1}^{|w|} P(w_i | w\\_{i-1}, w\\_{i-2}, ..., w\\_{i-n+1})$$\n$$\n\\\\hat{\\theta} = \\arg\\max\\_\\\\theta P(D | \\theta)\n$$\n\nwhere $w_1, w_2, ..., w_n$ is the observed data. The goal of estimating the n-gram probabilities is to maximize the likelihood function L, given the observed data.\n\nCounting n-grams is a simple and effective method to estimate the n-gram probabilities, where we count the frequency of each n-gram in the training data and normalize it by the count of its preceding (n-1)-gram. However, this method has a **limitation that it assigns zero probability to any n-gram that did not occur in the training data (i.e., zero-frequency problem), which can lead to poor performance in handling unseen data.**\n\nTo overcome this limitation, **smoothing techniques are often used to estimate the probabilities of unseen n-grams.** Smoothing methods aim to assign non-zero probabilities to unseen n-grams by borrowing probabilities from other n-grams or using prior knowledge.\n\n**Predition using n-gram LM**\n\nIf we have a sequence of words $w_1 . . . w_k$, then we can use the language model to predict the next word $w\\_{k+1}$:\n$$w^\\**{k+1} = \\operatorname{argmax}*{k+1} P(w\\_{k+1} | w_1, w_2, \\dots, w_k)$$\n\n### Can we model Machine Translation with n-gram?\n\nYes, we can: but there is a lot of machinery involved such as:\nSome issue: \n\n* Length mismatch b/w source and target language.\n* Change in word order b/w source and target languages\n\n**Machine translations is Conditional Language Modeling:**\n\n* We will have one-word or phrase from one language mapped to the other language\n* We will need to model this alignment. Now this alignment is actually unknown and unobserved, so HMM can be a good choice of model here.\n\n$$MT = P(|y|||x|)\\prod\\_{i=1}^{|y|} P(z_i||x|)P(y_i|x\\_{z_i})$$\nHere: $P(|y|||x|)$  means for all pairs of sentence lengths |y|, |x|. $P(y_i|x_i)$ is the pairs of all co-occuring words in both languages. $P(z_i||x|)$ probability of how $y_i$ aligns with $x_i$ according to some distribution.\n\nAlso: $P(z_i||x|)$ is the *transition probability* and $P(y_i|x\\_{z_i})$ is the *emission probability*. $z$ is the latent variable here which is unobserved. We can use **Expectation-Maximization** to estimate the MLE $\\hat{\\theta}$  .\n\nNow for **decoding**: \n$$\nP(y|x) = \\frac{P(x|y) P(y)}{P(x)} \\propto  P(y) P(x|y)\n$$\nwhere: $P(y)$ is **language model** and $P(x|y)$ is the **translation model.**\n\n**Why care about n-grams? Aren’t they obsolete?**\n\n1. Many of these ideas turn up again in neural models.\n   \n   • All machine learning maximizes some **objective function**.  \n   • Neural models still use **beam search**.  \n   • **Latent variables** are common in **unsupervised learning**.  \n   • **Alignment** directly inspired **neural attention**.  \n   • Neural models exploit same signals, though more powerful.\n\n1. Older models are still often useful in **low-data settings**.\n\n1. An extension of the model in this lecture translates  n-grams to n-grams: **phrase-based translation**. It is still  used by Google for some languages, despite move to  neural MT in 2017.\n\n1. Understanding the tradeoffs of working with Markov assumptions will help you appreciate the fact that neural models usually make them go away!\n","lastmodified":"2023-06-25T14:09:32.952578409+01:00","tags":[]},"/UoE/NLU/Week10":{"title":"Week10","content":"\n# Movie Summarization\n\nReadings\n[Movie Summarization via Sparse Graph Construction.](https://arxiv.org/abs/2012.07536) Papalampidi, Keller, Lapata, 2021.\n\n* **Motivation**\n  \n  * Summarize full-length movies by creating shorter videos containing their most informative scenes.\n* **Key Concept**\n  \n  * **Turning Points**\n    * Key events in a movie  that describe its storyline\n* **Problem Formulation**\n  \n  * **Data**\n    * Let $D$ denote a screenplay consisting of a sequence of scenes $D = {s_1, s_2, \\ldots, s_n}$. We aim at selecting a smaller subset $D' = {s_i, \\ldots, s_k}$ consisting of the most informative scenes describing the movie's storyline. \n  * **Objective**\n    * Our objective is to assign a binary label $y_i$ to each scene $s_i$ denoting whether it is part of the summary.\n  * **Process**\n    * **Classification Problem:** \n      * For each scene $s_i \\in D$ we assign a binary label $y\\_{it}$ denoting whether it represents turning point $t$. \n      * Specifically, we calculate probabilities $p(y\\_{it}|s_i, D, \\theta)$ quantifying the extent to which $s_i$ acts as the $t^{th}$ TP, where $t \\in \\[1, 5\\]$ (and $\\theta$ are model parameters). \n  * **Inference**\n    * During inference, we compose a summary by selecting $l$ consecutive scenes that lie on the peak of the posterior distribution $\\arg\\max\\_{i=1}^N p(y\\_{it}|s_i, D, \\theta)$ for each TP.\n* **Modeling Process**\n  \\* \n\n\n\n* **What does the graphs mean?**\n  \n","lastmodified":"2023-06-25T14:09:32.951229784+01:00","tags":[]},"/UoE/NLU/Week2":{"title":"Week2","content":"\n# Feedforward Language Model\n\nReading:\n\n* [A Course in Machine Learning](http://ciml.info/), chapter 4, Daumé.\n* Chapter 7 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), 3rd edition, Jurafsky \u0026 Martin.\n\n### Representing n-gram probabilities with neural networks\n\n* **For neural networks:**\n  * $\\mathbf{x}$ : input vector\n  * $\\mathbf{h}$ : hidden layer\n  * $\\mathbf{y}$ : output vector\n\n**Vector Rrepresentations:** \n\n* one-hot encoding\n* probability distribution as vector \n  * Softmax: turns any vector into a probability distribution\n  * Input: a vector $\\mathbf{v}$ of length $|\\mathbf{v}|$\n  * Output: softmax($\\mathbf{v}$) is a distribution $\\mathbf{u}$ over $\\mathbf{|v|}$ where: $$u_i = \\frac{exp (v_i)}{\\sum\\_{j=1}^{|v|} exp(v_j)}$$\n* **Logistic Regression:**$$ \\mathbf{y} = softmax(\\mathbf{Wx + b}) $$\n* $\\mathbf{W}$ and $\\mathbf{b}$ are parameters of the model\n* Input $\\mathbf{x}$ and output $\\mathbf{y}$ have different dimension, which determine the dimension of $\\mathbf{W}$ and $\\mathbf{b}$ \n  * $\\mathbf{W}$  = $|\\mathbf{y}| \\times |\\mathbf{x}|$ \n  * $\\mathbf{b}$ is bias = $|\\mathbf{y}|$\n\n### Learning and Objective Functions\n\n**Two views**\n\\* **Maximize Likelihood** : Attempt to put as much as probability on the target output\n\\* **Minimize Error** : Minimizes the **cross-entropy** loss, which penalizes the model for the proportion of the output probability mass that it does not assign to the target output.\n\\* Gradient descent: good framework, however, limited in power in practice, use the following to make it more efficient\n\\* with momentum\n\\* individual learning rates for each dimension\n\\* adaptive learning rate\n\\* decoupling step length from partial derivatives\n\n# Recurrent Networks for Language Modeling (RNN)\n\nReadings\n\n* Sections 4 and 5 of [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619), Neubig\n\nSo far we have seen n-gram and Feedforward networks:\n\n* both have fixed context width, however, linguistic dependencies can be arbitrarily long. This is where RNN come in!\n\n**- Why not to use a standard network for sequence tasks? There are two problems:**\n\n* **Inputs, outputs can be different lengths in different examples**.\n  * This can be solved for normal NNs by paddings with the maximum lengths but it's not a good solution.\n* **Doesn't share features learned across different positions of text/sequence**.\n  * Using a feature sharing like in CNNs can significantly reduce the number of parameters in your model. That's what we will do in RNNs.\n* Recurrent neural network doesn't have either of the two mentioned problems.\n\n* **Glossary**\n  \n  * $x_i$ : the input word transformed into one hot encoding\n  * $y_i$ : the output probability distribution\n  * $\\mathbf{U}$ : the weight matrix of the recurrent layer\n  * $\\mathbf{V}$ : the weight matrix between the input layer and the hidden layer\n  * $\\mathbf{W}$ : the weight matrix between the hidden layer and the output layer\n  * $\\sigma$ : the sigmoid activation function\n  * $\\mathbf{h}$ : the hidden layer\n* A diagram showing a RNN (this in an unfolded representation or a complete computational graph)\n  \\* \n  \\* No markov assumption is made here: $$\n  \\\\begin{align}\n  \u0026\\quad P(x\\_{i+1}|x_1, ...,x_i) = y_i \\\\\n  \u0026\\quad y_i = softmax(\\mathbf{Wh_i + b_2}) \\\\\n  \u0026\\quad h_i = \\sigma(\\mathbf{Vx_i + Uh\\_{i-1} + b_1}) \\\\\n  \u0026\\quad x_i = onehot(\\mathbf{x_i})\n  \\\\end{align}\n  $$\n  \n  * **Training:**\n    * SGD with cross entropy\n    * **BPTT (backpropagation through time)** can be done through arbitrary number of steps $\\tau$. This makes RNN a deep neural network with $\\tau$ depth, which can be arbitrarily large\n      * In practice we implement **Truncated BPTT**, i.e. we only update gradients only through a limited number of steps.\n      * If we have large number of steps $\\tau$, derivatives which are backpropagated becomes vanishingly small, resulting in no weight update. This problem is called **Vanishing Gradient Problem** which prevents the RNN from learning long term dependencies.\n\n# Long Short-term Memory Networks (LSTMs)\n\nReadings:\n\n* Section 6 of [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/abs/1703.01619), Graham Neubig. \n* [Backpropagation through Time](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf), Jiang Guo.\n\nHelpful Resources: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n\nLSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) that is designed to handle the vanishing gradient problem in traditional RNNs. LSTMs can effectively capture long-term dependencies in sequential data by allowing information to flow through \"memory cells\" that can be selectively read from or written to at each time step.\n\nHere are the main steps in LSTMs:\n\n* First we have a  **Forget gate**, which  determines how much of the previous memory cell value should be retained, from cell state $C\\_{t-1}$\n  \n  $f_t = \\sigma(W_f\\[x_t, h\\_{t-1}\\] + b_f)$\n\n\n\n* Next step, is to decide what new information we're going to store in the cell steate.  **Input gate:** determines how much of the new input should be incorporated into the memory cell. **Candidate memory cell value:** the new value to be potentially added to the memory cell\n  \n  $i_t = \\sigma(W_i\\[x_t, h\\_{t-1}\\] + b_i)$\n  $\\tilde{C}*t = \\tanh(W_c\\[x_t, h*{t-1}\\] + b_c)$\n\n\nIt's now time to update the old cell state, $C\\_{t-1}$ into the new cell state $C\\_{t}$. **Memory cell update:** combines the input, forget, and candidate values to update the memory cell value\n\n````\n$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n![[LSTM3-focus-C.png]]\n````\n\n* Finally, we need to decide what we’re going to output. **Output gate:** determines how much of the current memory cell value should be output. **Hidden state output:** applies the output gate to the updated memory cell value to compute the hidden state output at the current time step\n  \n  $o_t = \\sigma(W_o\\[x_t, h\\_{t-1}\\] + b_o)$\n  $h_t = o_t * \\tanh(C_t)$\n  \n\nwhere $x_t$ is the input at time step $t, h\\_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, \\sigma is the sigmoid activation function, and * denotes element-wise multiplication.\n\nThese equations allow the LSTM to selectively forget or remember information at each time step, based on the input and the current state of the memory cell. By doing so, LSTMs can effectively capture long-term dependencies in sequential data, making them well-suited for tasks such as natural language processing and speech recognition.\n\n**How does LSTM solve the gradient problem?**\n\n* the memory cell is linear, so its gradient doesn’t vanish;\n  * ````\n    $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n    ````\n  \n  * This is linear.\n  * If $f_t$ is 1 and $i_t$ is 0 then we can simply pass the activation from previous cell state $C\\_{t-1}$ ; this means LSTM block can retain information indefinitely.\n* All the gates are trainable; the LSTM block learns when to accept input, produce output and forget information\n\n# Gated Recurrent Unit (GRU)\n\nGRUs are similar to LSTMs in that they are also designed to handle the vanishing gradient problem in traditional RNNs, but they have a simpler architecture with fewer parameters. Like LSTMs, GRUs use \"gates\" to selectively filter information flow through the network. However, they have a more streamlined approach to gating, with just two gates: an update gate and a reset gate.\n\nHere are the main equations used in GRUs:\n\n* **Update gate:** determines how much of the previous hidden state should be retained\n  \n  $z_t = \\sigma(W_z\\[x_t, h\\_{t-1}\\] + b_z)$\n\n* **Reset gate:** determines how much of the previous hidden state should be ignored\n  \n  $r_t = \\sigma(W_r\\[x_t, h\\_{t-1}\\] + b_r)$\n\n* **Candidate hidden state output:** a new value to be potentially added to the hidden state output\n  \n  $\\tilde{h}*t = \\tanh(W_h\\[x_t, r_t * h*{t-1}\\] + b_h)$\n\n* **Hidden state output:** a linear combination of the current and candidate hidden state values, controlled by the update gate\n  \n  $h_t = (1 - z_t) * h\\_{t-1} + z_t * \\tilde{h}\\_t$\n\nwhere $x_t$ is the input at time step $t, h\\_{t-1}$ is the hidden state output from the previous time step, W and b are weight and bias parameters, and \\sigma is the sigmoid activation function. * denotes element-wise multiplication.\n\nThese equations allow the GRU to selectively update or ignore information at each time step, based on the input and the previous hidden state. The update gate $z_t$ controls how much of the new candidate hidden state value $\\tilde{h}*t$ should be added to the previous hidden state value $h*{t-1}$, while the reset gate $r_t$ controls how much of the previous hidden state value should be ignored. \n\nOverall, the GRU architecture is simpler than that of the LSTM, but it can still effectively capture long-term dependencies in sequential data. By selectively updating or ignoring information at each time step, the GRU can adapt to changes in the input and capture complex patterns in the data.\n\n### Difference b/w GRU v/s LSTM\n\nOne of the main differences is that GRUs have a simpler architecture with fewer parameters than LSTMs. **GRUs use two gates, an update gate and a reset gate, to selectively update or ignore information in the hidden state**, whereas LSTMs have **three gates, an input gate, a forget gate, and an output gate, as well as a memory cell.**\n\nAnother difference is in the way that the hidden state is updated. In GRUs, **the candidate hidden state value is computed using a single matrix multiplication of the input and the previous hidden state**, while in LSTMs, the **candidate memory cell value is computed separately from the input and the previous hidden state, using two matrix multiplications and an element-wise multiplication.**\nIn GRUs, the candidate hidden state is computed as the element-wise product of the reset gate $r_t$ and the previous hidden state $h\\_{t-1}$, which is then concatenated with the input $x_t$ and transformed by a weight matrix W_h and an activation function tanh. In contrast, in LSTMs, the candidate memory cell $\\tilde{C}*t$ is computed separately from the input $x_t$ and the previous hidden state $h*{t-1}$, using a weight matrix $W_C$ and an activation function $tanh$.\nFurthermore, the memory cell $C_t$ in LSTMs is updated using both the forget gate $f_t$ (which controls how much of the previous memory cell state to retain) and the input gate $i_t$ (which controls how much of the new candidate memory cell value to use). The hidden state $h_t$ is then computed using the output gate $o_t$ (which controls how much of the memory cell state to output) and the updated memory cell $C_t$. In GRUs, the hidden state is computed directly using the candidate hidden state and the update gate $z_t$.\n\nThe differences in architecture between GRUs and LSTMs have implications for their performance in different tasks. For example, GRUs may be more suitable for tasks that require faster training and less complex models, while LSTMs may be more suitable for tasks that require modeling more complex dependencies and handling longer sequences of data.\n","lastmodified":"2023-06-25T14:09:32.954556742+01:00","tags":[]},"/UoE/NLU/Week3":{"title":"Week3","content":"\n**Attention Model Intuition**\n\n\n\n# Seq2Seq Models with Attention\n\nIn a sequence-to-sequence (seq2seq) encoder-decoder model with attention, the goal is to generate a sequence of output tokens based on a sequence of input tokens. The model consists of two parts: an encoder that processes the input sequence and a decoder that generates the output sequence. The attention mechanism is used in the decoder to help the model focus on the most relevant parts of the input sequence while generating each output token.\n\nThe attention mechanism works as follows:\n\n1. The encoder produces a sequence of hidden states $h_1, h_2, ..., h_n$ that represent the input sequence. \n\n1. At each step $t$ of the decoder, the decoder produces a hidden state $s_t$ that represents the decoder's current state. \n\n1. The attention mechanism calculates a set of attention weights $a^t_1, a^t_2, ..., a^t_n$ that indicate how much attention should be given to each hidden state $h_i$ of the encoder when generating the output at step $t$ of the decoder. \n   \n   1. The attention weights are used to compute a context vector $c_t$ that is a weighted sum of the encoder hidden states, where each hidden state is weighted by its corresponding attention weight:\n\n$$c_t = \\sum\\_{i=1}^{n} a^t_i h_i$$\n\n5. The context vector $c_t$ is concatenated with the decoder hidden state $s_t$ to form a combined vector $z_t$:\n\n$$z_t = \\[c_t; s_t\\]$$\n\nwhere $\\[;\\]$ denotes concatenation.\n\n6. The combined vector $z_t$ is used to predict the probability distribution over the output vocabulary using a softmax function:\n\n$$P\\_{vocab} = \\text{softmax}(V'^T(Vz_t + b')$$\n\nwhere $V'$ is a weight matrix and $b'$ is a bias term.\n\nThe attention weights $a^t_i$ are calculated using the following equations:\n\n$$e_i^t= v^T \\text{tanh}(W_h h_i + W_s s_t + b\\_{attn})$$\n\n$$a^t_i = \\frac{\\text{exp}(e_i^t)}{\\sum\\_{j=1}^{n} \\text{exp}(e_j^t)}$$\n\nwhere $v$, $W_h$, $W_s$, and $b\\_{attn}$ are learnable parameters.\n\nThe first equation computes a score $e_i^t$ for each encoder hidden state $h_i$ based on the decoder hidden state $s_t$. The second equation applies a softmax function to the scores to obtain the attention weights $a^t_i$ that sum to 1. The attention weights indicate the relative importance of each encoder hidden state $h_i$ at the current step $t$ of the decoder.\n\nBy using the attention mechanism, the model can selectively focus on different parts of the input sequence at each step of the decoder, improving the model's ability to generate accurate and relevant output tokens.\n\n# Transformers\n\n \u003e \n \u003e **Calculating the parameters of transformers:**\n\n* https://stackoverflow.com/a/71472362\n  \n  [Transformer Encoder-Decoder Architecture](https://i.stack.imgur.com/BhVnx.png) The BERT model contains only the encoder block of the transformer architecture. Let's look at individual elements of an encoder block for BERT to visualize the number weight matrices as well as the bias vectors. The given configuration L = 12 means there will be 12 layers of self attention, H = 768 means that the embedding dimension of individual tokens will be of 768 dimensions, A = 12 means there will be 12 attention heads in one layer of self attention. The encoder block performs the following sequence of operations:\n  \n  1. The input will be the sequence of tokens as a matrix of S * d dimension. Where s is the sequence length and d is the embedding dimension. The resultant input sequence will be the sum of token embeddings, token type embeddings as well as position embedding as a d-dimensional vector for each token. In the BERT model, the first set of parameters is the vocabulary embeddings. BERT uses WordPiece\\[[2](https://arxiv.org/abs/1609.08144)\\] embeddings that has 30522 tokens. Each token is of 768 dimensions.\n  \n  1. Embedding layer normalization. One weight matrix and one bias vector.\n  \n  1. Multi-head self attention. There will be h number of heads, and for each head there will be three matrices which will correspond to query matrix, key matrix and the value matrix. The first dimension of these matrices will be the embedding dimension and the second dimension will be the embedding dimension divided by the number of attention heads. Apart from this, there will be one more matrix to transform the concatenated values generated by attention heads to the final token representation.\n  \n  1. Residual connection and layer normalization. One weight matrix and one bias vector.\n  \n  1. Position-wise feedforward network will have one hidden layer, that will correspond to two weight matrices and two bias vectors. In the paper, it is mentioned that the number of units in the hidden layer will be four times the embedding dimension.\n  \n  1. Residual connection and layer normalization. One weight matrix and one bias vector.\n  \n  Let's calculate the actual number of parameters by associating the right dimensions to the weight matrices and bias vectors for the BERT base model.\n  \n  **Embedding Matrices:**\n  \n  * Word Embedding Matrix size \\[Vocabulary size, embedding dimension\\] = \\[30522, 768\\] = 23440896\n  * Position embedding matrix size, \\[Maximum sequence length, embedding dimension\\] = \\[512, 768\\] = 393216\n  * Token Type Embedding matrix size \\[2, 768\\] = 1536\n  * Embedding Layer Normalization, weight and Bias \\[768\\] + \\[768\\] = 1536\n  * Total Embedding parameters = **𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 ≈ 𝟐𝟒𝑴**\n  **Attention Head:**\n  \n  * Query Weight Matrix size \\[768, 64\\] = 49152 and Bias \\[768\\] = 768\n  \n  * Key Weight Matrix size \\[768, 64\\] = 49152 and Bias \\[768\\] = 768\n  \n  * Value Weight Matrix size \\[768, 64\\] = 49152 and Bias \\[768\\] = 768\n  \n  * Total parameters for one layer attention with 12 heads = 12∗(3 ∗(49152+768)) = 1797120\n  \n  * Dense weight for projection after concatenation of heads \\[768, 768\\] = 589824 and Bias \\[768\\] = 768, (589824+768 = 590592)\n  \n  * Layer Normalization weight and Bias \\[768\\], \\[768\\] = 1536\n  \n  * Position wise feedforward network weight matrices and bias \\[3072, 768\\] = 2359296, \\[3072\\] = 3072 and \\[768, 3072 \\] = 2359296, \\[768\\] = 768, (2359296+3072+ 2359296+768 = 4722432)\n  \n  * Layer Normalization weight and Bias \\[768\\], \\[768\\] = 1536\n  \n  * Total parameters for one complete attention layer (1797120 + 590592 + 1536 + 4722432 + 1536 = **7113216 ≈ 7𝑀**)\n  \n  * Total parameters for 12 layers of attention (𝟏𝟐 ∗ 𝟕𝟏𝟏𝟑𝟐𝟏𝟔 = **𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 ≈ 𝟖𝟓𝑴**)\n  \n  **Output layer of BERT Encoder:**\n  \n  * Dense Weight Matrix and Bias \\[768, 768\\] = 589824, \\[768\\] = 768, (589824 + 768 = 590592)\n  *Total Parameters in 𝑩𝑬𝑹𝑻 𝑩ase = 𝟐𝟑𝟖𝟑𝟕𝟏𝟖𝟒 + 𝟖𝟓𝟑𝟓𝟖𝟓𝟗𝟐 + 𝟓𝟗𝟎𝟓𝟗𝟐 = **𝟏𝟎𝟗𝟕𝟖𝟔𝟑𝟔𝟖 ≈ 𝟏𝟏𝟎𝑴***\n","lastmodified":"2023-06-25T14:09:32.953259701+01:00","tags":[]},"/UoE/NLU/Week4":{"title":"Week4","content":"\n# Word Embeddings\n\nReadings:\n\n* [Efficient estimation of word representations in vector space](https://arxiv.org/abs/1301.3781). Mikolov et al., NIPS Workshop 2013.\n\n* [Contextual word representations: A contextual](https://arxiv.org/abs/1902.06006) [introduction.](https://arxiv.org/abs/1902.06006) Smith, 2019. This paper provides a conceptual overview of word embeddings, and explains why contextualized embeddings are an important innovation\n\n* **Pre-training**: train a generic source model (e.g., VGG-16) on a standard, large dataset (e.g., ImageNet). You can think of pre-training as a way of initializing the parameters of your target model to good values.\n* **Finetuning**: then take the resulting model, keep its parameters, and replace the output layer to suit the new task. Now train this target model on the dataset for the new task. Transfer learning by finetuning.\n\nWhile finetuning, keep following things in mind:\n\n* we typically have fewer output classes in the finetuning class\n\n* use smaller learning rate\n\n* finetune only a handful of layer, while keeping other fixed: **weight freezing**\n\n* **source model in finetuning is usually a neural language model**, however the target task is very different from that, as the target task is rarely **Next-Word-Prediction** \n\n* We can all weights of embedding layer for the target model to be trained - this is a limited form of **finetuning.**  - We can do full-scale finetuning for NLP, which is possible because of contextualized word embeddings.\n\n* **Static Word Embeddings**:\n  \n  * Eg: **Word2Vec**, **Glove**, **FastText**\n  * They assign fixed vector to each word; **context independent**\n  * Used for initializing the embedding layers of a target model; i.e. for feature extraction\n  * **Not designed for finetuning**\n  * efficient and can be trained from scratch with humble resources\n* **Contextualized Word Embeddings**\n  \n  * Eg: Bert, GPT etc.\n  * Often called pretrained language model or \\*\\*LLM (large language models)\n  * assign a vector to a word that depends on its context: i.e. on its preceding and following words - **context dependent**\n  * can be used in target model to initialize embeddings like static embeddings\n  * **designed to be finetuned** - we re-train some of the weights of the embedding model for the target task.\n  * **requires considerable resources to train from scratch**; finetuning is efficient way of using them\n\n# Static Word Embeddings\n\n* **Word2Vec : Continuous Bag-of-words model (CBOW)**\n\n* \n\n* **CBOW**: uses words within a context window to predict the current word (5 window size is used)\n\n* architecture: \n  \n  * has only a single, linear hidden layer\n  * Weights for differnt position are shared\n* **Word2Vec: Skipgram**\n\n* \n\n* **Skipgram:** uses the current word to predict the context words\n\n* architecture:\n  \n  * flipped version of CBOW\n* **Details:**\n  \n  * Objective function maximizes the probability of the corpus D (set of all word and context pairs extracted from text):\n    \n    * $$\n      \n      ````\n        \\arg\\max_\\theta \\prod_{(w,c) \\epsilon D} p(c|w;\\theta)\n      ````\n    \n    $$\n  \n  * Here: $p(c|w;\\theta)$  is defined as:\n    $$\n    p(c|w;\\theta) = \\frac{exp(v_c . v_w)}{\\sum\\_{c^` \\epsilon C}exp(V_{c^`} . v_w)}\n    $$\n  \n  * $v_c$, $v_w$ are vector representation of $c$ and $w$ and $C$ is the set of all available contexts\n  \n  * $W$ - word vocabulary\n  \n  * $C$ - Context vocabulary\n  \n  * $d$ - embedding dim\n  \n  * each $w  \\epsilon  W$  is associated with vector $v_w \\epsilon \\mathbb{R}^d$\n  \n  * each $c \\epsilon C$  is associated with vector $v_c \\epsilon \\mathbb{R}^d$\n  \n  * NOTE: the formulation is impractical due to $\\sum\\_{c^` \\epsilon C}exp(V_{c^`} . v_w)$ over all context $c^\\`$ . This makes the softmax too expensive to compute for large vocabulary. Negative sampling helps us make the sum tractable.\n\n* **Negative Sampling**\n  \n  * Check if you have time: [word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf) - has the derivation of negative sampling objective and good information on skip-gram model.\n  * The idea is to not evaluate the full output layer based on the hidden layer. \n    * Treat this as independent logistic regression classifiers (i.e. **replace softmax objective with a binary classification objective**)\n      * The model is trained to distinguish\n        * the positive class / **target word**\n        * a few randomly sampled neurons or **randomly sampled negative words**\n      * This make training speech independent of vocabulary size and this can be parallelized\n* **Detailed derivation of Negative sampling Objective:**\n\nConsider a pair $(w, c)$ of word and context. Did this pair come from the   training data? Let’s denote by $p(D = 1|w, c)$ the probability that (w, c) came   from the corpus data. Correspondingly, $p(D = 0|w, c) = 1 − p(D = 1|w, c)$ will   be the probability that $(w, c)$ did not come from the corpus data. As before,  assume there are parameters θ controlling the distribution: $p(D = 1|w, c; θ)$.\n\n$$ \n\\\\begin{align}\n\u0026\\quad \\arg\\max\\_{\\theta} \\prod\\_{(w,c)\\in D} p(D=1|w,c;\\theta)\\\\ \u0026\\quad = \\arg\\max\\_{\\theta} \\log \\prod\\_{(w,c)\\in D} p(D=1|w,c;\\theta)\\\\ \u0026\\quad = \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log p(D=1|w,c;\\theta)\n\\\\end{align}\n$$\nwhere $p(D=1|w,c;\\theta)$ can be defined using softmax:\n$$ \\begin{align}\n\n\u0026\\quad p(D=1|w,c;\\theta) = \\frac{1}{1+e^{-v_c \\cdot v_w}} \\\\\n\\\\end{align}\n$$\nLeading to the objective: \n$$\\begin{align}\n\u0026\\quad \\Rightarrow \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log \\frac{1}{1+e^{-v_c \\cdot v_w}}\n\\\\end{align}\n$$\nThis objective has a trivial solution if we set $θ$ such that $p(D = 1|w, c; θ) = 1$   for every pair $(w, c)$. This can be easily achieved by setting θ such that $v_c = v_w$  and $v_c · v_w = K$ for all $v_c$, vw , where K is large enough number (practically, we   get a probability of 1 as soon as $K ≈ 40)$.  \nWe need a mechanism that prevents all the vectors from having the same  value, by disallowing some $(w, c)$ combinations. One way to do so, is to present  the model with some $(w, c)$ pairs for which $p(D = 1|w, c; θ)$ must be low, i.e.  pairs which are not in the data. This is achieved by generating the set $D^′$  of random $(w, c)$ pairs, assuming they are all incorrect (the name “negative-sampling” stems from the set $D^′$ of randomly sampled negative examples). The  optimization objective now becomes:\n\n$$\n\\\\begin{align}\n\u0026\\quad= \\arg\\max\\_{\\theta} \\prod\\_{(w,c)\\in D} p(D=1|c,w;\\theta) \\prod\\_{(w,c)\\in D'} (1 - p(D=1|c,w;\\theta)) \\\\\n\u0026\\quad= \\arg\\max\\_{\\theta} \\prod\\_{(w,c)\\in D} p(D=1|c,w;\\theta) \\prod\\_{(w,c)\\in D'} (1 - p(D=1|c,w;\\theta)) \\\\\n\u0026\\quad= \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log p(D=1|c,w;\\theta) + \\sum\\_{(w,c)\\in D'} \\log(1-p(D=1|w,c;\\theta)) \\\\\n\u0026\\quad= \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log \\frac{1}{1+e^{-v_c\\cdot v_w}} + \\sum\\_{(w,c)\\in D'} \\log \\frac{1}{1+e^{v_c\\cdot v_w}} \\\\\n\u0026\\quad= \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log \\frac{1}{1+e^{-v_c\\cdot v_w}} + \\sum\\_{(w,c)\\in D'} \\log \\frac{1}{1+e^{v_c\\cdot v_w}}\n\\\\end{align}\n$$\n\nIf we let $σ(x) = \\frac{1}{1+e^{−x}}$ we get:\t\n$$\n\\\\begin{align}\n\u0026\\quad\n\\\\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log \\frac{1}{1+e^{-v_c\\cdot v_w}} + \\sum\\_{(w,c)\\in D'} \\log \\frac{1}{1+e^{v_c\\cdot v_w}} \\\\\n\u0026\\quad= \\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log \\sigma(v_c\\cdot v_w) + \\sum\\_{(w,c)\\in D'} \\log \\sigma(-v_c\\cdot v_w)\n\\\\end{align}\n$$\nThis is similar to what we have in Mikolov's paper, however they assume $D^\\cdot \\epsilon D$  . i.e. the $(w,c)$ negative samples are taken from the dataset itself. hence their equation is \n\n$$\n\\\\arg\\max\\_{\\theta} \\sum\\_{(w,c)\\in D} \\log p(c|w) = \\sum\\_{(w,c)\\in D} \\left( \\log e^{v_c\\cdot v_w} - \\log \\sum\\_{c'} e^{v\\_{c'}\\cdot v_w} \\right)\n$$\n\n---\n\n \u003e \n \u003e **why does CBOW do better on syntactic tasks, given that word position isn't a factor, and skip-gram do better on semantic tasks?**\n\nSome types of syntactic tasks may not require, long-term context information or word order information, let's take these examples:\n\n1. \"The dogs chase the cat\"\n\n1. \"The big dog barks\"\n\n1. \"The dog that chases the cat barks loudly\"\n\nIn case 1) the relationship between the word \"dogs\" is in the local context of the verb \"chase\", and the verb can be inferred to be in the plural form to match the subject.\n\nIn case 2) the word \"dog\" is in the local context of the adjective \"big\", and the adjective can be inferred to be in the singular masculine form to match the noun.\n\nNow, given that word order information is not present in CBOW, this grammatical information may be inferred indirectly from the distributional semantics of the words which often occur together. Maybe because some tasks such as subject-verb agreement or noun-adjective agreement, can be performed based on local context without positional information, CBOW learns to focus on the syntactic properties instead of semantic relations between words. (*Neural network has a tendency to take shortcuts*).\n\nHowever, in case 3) where the agreement relationship is more complex or depends on longer-range dependencies, positional information may be necessary to capture the relationship accurately. In this case, CBOW may not perform well and skip-gram which better captures semantic relationships may serve better.\n\nnow; why does skip-gram do better on semantic tasks?\n\nIt is probably because of the architecture; skip-gram predicts the neighbouring words, which requires more semantic information. By predicting the context words based on the target word, skip-gram is able to capture broader semantic relationships between words that may not be captured by CBOW.\n\nHopefully, this helps.\n\nThis is also a good answer on SO: [https://ai.stackexchange.com/a/18637](https://ai.stackexchange.com/a/18637) if you want to check out.\n\n---\n\n# Pretrained Language Models\n\nReadings:\n\n* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). Devlin et al., NAACL 2019.\n\n* Using transformers, we can build language models that represent context very well: **contextualized embeddings**. \n* These language models can be **used for feature extraction**, but also in a **pre-training/finetuning setup**.\n\n### [Bert ](https://aclanthology.org/N19-1423.pdf)\n\n* Bert (Bidirectional Encoder Representation from Transformer)\n\n* **Purpose** : \n  \n  * **designed for pre-training** deep bidirectional representations from unlabeled text; \n  * **conditions on left and right context** in all layers; \n  * pre-trained model can be **finetuned with one additional output layer for many tasks** (e.g., NLI, QA, sentiment); \n  * for many tasks, no modifications to the Bert architecture are required;\n* **Architecture**:\n  \n  * Uses bidirectional representation - i.e. **uses the left and right context both**\n    * In contrast:\n      * GPT - uses only left-to-right context only (unidirectional)\n      * ELMO - uses shallow concatenation of independently trained left-to-right and right-to-left LSTMs\n  * **Input / Output Representation**:\n    * **Input sequence:** can be \\\u003cQuestion, Answer\u003e pair, single sentence, or any other string of tokens; \n    * **Token Types:** \n      * 30,000 token vocabulary, represented as **WordPiece embeddings** (handles OOV words); \n      * first token is always \\[**CLS**\\]: aggregate sentence representation for classification tasks; \n      * sentence pairs separated by \\[**SEP**\\] token; and by **segment embeddings**; \n        * **segment embedding**: segment embeddings are identical across all sub-words of a segment and only differ if input strings containing multiple  segments are fed into the model simultaneously (e.g. an  input containing two sentences separated by a \\[SEP\\]  token).\n        * the intuition behind segment embeddings is less obvious, especially  for unsegmented (i.e. single sentence) input\n          * While absent in the original Transformer, **BERT further  adds segment embeddings to the non-contextualized input representation**. In practice, this means that the input is combined with a sequence of segment IDs (a lookup of the sequence “0 0 ... 0 1 ... 1 1” ), which are found  in a lookup table of size 2 × embedding dimensionality (768 in BERTbase).\n          * Actually gets applied for the NSP task. Two sentences are fed as input to BERT and the model is asked to discriminate  usbetween the true next sentence and a randomly sampled alternative sentence occurring 50% of the time. For this task, it is important for the model to be able to distinguish between the two input sentences, hence segment embeddings are used.\n        * https://aclanthology.org/2022.lrec-1.152.pdf\n      * token position represented by **position embeddings**.\n\n\n\n* **Pretraining and Finetuning approaches**:\n  * The main reason why Bert is famous - Pretraining-Finetuning (though GPT can also be used in the same way)\n  \n  * **Pre-training BERT**:\n    \n    * **Task1: Masked LM**\n      \n      * **Try to predict the masked-tokens (this is simlar to skip-gram, where we predict the surrounding words)**\n      * We simply **mask some percentage of the input  tokens at random**, and then predict those masked  tokens. We refer to this procedure as a “masked  LM” (MLM), although it is often referred to as a  **Cloze task** in the literature.\n      * mask 15% of the tokens in the input sequence; train the model to predict these using the context word\n        * This however creates a mismatch between pre-training and fine-tuning, since the \\[MASK\\] token does not appear during fine-tuning.\n          * do not always replace masked words with \\[MASK\\], instead choose 15% of token positions at random for prediction; \n            * if $i^{th}$ token is chosen, we replace the $i^{th}$ token with: \n              1. the \\[MASK\\] token 80% of the time; \n              1. a random token 10% of the time; \n              1. the unchanged $i^{th}$ token 10% of the time.\n        * Now use $T_i$ to predict original token with cross entropy loss.\n    * **Task2: Next Sentence Prediction (NSP)**\n      \n      * Tasks like Question-Answering (QA) and NLI are based on relationship b/w two sentences, which is not directly captured by language modeling.\n      * Solution: Pre-train for a binarized NSP task; trivially this can be generated on any monolingual corpus. If we have two sentences $A$ and $B$ such that $B$ follows $A$\n        * 50% of the time $B$ is the actual next setence.\n        * 50% of the time $B$ is a random sentence from the corpus.\n  * Pretraining-Finetuning can be understood as:\n    \n    \n    * (1) sentence pairs in **paraphrasing**, \n    * (2) hypothesis-premise pairs in **entailment**, \n    * (3)  question-passage pairs in **question answering**\n    * (4) a degenerate $text-∅$ pair in text classification or **sequence tagging**.\n  * Token Level Task v/s Full sentence task:\n    \n    * **Token Level:**\n      * At the output, the token representations are fed into an output layer for token-level tasks, such as **sequence tagging** (eg: NER, POS) or **question answering**  (predicts span), \n    * **Sentence Level:**\n      * the $\\[CLS\\]$ representation is fed into an output layer for classification, such as **entailment** or **sentiment analysis** (predict class label)\n","lastmodified":"2023-06-25T14:09:32.953288076+01:00","tags":[]},"/UoE/NLU/Week5":{"title":"Week5","content":"\n### Language models are unsupervised multitask learners:\n\n**Moving from single task to multi-task | this time by using textual prompts.**\n\n* Learning to perform a single task can be expressed in a  probabilistic framework as estimating a conditional distribution $p(output|input)$. Since a general system should be  able to perform many different tasks, even for the same  input, it should condition not only on the input but also  on the task to be performed. That is, it should model  $p(output|input, task)$.\n  \n  For example, a translation training example can be written as the sequence \n  \n  * $(\\text{translate to french}, \\text{english text}, \\text{french text})$ . \n  * In order to help it infer that  this is the desired task, we condition the language model  on a context of example pairs of the format $\\text{english  sentence} = \\text{french sentence}$ and then after a final prompt of $\\text{english sentence =}$ . The model samples and gives translation.\n    Likewise, a reading comprehension training example can  be written as \n  * $(\\text{answer the question}, document,  question, answer)$\n\n**Conclusion:** \nWhen a large language model is trained on a sufficiently  large and diverse dataset it is able to perform well across  many domains and datasets. The diversity of tasks the model is able to  perform in a zero-shot setting suggests that high-capacity  models trained to maximize the likelihood of a sufficiently  varied text corpus begin to learn how to perform a surprising  amount of tasks without the need for explicit supervision\n\n# Prompting\n\nReadings:\n\n* Sections 1-3 [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language](https://arxiv.org/abs/2107.13586) [Processing](https://arxiv.org/abs/2107.13586), Liu et al. (2021)\n* [Language models are unsupervised multitask learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf), Radford et al. (2019)\n* [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)- GPT\n\n### A short overview of change in language modeling paradigm\n\n* At the start we used to have `fully-supervised learning` modeling. Here, we were doing task specific supervised modeling\n  * `feature-engineering`: domain knowledge requirement\n  * then, we moved towards `architecture-engineering`, for automatic learning of features for the supervised task.\n* Then we moved towards `pretrain-finetune` paradigm. `pre-train` a LM on a general purpose dataset as that is available in abundance. Adapt the `pre-trained` LM to downstream tasks by introducing additional parameters and `finetune` using task-specific objective functions.\n  * focus is shifted to `objective-engineering` i.e. the training objectives to be used at both `pretraining` and `finetuning` stage. We add a pre-training task which is similar to downstream task, this improves performance on the downstream task later.\n* Now we are moving towards `pre-train, prompt, predict` paradigm. Instead of adapting pre-trained LMs to downstream tasks via `objective-engineering`, we are reformulating the downstream tasks to look more like those solved during the original LM training with the help of textual `prompt`\n  * Eg:`“I felt so ___” `, and ask the LM to fill the blank with an emotion-bearing word. Or if we choose the prompt `“English: I missed the bus today. French: ”`), an LM may be able to fill in the blank with a French translation.\n\n\n\n### Prompting Basics\n\n\n\n* **Terminologies**:\n  \n  * `prefix prompt`: variety of prompt where the input text comes entirely before $\\bf{z}$\n  * `cloze prompt`: the first variety of prompt with a slot to fill in the middle of the text\n* **Prompt Addition**: $f\\_{prompt}(x)$ is applied on $\\bf{x}$ to to generate $\\mathbf{x}' = f\\_{prompt}(x)$\n  \n  1. Apply a template, which is a textual string that has two slots: an input slot \\[X\\] for input x and an answer slot \\[Z\\] for an intermediate generated answer text z that will later be mapped into y.\n  1. Fill slot \\[X\\] with the input text $\\bf{x}$.\n* **Answer Search**: \n  \n  * we search for the highest-scoring text $\\bf{z}ˆ$ that maximizes the score of the LM. We first define $Z$ as a set of permissible values for $\\bf{z}$.\n    $$\n    \\\\hat{z} = \\underset{z \\epsilon Z}{search} P(f\\_{fill}(x', z);\\theta)\n    $$\n  * $Z$ could take variety of input:\n    * **classification**: could be a small subset of the words `{“excellent”, “good”, “OK”, “bad”, “horrible”}` or `{++, +, ~, -, --}`\n    * **regression**: continuous values, constants \n* **Answer Mapping**: we would like to go from the highest-scoring answer $zˆ$ to the highest-scoring output $yˆ$. This is trivial for cases, where answer itself is the output, however for cases where multiple result could result in the same output, we need a mapping function:\n  \n  * sentiment-bearing words (e.g. “excellent”, “fabulous”, “wonderful”) to represent a single class (e.g. “++”)\n* **Design Considerations for Prompting**\n\n\n\n### Pre-trained Language Models\n\nSystematic view of various pre-trained LMs:\n\n* **main training objective**\n  * auxiliary training objective\n* **type of text noising**\n* **Directionality**: attention mask\n\n#### Main Training Objective\n\nThe main training objective of the pre-trained LMs plays an important role in determining its applicability to particular prompting tasks.\n\n* **Standard Language Model (SLM)**\n  \n  * Autoregressive prediction (left to right)\n    * These are particularly suitable for `prefix prompts`\n* **Denoising Objective**:\n  \n  * Noising function: $\\tilde{f} = f\\_{noise}(x)$\n  * Task to predict: $P(x|\\tilde{x})$\n  * These types of reconstruction objectives are suitable for `cloze prompts`\n  * Two common types of denoising objectives\n    * **Corrupted Text Reconstruction (CTR)**: the processed text to its uncorrupted state by calculating *loss over only the noised parts* of the input sentence\n    * **Full Text Reconstruction (FTR)**: reconstruct the text by *calculating the loss over the entirety of the input texts* whether it has been noised or not\n  * **Noising Functions**\n    * the specific type of corruption applied to obtain the noised text $\\tilde{x}$ has an effect on the efficacy of the learning algorithm\n    * **prior knowledge can be incorporated by controlling the type of noise**, e.g. *the noise could focus on entities of a sentence, which allows us to learn a pre-trained model with particularly high predictive performance for entities*\n\n\n\n* **SLM** or **FTR** objectives are maybe more suitable for *generation tasks*\n\n* tasks such as *classification* can be formulated using models trained with any of these objectives\n\n* **Auxiliary Training Objective**:\n  \n  * improve models’ ability to perform certain varieties of downstream tasks.\n  * **Next Sentence Prediction**: Next Sentence Prediction: do two segments appear consecutively - better sentence representations - `BERT`\n  * **Discourse Relation Prediction**: predict rhetorical relations between sentences - better semantics - `ERNIE [Sun et al., 2020]`\n  * **Image Region Prediction**: predict the masked regions of an image - for better visual-linguistic tasks - `VL-BERT [Su et al., 2020]`\n\n#### Directionality (Type of attention masking)\n\n* pre-trained LM can be different based on the directionality of the calculation of representations\n\n* **Bidirectional:** full attention no masking \n\n* **Left-to-right:** diagonal attention masking \n\n* Mix the two strategies\n\n\n\n#### Typical Pre-training Methods\n\nFollowing is a representation of popular pre-training methods:\n\n\n\n\n* **Left-to-Right Language Model**\n  * Popular backbone for many prompting methods. Representative examples of modern pre-trained left-to-right LMs include **GPT-3** , and **GPT-Neo**\n  * Generally large and difficult to train - generally not available to public, thus `pretraining and finetuning`  is generally not possible\n  * Useful for **generative tasks**\n* **Masked Language Models**\n  * Take advantage of full context. When the focus is shifted on generating optimal representation for downstream tasks.\n  * BERT is a popular example which aims to predict masked text pieces based on surrounded context\n  * In prompting methods, MLMs are generally most suitable for **natural language understanding or analysis tasks** (e.g., text classification, natural language inference , and extractive question answering).\n  * Suitable for `cloze prompting`. \n  * `pretraining-finetuning` is generally possible\n* **Prefix and Encoder-Decoder**\n  * Useful for conditional text-generation tasks such as **translation** and **summarization**\n    * such tasks need a pre-trained model both capable of endcoding the text and generating the output\n  * (1) using an encoder with **fully-connected mask** (full-attention, no masking) to encode the source $x$ first and then (2) decode the target $y$ **auto-regressively** (from the left to right)\n  * **In Prefix-LM**: Encoder-Decoder weights are shared. So same parameters are used to encode $x$ and $y$\n    * Eg: UniLM 1-2, ERNIE-M\n  * **In Encoder-Decoder**: Weights are different for E \u0026 D. $x$ is encoded using encoder weight whereas, $y$ is encoded using decoder weight. \n    * Eg: T5, BART\n  * These models were typically used for **text generation purposes**, however, recently they are **being used for non-generation tasks** such as QA, Information Extraction etc. \n\n### Prompt Engineering\n\n* Creating a promtping function $f{prompt}(x)$\n* Manual template engineering\n* Automated template learning of discrete prompts: \n  * Prompt mining ”\\[X\\] middle words \\[Z\\]” \n  * Paraphrase existing prompts - select the ones with highest accuracy \n* Continuous prompts: perform prompting directly in the embedding space of the model \n  * Initialise with discrete prompt, fne tune on task \n  * Template embeddings have their own parameters that can be tuned\n\n### Training\n\n* Promptless fne-tuning (BERT, ELMO) \n* Tuning free prompting: zero-shot (GPT3) \n* Fix prompt tune LM (T5) \n* Additional prompt parameteres: \n  * Fix LM tune prompt\n  * Tune LM and prompt (high resource)\n\n### T5 (Text-to-Text Transfer Transformer)\n\nReadings:\n\n* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf)\n\n\n\n* Model Size: up to 11B parameters - BERT-large is 330M \n\n* Amount of training data: 120B words of data \n\n* Domain/Cleanness of training data \n\n* Pretraining objective \n\n* Finetuning recipe \n\n* **Conclusion**: Scaling up model size and training data really helps Really easy to use pretrained model for multiple tasks using prompts!\n\n# Evaluating Machine Translation Systems\n\nReadings:\n\n* [Bleu: a method for automatic evaluation of machine translation](https://aclanthology.org/P02-1040.pdf), Papenini et al. (2002)\n* [COMET: A neural framework for MT evaluation](https://aclanthology.org/2020.emnlp-main.213/), Rei et al. (2020)\n\n* [To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation,](https://aclanthology.org/2021.wmt-1.57/) Kocmi et al. (2021)\n\n* **Why do we need to evaluate machine translation systems?**\n  \n  * Decide which of two (or more) systems to use. \n  * Evaluate incremental changes to systems. \n    * Does a new idea make it better or worse? \n    * Does it change things in the intended way? \n  * Decide whether a system is appropriate for a given use case. \n    * Understanding a restaurant menu. - allergy (life/death)\n    * Understanding a news about safety of a city you are visiting. \n    * Translating legal notices of a product you are selling. (MS - 150 languages, need to get client specifications right)\n    * Negotiating a peace treaty\n  * Different level/severity of consequences:\n    * assimilate - understand what is written / do some analysis\n    * disseminate - BBC news \n* There can be different translations from different translators. A good translation is both **adequate** and **fuent**\n  \n  * **Adequate**: Does the output convey the same meaning as the input sentence? Is part of the message lost, added, or distorted?\n  * **Fluent**: Is the output good fuent English? Is is grammatically correct? Does it use appropriate words and idioms?\n* Axis of quality of MT:\n  \n  * Adequacy:\n    * Earlier evaluated like this:\n      * `5) all meaning 4) most meaning 3) much meaning 2) little meaning 1) none`\n  * Fluency:\n    * Earlier evaluated like this:\n      * `5) flawless english 4) good english 3) non-native english 2) dis-fluent english 1) incomprehensible`\n  * Rating\n  * This is subjective, inconsistent, and non-reproducible.\n  * High inter-annotator disagreeability\n* **Goals for Evaluation Metrics**\n  \n  * **Low cost**: reduce time and money spent on carrying out evaluation\n  * **Tunable**: automatically optimize system performance towards metric\n  * **Meaningful**: score should give intuitive interpretation of translation quality\n  * **Consistent**: repeated use of metric should give same results\n  * **Correct**: metric must rank better systems higher\n* **Measuring agreement between evaluators**\n  $$\\kappa = \\frac{\\rho{(A)} - \\rho{(E)}}{ 1 - \\rho{(E)}}$$\n  \n  * $\\rho{(A)}$ is proportion of times that evaluators agree\n  * $\\rho{(E)}$ is proportion of times that they would agree by chance\n  * Adequacy, Fluecy are very hard to measure, low but positive agreement empirically\n  * Agreement on rating is relatively higher, but still low\n* **Can we evaluate automatically?**\n  \n  * Idea: Use human annotated reference to automatically compare the translations\n  * Requirement: Need an automatic metric\n* **Automatic metrics**\n  \n  * Idea 1:\n    \n    * **Precision**: $\\frac{\\text{\\# of correct words}}{\\text{\\# of output words}}$\n    * **Recall**: $\\frac{\\text{\\# of correct words}}{\\text{\\# of reference words}}$\n    * **F-Measure**: $\\frac{2 (\\text{precision x recall})}{\\text{precision + recall}}$\n    * Eg:\n      * System 1: Victory is `the opening` game `is always important`\n      * Reference: It `is always important` to win `the opening` match\n      * System 2: the it opening important is match always win to\n      * For System 1: P $( 5 / 8)$ R $(5 / 9)$ F1 $(0.58)$\n      * For System 2: P $(9 / 9)$ R $( 9/ 9)$ F1 $(1)$\n    * **We see the issue that : These do not take word order into account**\n  * Idea 2:\n    \n    * Count all of the n-grams that match\n    * **BLEU** score - computes precision for n-grams of size 1-4 against multiple reference.\n      * Recall is not defined in this setting\n      \n      * BLEU compares system length to an *effective reference length* and penalize if too short\n      \n      * Formula: $$\\text{BLEU} = BP \\cdot \\exp(\\sum\\_{n=1}^{N}w_n \\log p_n)$$\n      \n      * where BP is the brevity penalty,  - Add brevity penalty (for too short translations)\n      \n      * $w_n$ is the weight for n-gram precision, and \n      \n      * $p_n$ is the n-gram precision.\n      \n      * Another formula: $$\n        \\\\begin{equation\\*}\n        \\\\text{BLEU} = \\min\\left(1, \\frac{\\text{output-length}}{\\text{reference-length}}\\right) \\left( \\prod\\_{i=1}^4 \\text{precision}\\_i \\right)^{\\frac{1}{4}}\n        \\\\end{equation\\*}$$\n    \n    * Typically computed over the entire corpus, not single sentences\n    * To account for variability, use multiple reference translations  \n      – n-grams may match in any of the references  \n      – closest reference length used\n      * Used for:\n        * Machine Translation, Image Captioning, Text Summarization, Speech Recognition\n    * Cons:\n      * Human translators score low on BLEU (possibly because of higher variability, different word choices)\n      * Ignore relevance of words - names and core concepts more important than determiners and punctuation\n  * Idea 3: \n    \n    * Using trained metrics\n    * **COMET**  - Crosslingual Optimized Metric for Evaluation of Translation\n    * Pros:\n      * It has shown good correlation with human judgments of translation quality\n      * It is designed to be language-independent and can be used to evaluate translations across multiple languages.\n      * It takes into account the semantic similarity between the source and target sentences, which can be more meaningful than just comparing n-grams.\n      * It has been shown to be more reliable and consistent than other automatic evaluation metrics, such as BLEU.\n    * Cons:\n      * Scores are meaningless - absolute values are not informative\n      * It requires a pre-trained cross-lingual sentence encoder, which can be computationally expensive to train.\n      * It may not always align sentences correctly at the semantic level, which could lead to incorrect similarity scores.\n      * It may not be as interpretable as other metrics, such as BLEU, which simply count n-gram matches.\n","lastmodified":"2023-06-25T14:09:32.955113117+01:00","tags":[]},"/UoE/NLU/Week6":{"title":"Week6","content":"\n# Open Vocabulary Models\n\nReadings\n\n* [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162/), Sennrich et al. (2016)\n\n* [BPE-Dropout: Simple and Effective Subword Regularization](https://aclanthology.org/2020.acl-main.170.pdf), Provilkov et al. (2020)\n\nWe are encoding rare and unknown words as sequences of subword units. This is based on  the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations).\n\n**Problem:**\n\n**how do we represent text?** \n\n* 1-hot encoding \n  \n  * lookup of word embedding for input \n  * probability distribution over vocabulary for output \n* large vocabularies \n  \n  * increase network size \n  * decrease training and decoding speed \n* typical network vocabulary size: 10,000–100,000 symbols\n\n**NLU and NLG are open-vocabulary problems** \n\n* many training corpora contain millions of word types \n* productive word formation processes (compounding; derivation) allow formation and understanding of unseen words \n  * names, numbers are morphologically simple, but open word classes \n* Rest of this class we are going to focus on translation\n\n**Some common approaches which do not work:**\n\n* **Ignoring rare words:**\n  * replace out-of-vocabulary words with UNK \n  * a vocabulary of 50,000 words covers 95% of text  which gets you 95% of the way; if you only care about automatic metrics\n  * However:\n    * rare words are generally high information words:\n      * Eg: We we miss the name below; we won't know the `subject` of the sentence\n        * **source**: Mr `Gallagher` has offered a ray of hope. \n        * **reference**: Herr `Gallagher` hat einen hoffnungsstrahl ausgesandt .\n\n**Some initial approaches, which are not good enough:**\n\n* **Approximative Softmax**\n  * compute softmax over \"active\" subset of vocabulary (20 subsets)\n    * → smaller weight matrix, faster softmax \n  * at training time: vocabulary based on words occurring in training set partition \n  * at test time: determine likely target words based on source text (using cheap method like translation dictionary) \n  * limitations \n    * allows larger vocabulary, but still not open (completely new words don't have any representation)\n    * network may not learn good representation of rare words (eg. a word seen only once in 20 sets, will not have a good representation)\n* **Using back-off models**\n  * replace rare words with **UNK** at training time \n  * when system produces **UNK**, align **UNK** to source word, and translate this with back-off method\n  * limitations:\n    * **compounds**: hard to model 1-to-many relationships  (assumes that there is a 1-1 mapping of words in source and target languages)\n    * **morphology**: hard to predict inflection with back-off dictionary (eg. in languages like turkish with complex morphology)\n    * **names**: if alphabets differ, we need transliteration \n    * **alignment**: attention model unreliable (if you use alignment, to map words, the alignment from attention could be unreliable)\n* **character-based translations with phrase based models** - good results for closely related languages\n* **segmentation algorithm for phrase-based SMT are too conservative** - we need aggressive segmentation for open-word vocabulary (compact, without need for back-off dictionary)\n\n**Important approaches, which work and are generally good:**\n\n* **Subword NMT**\n  * **Subword Translation:**\n    * Idea: Rare words are potentially translateable through smaller units\n    * Subword segmentation can also avoid the information bottleneck of a fixed-length representation - i.e. in character models when we have too long words, joining embeddings could loose information.\n    * Potential category or words:\n      * **named entities**: Between languages that share an alphabet, names can often be copied from source to target text. Transcription or transliteration may be required, especially if the alphabets or syllabaries differ. Example:\n        * Barack Obama (English; German)\n        * バラク・オバマ (ba-ra-ku o-ba-ma) (Japanese)\n      * **cognates and loanwords**: Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient . Example:\n        * claustrophobia (English)\n        * Klaustrophobie (German)\n      * **morphologically complex words**: Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example:\n        * solar system (English)\n        * Sonnensystem (Sonne + System) (German)\n        * Naprendszer (Nap + Rendszer) (Hungarian)\n  * **Bye Pair Encoding (BPE)** (A method that actually works)\n    * merge frequent pairs of characters or character sequences.\n    * **why BPE**? \n      * **open-vocabulary**: operations learned on training set can be applied to unknown words \n      * **compression of frequent character sequences improves efficiency** → trade-off between text length and vocabulary size\n    * **Algorithm**:\n      \n    * two strategies:\n      * apply BPE separately for source and target language\n      * **apply BPE jointly for source and target language** (**Shared BPE**):\n        * translitration is used such that both language have same characters\n        * **Big Idea:** *If we apply BPE independently, the same name may be segmented differently in the two languages, which makes it harder for the neural models to learn a mapping between the subword units.*\n    * Example:\n      \n  * **BPE-Dropout**\n    * Adding stochastic noise to increase model robustness \n    * **Idea:** \n      * In BPE most frequent words are intact in vocabulary, learns how to compose with infrequent words \n      * If we sometimes forget to merge, we will learn how words compose, and better transliteration \n      * forget 1 in 10 times for most scripts, 6/10 in CKJ scripts \n    * **Algorithm:**\n      \n    * Consistently give 1+ BLEU scores across language pairs - widely used\n    * `-` (merge performed) ; `_` (red) (merge dropped) ; `_` (green) (merge performed)\n      * \n* **Character level NMT**\n  * Character-level Models:\n    * **advantages**: \n      * (mostly) open-vocabulary \n      * no heuristic or language-specific segmentation \n      * neural network can conceivably learn from raw character sequences •\n    * **drawbacks**: \n      * increasing sequence length slows training/decoding (reported x2–x8 increase in training time) \n    * open questions \n      * on which level should we represent meaning? \n      * on which level should attention operate?\n      * **The disappointing answer is:** whichever gives the better downstream performance\n\n# Low Resource MT\n\nReadings\n\n* [Survey of Low-Resource Machine Translation](https://arxiv.org/abs/2109.00486), Haddow et al. (2021)\n* [Improving Neural Machine Translation Models with Monolinguagl Data](https://aclanthology.org/P16-1009.pdf)\n* [Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets](https://arxiv.org/pdf/2103.12028.pdf)\n* [Transfer Learning for Low-Resource Neural Machine Translation](https://aclanthology.org/D16-1163.pdf)\n* [Trivial Transfer Learning for Low-Resource Neural Machine Translation](https://aclanthology.org/W18-6325.pdf)\n* [Choosing Transfer Languages for Cross-Lingual Learning](https://arxiv.org/pdf/1905.12688.pdf)\n  * https://github.com/neulab/langrank\n  * LangRank is a program to solve this task of automatically selecting optimal transfer languages, treating it as a ranking problem and building models that consider the aforementioned features to perform this prediction.\n* [Multilingual Denoising Pre-training for Neural Machine Translation](https://aclanthology.org/2020.tacl-1.47.pdf) - mBART\n  * [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)\n* [Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges](https://arxiv.org/pdf/1907.05019.pdf)\n* [Facebook AI’s WMT21 News Translation Task Submission](https://arxiv.org/pdf/2108.03265.pdf)\n* [An Analysis of Massively Multilingual Neural Machine Translation for  Low-Resource Languages](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.458.pdf)\n\n***“Low-resourced”-ness is a complex problem going beyond data availability and refects systemic problems in society.*** - Masakhane\n\n* **Corpus Creation**\n  \n  * Web crawling:\n    * Extract text from websites identified as multilingual `-\u003e` Align documents then sentences `-\u003e` Collate, deduplicate and filter\n    * For new languages: ask native speakers for websites where we can collect such parallel data\n  * Process of large scale extraction:\n    * \n  * How can we extract parallel data?\n    * Extraction from monolingual data:\n      \n      * Large collections of monolingual data contain parallel sentences: ***Common Crawl, Internet Archive***\n      * How do we detect these parallel data:\n        * Map sentences into a common embedding space using eg. LASER\n        * Nearest neighbours to find parallel sentences\n      * Eg: Datasets: **Paracrawl, WikiMatrix, CCMatrix, Samanantar**\n    * Problems with large scale extraction:\n      \n      * **Tools for low-resource languages are poor**  - such techniques may work for resource rich languages, but for resource scarce languages the tools might not be evolved and may not succeed to give the required overall outcome of sufficient quality\n      * **False positives can dominate** - suppose we have very less parallel data - as we are working at petabyte of data, 1% of the false data will be too much false data.\n      * The crawled resource itself may not have enough parallel data\n    * Quality of crawled data\n      \n      * \n      \n      \n      * **ParaCrawl v7.1** seems to be best\n* **Using Monolingual data for MT**\n  \n  * By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task `English↔German (+2.8–3.7 BLEU)`, and for the low-resourced IWSLT 14 task  `Turkish→English (+2.1–3.4 BLEU)`, obtaining new state-of-the-art results\n  * \n  * Iterated back translation for 2-3 iteration is sufficient, however this can fail if initial system is too weak.\n* **Using Multilingual data for MT**\n  \n  * **Big Idea:** \n    * First train a high-resource language pair (the parent model), then transfer some of the  learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 BLEU on four low-resource language pairs\n  * **How transfer learning was done exactly:** \n    * In the French–English to Uzbek–English example, as a result of the initialization, **the English word embeddings from the parent model are copied**, *but the Uzbek words are initially mapped to random French embeddings*. **The parameters of the English  embeddings are then frozen**, while *the Uzbek embeddings’ parameters are allowed to be modified, i.e. fine-tuned, during training of the child model*.\n      \n    * Parent and Child do not need to be related `-\u003e` [Trivial Transfer Learning for Low-Resource Neural Machine Translation](https://aclanthology.org/W18-6325.pdf)\n    * Extensive investigation of choice of parents `-\u003e` [Choosing Transfer Languages for Cross-Lingual Learning](https://arxiv.org/pdf/1905.12688.pdf)\n      * Data set size and lexical overlap important\n* **Transfer learning from Many Monolingual Corpora**\n  \n  * **mBART**:\n    * **Data**: 25 languages from Common Crawl `-\u003e` Then finetune parallel data separately for each task\n    * **Architecture:** Encoder-Decoder \n    * **Learning Method:** Our training data covers $K$ languages: $D = {D_1, . . . , D_K }$ where each $D_i$ is a collection  of monolingual documents in language $i$. We assume access to a noising function $g$, defined below, that corrupts text, and (2) train the model to predict the original text $X$ given $g(X)$. More formally, we aim to maximize $L_θ$:  $$ L\\_{\\theta} = \\sum\\_{D_i \\epsilon D} \\sum\\_{X \\epsilon D_i} log P(X|g(X);\\theta)$$ \n    * **Objective:** loss over full text reconstruction (not just over masked spans)\n      * **Noise**: \n        * **mask spans** of text: 35% of words\n        * **permute the order of sentences**\n    * **Token Type:** Language token for both source and target language\n    * *mBART*, *mBART50* `-\u003e` Basis of much practical work on low-resource MT\n* **Multilingual Models**:\n  \n  * **Idea:** Handle all $N$ by $N$ translation directions with a single model (instead of $O(N^2)$\n  * Usually 1-n or n-1\n  * Use a small number of related langauges (As not all language pair gives good result - different linguistic properties have an effect, other possibilities as well)\n  * Or go big: 103 languages [Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges](https://arxiv.org/pdf/1907.05019.pdf)\n  * There is a trade-off: \n    * Transfer: benefit from addition of other languages\n    * Interferance: performance is degraded due to having to  also learn to translate other languages\n  * **Pros**: Benefits are more noticeable for the many-to-English and low-resource pairs\n  * **Cons:** High-resource pairs tend to be harmed\n  * **Cons:** Massive systems require capacity\n* **Evaluation of Low-resrouce MT**\n  \n  * Is automatic evaluation of low-resource languages harder?\n    * Metrics are designed with high-resource langauges in mind\n    * Metrics are less reliable on poor systems\n    * Lack of good test sets and human evaluations for training metrics\n  * Human evaluation is preferable for low resource language\n  * Researchers need to connect to language communities\n\n# NLP Ethics\n\nReadings\n\n* [The Social Impact of Natural Language Processing](http://aclweb.org/anthology/P16-2096), Hovy and Spruit (2016)\n* [Ethical and social risks of harm from Language Models](https://arxiv.org/abs/2112.04359)   Weidinger et al. (2021)\n* [An Introduction to Data Ethics](https://www.scu.edu/media/ethics-center/technology-ethics/IntroToDataEthics.pdf), Vallor and Rewak. \n* [A Framework for Understanding Unintended Consequences of Machine Learning](https://arxiv.org/abs/1901.10002), Suresh and Guttag (2019)\n\n* **NLP affects people's lives**\n  \n  * We need to ask who is affected by an NLP experiment?\n  * Have they consented to the experiment or participating in it? - **Facebook's contagion experiment**\n  * What derived information about you is collected which you have not consented to? they can be traced from their data\n* **Who do these systems harm?**\n  \n  * Who gets admitted. \n  * Who gets hired. \n  * Who gets promoted. \n  * Who receives a loan. \n  * Who receives treatment for medical problems. \n  * Who receives the death penalty.\n* **Type of Risks**\n  \n  * The NLP model accurately refects natural speech, including unjust, toxic, and oppressive tendencies present in the training data.\n  \n  * **Discrimination, Exclusion and Toxicity**\n    \n    * **Allocational (material) harm**: discrimination\n      * eg. Models that analyse CVs for recruitment can be less likely to recommend historically discriminated groups\n      * eg: Accent challenge\n        * \n    * **Representative harm**: exclusionary norms eg. Q: what is a family? A: a man and a woman who get married and have children, and social stereotypes (Dr. - Man | Nurse - Woman)\n      * Audacious is to boldness as \\[religion\\] is to ...\n        * Muslim - Terrorism\n        * Jewish - Money\n    * **Ofensive Behaviour**: generate toxic language\n  * **Information Hazards** (Leads to Privacy and safety harms)\n    \n    * The language models could be prompted to give the private information / safety critical information which is present in it\n  * **Misinformation Harms** \n    \n    * The LM assigning high probabilities to false, misleading, nonsensical or poor quality information.\n    * **Harms involve**: people believing and acting on false information\n  * **Malicious Uses**\n    \n    * From humans intentionally using the LM to cause harms\n    * Types of harms:\n      * Illegitimate surveillance and censorship\n        * Saudi Arabia monitoring social media `-\u003e` persecutes dissidents without trial, often violently\n      * Facilitating fraud and impersonation scams\n      * Cyber attacks\n      * Misinformation campaigns\n* **While solving a problem we can ask these questions:**\n  \n  * Who are the stakeholders?\n  * What could go wrong?\n  * Who could benefit, and how?\n  * Who could be harmed, and how?\n  * What can you do to mitigate possible harms?\n","lastmodified":"2023-06-25T14:09:32.955535034+01:00","tags":[]},"/UoE/NLU/Week7":{"title":"Week7","content":"\n# Bias in Embeddings and Language Models\n\n# Summarization\n\nReading\n\n* [Get To The Point: Summarization with Pointer-Generator Networks](https://aclanthology.org/P17-1099.pdf)\n\n* [Text Summarization with Pretrained Encoders](https://aclanthology.org/D19-1387.pdf)\n\n* [Planning with Learned Entity Prompts for Abstractive Summarization](https://watermark.silverchair.com/tacl_a_00438.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAs4wggLKBgkqhkiG9w0BBwagggK7MIICtwIBADCCArAGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQML9ZBJlXoc5EThXYTAgEQgIICgemuaxuFlkWgZQg6GOwSCTfcKUZzH3RAxrV0KI1W22jaZA1jnX_1-i6OMTPJnE01m3nN2E0Z-jCwiH0gNiti5XW-vBVBpWKPFG7UbZqkQgs4G00Sf7RY38nvFE1qPbMuZ1rgQkMOXrjfuLMPmt-_Uyc8Edbv-vQ2MadOzK-tXZlTteVkV22rVzwhN8ptYhiXv9Kxeyl-r2Oov4NEaB4AyoJ79weu95aC-92Wxl0ItCijpYER-kq2NkXLWDRRpfAaLiJYgKTGBct2VvSt4fF-a22Hr4jh0gyRGqwlsE0ADYop0Ods7kIxsyybOTa5Ohg_X7T66A0mkeA4TFAxb0hpUUPjKJK5ScoE7xB0Y3FlkiWSBz_SHAToUTUjIG-qIu3AqynQ1mPX1SvU52CHfxN8Z5HE81Kz_wpwOmf6hy8Mvg4VsF5cnyyaqDTwD4kEjBCGkzp9b3s9Ouw_SfrbczG-5F01HzzQy9fu4mynH3o2w7EXFnIvnAHxJ_hX_hqXNgwp6eRnUQffm-6AhwCuT3GV9jPkJ5j4KbBh8qVyrJ6DfUfYr-_21yqwd9DCYJP5V4-PPjsOy0ojPX_Kxbwcd6rRIHg1N2r9HrVZewLqMkm-tqpOMQ-e5W2WFqK_RE-zQcoYt94fUuWjSw-nrDWTGYkcbXFdBi0zQuFREhhL11kuH6an3vyfaru8Noy-fReEjL6lP4lvZ1_UvDdLMTZZloF9PENlGTFg59PTkc8TCLKh8hpbPvslbODrbsAva0HMA8lyIqsglvWHQ8a0ZC0Xzf8KNV9ttmQOrMyAm-W53JXRY2yCcorEEHOXaN4i5uZl4SxQ0q_W4UbPoNybbqVC66TdlCBO)\n\n* [Conditional Generation with a Question-Answering Blueprint](https://www.researchgate.net/publication/361734006_Conditional_Generation_with_a_Question-Answering_Blueprint/fulltext/62c277363d26d6389e907d55/Conditional-Generation-with-a-Question-Answering-Blueprint.pdf?origin=publication_detail)\n\n* [Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation](https://aclanthology.org/2023.eacl-demo.13.pdf)\n\n* From both the examples below:\n  \n  * We can say that NLG is subjective i.e. the summaries, captions etc. could be different from different person's perspective\n    * eg: child migh need simpler sentences\n    * person focused on diseases will say it's a covid-super spreader event etc.\n      \n* A Language model predicts `next word` given predicted words so far: $p(y_t|y_1,... y\\_{t-1})$\n\n* A conditional language model predicts `next word`, given the words so far, and also some other input $x$: $p(y_t|y_1,...,y\\_{t-1}, x)$ , where $x$ can be image, text or a database etc.\n  \n  * For MT `-\u003e` $x=$ source sentence, $y=$ target sentence\n  * For Summarization `-\u003e` $x=$ input text, $y=$ summarized text\n* **Modeling Approach**\n  \n  here $J=\\frac{1}{T} \\sum\\_{t=1}^{T} J_t$  is the loss function used to optimize the model\n\n* **Summarization: Task Definition**\n  \n  * Given input text $x$, write summary $y$ which is shorter and contains main information of $x$. \n  * Summarization can be single-document or multi-document. Typically multi-document summarization will have overlapping content i.e. same topic or many articles about same event\n    * **Single-document** means we write summary y of single document $x$. \n    * **Multi-document** means we write summary y of multiple documents $x_1,..., x_n$\n  * Generally multi-document summarization is difficult:\n    * redundant content - but paraphrased so there might be repetition in summary\n    * $x$ in the conditional probability formulation will be now very long i.e $m \\times 500$, $m$ is number of document. This is more difficult to model.\n* **Type of summarization**\n  \n  * Extractive Summarization:\n    * Select parts (typically sentences) of the original text to form a summary\n    * Generally this is easier and restrictive (no paraphrasing)\n    * Less Fluent, Less Coherent\n  * Abstractive Summarization:\n    * Generate new text using NLG techniques\n    * More difficult but more flexible (human-like)\n    * Generally more fluent and coherent if done well\n* **Dataset for Summarization**\n  \n  * CNN Daily Mail:\n    * Pair of news articles (average 800 words) and summaries (aka story highlights), usually 3 or 4 sentences long (average 56 words)\n      * highlights are written by journalists, in a compressed telegraphic manner\n      * highlight need not form coherent summary - each highligh is relatively stand-alone, with little co-referencing (i.e. he, she, her, it etc.)\n    * CNN 100K pairs; Daily Mail 200K pairs\n* **Modeling Summarization Task**\n  \n  * **Summarization with Sequence-to-Sequence Attentional Model**\n    \n    \n    \n    * This is how the modeling is described here:\n      \n      * **Token:** Word (In summarization in this model, we used most common words for vocabulary with word as token `-\u003e` no BPE/wordpiece)\n      * **Encoder:** single-layer bidirectional LSTM produces a sequence of `hidden states` $h_i$.\n      * **Decoder:** single-layer unidirectional LSTM receives word embedding of previous word emitted by decoder and has `decoder state` $s_t$\n      * **Attention distribution:**  This tell the decoder which words are most important in the context vector and decoder knows where to focus\n        $$\n        \\\\begin{align} \n        \u0026\\quad e_i^t= v^T tanh(W_h h_i+W_s s_t + b\\_{attn}) \\\\ \u0026\\quad a^t = softmax(e^t) \n        \\\\end{align}\n        $$\n      * **Context Vector:** weighted sum of encoder hiddher states $h_i^\\* = \\sum\\_{i} a_i^t h_i$ ; this is what decoder will see from encoder.\n      * **Vocabulary Distribution:** probability distribution over all words in vocabulary, produced by decoder $P\\_{vocab} = \\text{softmax}(V'(V(\\[s_t, h_t^\\*\\] + b) + b'))$ ; $s_t$ is the decoder state; $V'$ is not important so we can ignore \n      * **Training Loss:** for time step $t$ is negative log likelihood of target word $w_t^*$, $loss_t=-log P(w_t^*)$\n    * Now the first problem with this model is that there are words which are not present in our data; we use UNK tokens in to represent them, however, it is possible that we start using them repeatedly in our summaries.\n    \n    * **Pointer Generator Network** comes to the rescue here; now instead of repeatedly using UNK tokens, we add a copying mechanism which is useful for rare words and phrases.\n      \n      * this model allows us both `copying words by pointing`, and `generating words` from a fixed vocabulary\n      * on each decoder step, we calculate $p\\_{gen}$, probability of `generating` next word (rather than copying it)\n      * we learn $p\\_{gen}$ during training; this balances of copying v/s generating  $$\n        P(w) = p\\_{gen}P\\_{vocab}(w) + (1 - p\\_{gen}) \\sum\\_{i:w_i=w}a_i^t\n        $$\n      * so during prediction of next word in decoder; we look at  $p\\_{gen}$ ; which acts as a gate; which tells us if we generate a word from vocabulary or if we should look into the encoder and if there is a word which is should copy it.\n    * Pointer Generator fixes the problem of generating UNK; however now it does not know if it has mentioned a word before and it will keep repeating it in the summary, which mean our summary will be repeatitive now. \n    \n    * We introduce a mechanism to check for the coverage of a word; which ensure that we do not repeat words in summary. This is called **Coverage Mechanism**\n      \n      * We add a **Coverage vector** $c^t$, which tells us what has been attended so far:\n        * $$c^t = \\sum\\_{t'=0}^{t-1} a^{t'}$$\n        * Use coverage vector as extra input to attention mechanism:$$\n          e_i^t = v^T tanh(W_h h_i + W_s s_t + W_c c_i^t + b\\_{attn})\n          $$\n        * **Coverage loss**: is added to training objective which penalizes overlap between coverage vector $c^t$ and new attention distribution $a^t$: $$\n          covloss_t = \\sum\\_{i} min (a_i^t, c_i^t)\n          $$\n          * this adds a penalty which promotes coverage if the word has not been attended to and discourages coverage from attending to already covered words\n  * **Summarization with Pre-Trained Encoders**\n    \n    * Pre-trained encoders like BERT are very successful in may NLU tasks, however for they were not setup for summarization. Why?\n      \n      * BERT is trained on sentence level (sentence pairs), will it work on document?\n      * Here's the encoder for BERT: \n      * How to make it ready for document as input? \n        * as we can see we have inserted \\[CLS\\] token at the start of each sentence; also sengment embeddings tells us where a new sentence start and end\n    * However this change brings another challenge:\n      \n      * there is a mismatch between encoder and decoder:\n        * difference between how they are setup and optimized - our decoder has not seen any data whereas our encoder has seen a lot of data.\n        * this implies that same training strategy won't work for both\n      * How to fix this mismatch?\n        * fine-tune strategy\n          * learning rate schedule (vaswani et al. 2017)\n            * ${lr} = \\tilde{lr}.min(step^{-0.5}, step.warmup^{-1.5})$\n        * we choose a `smaller learning rate` and `longer warming-up` for the encoder:\n          * $\\tilde{lr}\\_e = 2e^{-3}, warmup_e = 20,000$ \n        * we choose `larger learning rate` and `shorter warming-up` for the decoder:\n          * $\\tilde{lr}\\_e = 0.1, warmup_e = 10,000$ \n* **Evaluation of Summarization**\n  \n  * **ROUGE** : Recall-Oriented Understudy for Gisting Evaluation$$\n    ROUGE-N = \\frac{\\sum\\limits\\_{S\\epsilon{ReferenceSummaries}}  \\sum\\limits\\_{gram_n \\epsilon S}{Count\\_{match}(gram_n)}}{\\sum\\limits\\_{S\\epsilon{ReferenceSummaries}}\\sum\\limits\\_{gram_n \\epsilon S}{Count(gram_n)}}\n    $$\n  \n  * Like BLEU, it is based on n-gram overlap \n  \n  * ROUGE has no brevity penalty and is based on recall \n  \n  * Often F1 (combination of precision and recall)  and ROUGE is reported \n  \n  * Most commonly-reported ROUGE scores: **ROUGE-1** `unigram` overlap **ROUGE-2** `bigram` overlap, and **ROUGE-L** `longest common subsequence` overlap\n  \n  * **Summarization Results**\n    \n\n* **Conditional Generation: Objectives**\n  \n  * Generate natural language towards a `communicative goal` \n  \n  * which is `faithful` and can be `attributed` to is sources - `no hallucination`\n  \n  * while users Iexplicitly `control` generation outcome - `style - short, long` \n  \n  * Long-form QA\n    \n    * some queries have long-form answer and they require `multiple` documents to answer - `hallucinations` and `attribution` can be more problematic\n* changes to summarization systems:\n  \n  * Change the way entities are represented (Puduppully et al., 2019; Iso et al., 2019) \n  * The decoder skips low-confidence tokens (Tian et al., 2019)\n  * Encode documents hierarchically (Rhode et al., 2021)\n  * Adopt sparse attention mechanisms (Child et al., 2019; Beltagy et al., 2020) \n  * Introduce `planning` components (Puduppully et al., 2022; Narayan et al., 2022)\n* **Planning with Entity Chains** (Narayan et al., 2022)\n  \n  * \n  \n  * **Big Idea:** We extract entities and then creates a summary which chain them together\n    \n    * however, the entities are context dependent - eg. `Titanic` - `the boat / the movie`\n    * hence; if we miss the context we end up getting a wrong summary\n    * **how is this control/planning component added?**\n      * There is  **Questions under discussion (QUD)** theory of discourse structure which tells that during a discourse participants are mutually committed to resolving a partially structured **set of questions**  at a given point in time.\n      * now; **discourse** has these questions/plans **implicitly** which are eventually turned into answers in a successful discourse\n  * **Our plan is to turn the question into PLAN explicitly!!** how? lets see!\n    \n    * **Question-Answering Blueprints as Content Plans**\n      * Blueprints as intermediate (discrete) planning stage for conditional generation\n      \n      * Reduce faithfulness errors\n      \n      * Increase controllability\n      \n      * Are better for long-form inputs and outputs\n      \n      * **Blue Print Annotation**\n        \n        * Large-scale QA generation for output (summary, answer)\n        * QA selection and filtering for final blueprint\n        * **Steps for QA blueprint annotation:**\n          1. Question-Answering Overgeneration - Identify **noun phrases** and **named entities** as answer candidates\n          1. **Generate questions for each answer candidate** using SQuAD trained Question Generation model (T5-11B) - try to generate as many questions as we want\n          1. **FIltering** - Perform **round-trip consistency check** (Alberti et al., 2019)! - After getting QA normally , we ask the same questions again to the summary and if we get the same answer, then our question is selected \n             \n          1. **Filtering** - **Rheme-based selection** prioritizes new-information seeking questions\n          1. **Filtering** - **Coverage** prioritizes the selection of informative QA pairs by selecting non-overlapping ones.\n          * **QA Blueprint Annotation (Full picture)**\n      * **Blueprint Model**\n        \n        * \n        \n        * **End-to-end Blueprint Model**\n          \n          * Treat the blueprint as **prefix** maybe like in T5 model `-\u003e` Decoder blueprint `-\u003e` decode output \n          * Model ouputs the blueprint and summary\n          * **Issue:** The output sequence is too long and the decoder runs out of memory\n        * **Multitask Blueprint Model**\n          \n          * Two tasks that model does:\n            1. answer plan and output sequence\n            1. answer plan and questions (output and blueprint are generated separately during inference)\n          * Issue:  a) The output is conditioned only on answers and not on the questions b) we run the decoder and get the output sequence, to get the blueprint we have to rerun the model.\n        * **Iterative Blueprint Model**\n          \n          * Issue: a) No global plan (only local planning) b) This is slow as everything is planned sentence by sentence.\n      * **Evaluating Model:**\n        \n        * Datasets:\n          * **AQuaMuse** (Kulkarni et al., 2002, 2021): **long-form question answering**, simulates search engine, answer based on multiple retrieved documents.\n          * **WikiCatSum** (Perez-Beltrachini et al., 2019): **topic-focused multi-document summarization**, generate Wikipedia abstracts.\n          * **SummScreen** (Chen et al., 2022): **dialogue summarization**, generate summaries of TV episodes (e.g., CSI, The Bing Bang Theory). `-\u003e` `Hardest dataset as it has very long documents.`\n        * Import notes:\n          * \n          * We found out that with blueprint methods, our summaries are much longer than earlier.\n          * **Is this bad?**\n            * Not necessarily as we will see below\n            * First we evaluate using `ROUGE-N` and find out that all the blueprint based models (E2E, Multitask and Iterative) are similarly fluent as non-blueprint based model (LongT5):\n        * But can we only evaluate using ROUGE? isn't there anything else. Yes there are few more things we can do!\n          * **QA-based** metric: (QAFactEval; Fabbri et al, 2022)\n            * **Big Idea:** We have QA pairs, let's do QA on summary and see if we get the same answer; if we get the same answer we can claim that our **summaries are more grounded**\n            * **Is blueprint more grounded?** (yes it is)\n            * \n          * **Textual Entailment based metric**\n            * Quantify whether model summaries are **faithful** (*consistent text to the input document*) to input with textual entailment. $(t =\u003e h)$ **t** entails **h**?\n              * if human reading $t$ will infer $h$ is most likely true$$\n                F(s) = \\frac{1}{n}\\sum\\limits\\_{i=1}^n E(D, s_i)$$\n              * E is a textual entailment model trained on public data (Honovich et al., 2022) \n              * $n$ is the number of sentences in the summary; $D$ is input document/s. \n              * $F(s)$ correlates well with human ratings $(\\rho = 0.774)$.\n              * We can see that for SumScreen the difference is most discerable even though it is low.  Yes -\u003e blueprint methods are more faithful\n","lastmodified":"2023-06-25T14:09:32.957267158+01:00","tags":[]},"/UoE/NLU/Week9":{"title":"Week9","content":"\n# Neural Parsing\n\nReadings\n\n* [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449), Vinyals et al., NeurIPS 2015. This is the encoder-decoder-based pasing model introduced in the lecture.\n\n* [Constituency parsing with a self-attentive encoder.](https://arxiv.org/abs/1805.01052) Kitaev and Klein, ACL 2018. This is the transformer-based parsing model introduced in the lecture.\n\n* **The Big Idea**\n  \n  * The big idea is that we can generate any structured representation that can be linearized.\n    * Eg: syntactic parsing, where the input is a string of word and the output is a tree. It's a fairly high it's a highly structured object.\n    * Eg: Trees, Graphs\n* Parsing is the task of turning a sequence of words: (it helps disambiguate)\n  \n  * (1) You saw a man with a telescope. into a syntax tree:\n* **How can we use an encoder-decoder model for parsing?**\n  \n  * **Input**: is a string\n  * **Outpu**t: is not a string yet, we need to **linearize the syntax tree**:\n    * `(S (NP (Pro You ) ) (VP (V saw ) (NP (Det a ) (N man ) (PP (P with ) (Det a ) (N telescope ) ) ) ) )`\n    * We can even get rid of the words (is this a valid assumption?)\n      * `(S (NP Pro ) (VP V (NP Det N (PP P Det N ) ) ) )`\n    * And we can make it *easier to process by annotating also the closing brackets*:\n      * $(S\\ (NP \\ Pro)*{NP} (VP\\ V(NP\\ Det\\ N(PP\\ Det\\ N)*{PP} )*{NP} )*{VP} )\\_S$\n\n**What could go wrong?**\n\n* How do we know if the generated tree is valid?\\*\\*\n  \n  * turns out that RNN learns to close open brackets most of the time\n* **How about cross branches in syntax tree?**\n  \n  * model learns it as well\n* **Encoder-Decoder for Parsing** [Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449),\n  \n  * \n  * To make it work properly, we need to do following:\n    * **Add an end-of-sequence symbol**, as output sequences can vary in length. \n    * **Reverse the input string**: results in small performance gain. \n    * Make the **network deeper**. Vinyals et al. (2015) use three LSTM layers for both encoder and decoder. \n    * Add **attention**. This essentially works like the encoder-decoder with attention we saw for MT (lecture 7).\n    * Use pre-trained word embeddings as input (here: word2vec).\n    * **Get lots of training data**. Vinyals et al. (2015) use an existing parser (the Berkeley parser) to parse a large amount of text, which they then use as training data.\n* **Potential Problems**\n  \n  * How do we make sure that opening and closing brackets match? Else we won’t have a well-formed tree!\n    * This is really rare (0.8–1.5% of sentences). And if it occurs, just fix the brackets in post-processing (add brackets to beginning or end of the sequence).\n  * How do we associate the words in the input with the leaves of the tree in the output?\n    * You could just associate each input word with a PoS in the output, in sequence order. But in practice: only the tree is evaluated. Vinyals et al. (2015) replace all PoS tags with XX.\n  * The output sequence can be longer than the input sequence, isn’t this a problem?\n    * encoder-decoder can match sequence length mismatch; there is no restriction on sequence length both input and generated\n  * How can I make sure that the model outputs the best overall sequence, not just the best symbol at each time step?\n    * Use beam search to generate the output (as in MT). However, in practice, beam size has very little impact on performance. (this may not be the optimal one)\n    * How about - viterbi decoding or probabilstic CYK  (guranteed optimal)\n* **Evaluation**\n  \n  * Training Corpora: (part of penn tree bank was used)\n    * **Wall Street Journal (WSJ)**: treebank with 40k manually annotated sentences.\n    * **BerkeleyParser corpus**: 90k sentences from WSJ and several other treebanks, and 11M sentences parsed with **Berkeley Parser**.\n    * **High-confidence corpus**: 90k sentences from WSJ from several treebanks, and 11M sentences for which two parsers produce the same tree (length resampled).\n    * \n* **Parsing with Transformers** [Constituency Parsing with a Self-Attentive Encoder](https://arxiv.org/pdf/1805.01052.pdf)\n\n* **Architecture:**\n  \n  * \n  * **Context-Aware Word Representation**\n    * sub-words are used for representation of token\n    * simple concatenation of character embeddings or word prefixes/suffixes can outputperform POS information from external system\n    * \n* **Content v/s Position Attention**\n  \n  * **Attention on position**:  Different locations in the sentence can attend to each other based on their positions, \n  * **Attention on content/context**:  also based on  their contents (i.e. based on the words at or around those positions)\n  * We will later factor these two components in attention and we will find an improved performance in the parsing task.\n\n# Unsupervised Parsing\n\nReadings\n\n* [Unsupervised parsing via constituency tests](https://arxiv.org/abs/2010.03146). Cao et al., EMNLP 2020.\n\n**What happens when we do not have a tree bank?** Let's find out!\n\n* **General approach of Cao. et. al:**\n  \n  * **constituency test**: \n    * take a sentence, modify it via a transformation (e.g., replace a span with a pronoun);\n    * One type of constituency test involves modifying the sentence via some transformation and then judging the result.\n    * An unsupervised parser is designed by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.\n    * If the constituents pass the linguistic tests; we have evidence that a proposed constituent is an actual constituent\n  * **Neural grammaticality models**\n    \n    * check if the result is grammatical using a neural grammaticality model;\n    * produce a tree for the sentence by aggregating the results of constituency tests over spans;\n    * Specifically, we  score the likelihood that a span is a constituent by  \n      applying the constituency tests and averaging their grammaticality judgments\n      * $$s\\_{\\theta}(\\text{sent}, i, j) = \\dfrac{1}{\\lvert C \\rvert}\\sum\\_{c \\in C} g\\_{\\theta}(c(\\text{sent}, i, j))$$\n        * C denotes the set of constituency tests\n        * $c(sent, i, j)$ is a proposed constintuent\n        * a judgment function (grammaticality model) $g : sent  → {0, 1}$ that gives a score. If 1 -\u003e constituent is grammaticaly otherwise not.\n    * **How we learn the grammaticality model?**\n      \\* \n  * **Parsing Algoritm**\n    \n    * select the tree with the highest score; $$t^\\*(sent) = arg\\max\\_{t \\in T (len(sent))} \\sum\\_{(i,j) \\in t} s\\_{\\theta}(sent, i, j)$$\n      * $T(len(sent))$ denotes the set of binary  trees with $len(sent)$ leaves.\n      * If we interpret the score $s_θ(sent, i, j)$ as estimating the probability that the span $(sent, i, j)$ is a constituent, then this **formulation corresponds to choosing the tree with the highest expected number of constituents**, i.e. minimum risk decoding\n        * This allows for a low probability span in the tree if the model is confident about the rest of the tree\n    * we **score each tree by summing the scores**  of its spans and choose the **highest scoring binary tree via CKY**\n  * **Refinement:**\n    * add to this refinement: alternate between improving the tree and improving the grammaticality model.\n    * ## **How does this work?**\n","lastmodified":"2023-06-25T14:09:32.955151825+01:00","tags":[]},"/UoE/Speech-Synthesis/9.-Sequence-to-Sequence":{"title":"9. Sequence to Sequence","content":"\n\n","lastmodified":"2023-06-25T14:09:32.952888659+01:00","tags":[]},"/Writing-Mentor/Modeling-Editing-Process":{"title":"Modeling Editing Process","content":"\n* Learning to Model Editing Processes - https://arxiv.org/pdf/2205.12374.pdf\n* DiffusER: Discrete Diffusion via Edit-based Reconstruction - https://arxiv.org/pdf/2210.16886.pdf\n\n**Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation**\n\n* https://arxiv.org/pdf/1904.02357.pdf\n* https://www.youtube.com/watch?v=-hGd2399dnA\n","lastmodified":"2023-06-25T14:09:32.947972868+01:00","tags":[]}}